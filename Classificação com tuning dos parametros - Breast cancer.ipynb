{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622b3c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45d6f3ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from skorch import NeuralNetBinaryClassifier\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe76e093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x25898bf8430>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c17236ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = pd.read_csv('entradas_breast.csv')\n",
    "classe = pd.read_csv('saidas_breast.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "054a2294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a53cb443",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\seaborn\\_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(classe['0']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e0f6227",
   "metadata": {},
   "outputs": [],
   "source": [
    "previsores = np.array(previsores, dtype='float32')\n",
    "classe = np.array(classe, dtype='float32').squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6dc60288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db4da90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(previsores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79a982eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class classificador_torch(nn.Module):\n",
    "    def __init__(self, activation, neurons, initializer):\n",
    "        super().__init__()\n",
    "        self.dense0 = nn.Linear(30, neurons)\n",
    "        initializer(self.dense0.weight)\n",
    "        self.activation0 = activation\n",
    "        self.dense1 = nn.Linear(neurons, neurons)\n",
    "        initializer(self.dense1.weight)\n",
    "        self.activation1 = activation\n",
    "        self.dense2 = nn.Linear(neurons, 1)\n",
    "        initializer(self.dense2.weight)\n",
    "        self.output = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, X):\n",
    "        X = self.dense0(X)\n",
    "        X = self.activation0(X)\n",
    "        X = self.dense1(X)\n",
    "        X = self.activation1(X)\n",
    "        X = self.dense2(X)\n",
    "        X = self.output(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51bd5294",
   "metadata": {},
   "outputs": [],
   "source": [
    "classificador_sklearn = NeuralNetBinaryClassifier(module = classificador_torch,\n",
    "                                                 lr = 0.001,\n",
    "                                                 optimizer__weight_decay = 0.0001,\n",
    "                                                 train_split=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6844bb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo parametros para que o computador encontre o melhor conjunto de parametros para definir a estrutura da rede neural\n",
    "#ao inv√©s de ficarmos alterando manualmente procurando o melhor resultado.\n",
    "params = {'batch_size': [10],\n",
    "         'max_epochs': [100],\n",
    "         'optimizer': [torch.optim.Adam, torch.optim.SGD],\n",
    "         'criterion': [torch.nn.BCELoss, torch.nn.HingeEmbeddingLoss],\n",
    "         'module__activation': [F.relu, F.tanh],\n",
    "         'module__neurons': [8,16],\n",
    "         'module__initializer': [torch.nn.init.uniform_, torch.nn.init.normal_]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0999e92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(estimator=classificador_sklearn, param_grid=params, scoring='accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9c6373ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1429\u001b[0m  0.0410\n",
      "      2       37.1429  0.0410\n",
      "      3       37.1429  0.0390\n",
      "      4       37.1429  0.0390\n",
      "      5       37.1429  0.0400\n",
      "      6       37.1429  0.0430\n",
      "      7       37.1429  0.0400\n",
      "      8       37.1429  0.0400\n",
      "      9       37.1429  0.0410\n",
      "     10       37.1429  0.0400\n",
      "     11       37.1429  0.0400\n",
      "     12       37.1429  0.0400\n",
      "     13       37.1429  0.0400\n",
      "     14       37.1429  0.0410\n",
      "     15       37.1429  0.0400\n",
      "     16       37.1429  0.0390\n",
      "     17       37.1429  0.0420\n",
      "     18       37.1429  0.0400\n",
      "     19       37.1429  0.0390\n",
      "     20       37.1429  0.0400\n",
      "     21       37.1429  0.0400\n",
      "     22       \u001b[36m27.7222\u001b[0m  0.0390\n",
      "     23        \u001b[36m0.5138\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.4695\u001b[0m  0.0390\n",
      "     25        0.4867  0.0430\n",
      "     26        \u001b[36m0.4507\u001b[0m  0.0420\n",
      "     27        0.5292  0.0400\n",
      "     28        0.5367  0.0400\n",
      "     29        0.4636  0.0410\n",
      "     30        0.4549  0.0400\n",
      "     31        0.4534  0.0400\n",
      "     32        0.4581  0.0390\n",
      "     33        0.4652  0.0400\n",
      "     34        0.4758  0.0400\n",
      "     35        0.4654  0.0390\n",
      "     36        \u001b[36m0.4390\u001b[0m  0.0420\n",
      "     37        0.4597  0.0410\n",
      "     38        0.4543  0.0410\n",
      "     39        \u001b[36m0.4364\u001b[0m  0.0410\n",
      "     40        \u001b[36m0.4127\u001b[0m  0.0400\n",
      "     41        0.4143  0.0400\n",
      "     42        0.4128  0.0420\n",
      "     43        \u001b[36m0.4019\u001b[0m  0.0400\n",
      "     44        \u001b[36m0.4000\u001b[0m  0.0400\n",
      "     45        \u001b[36m0.3941\u001b[0m  0.0390\n",
      "     46        \u001b[36m0.3838\u001b[0m  0.0410\n",
      "     47        \u001b[36m0.3811\u001b[0m  0.0390\n",
      "     48        \u001b[36m0.3805\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.3628\u001b[0m  0.0400\n",
      "     50        0.3640  0.0400\n",
      "     51        \u001b[36m0.3595\u001b[0m  0.0410\n",
      "     52        \u001b[36m0.3568\u001b[0m  0.0390\n",
      "     53        \u001b[36m0.3532\u001b[0m  0.0400\n",
      "     54        \u001b[36m0.3489\u001b[0m  0.0390\n",
      "     55        0.3494  0.0400\n",
      "     56        \u001b[36m0.3430\u001b[0m  0.0400\n",
      "     57        \u001b[36m0.3322\u001b[0m  0.0400\n",
      "     58        0.3345  0.0390\n",
      "     59        \u001b[36m0.3286\u001b[0m  0.0410\n",
      "     60        \u001b[36m0.3258\u001b[0m  0.0410\n",
      "     61        \u001b[36m0.3253\u001b[0m  0.0400\n",
      "     62        \u001b[36m0.3174\u001b[0m  0.0410\n",
      "     63        \u001b[36m0.3173\u001b[0m  0.0410\n",
      "     64        \u001b[36m0.3104\u001b[0m  0.0390\n",
      "     65        \u001b[36m0.3086\u001b[0m  0.0400\n",
      "     66        \u001b[36m0.3036\u001b[0m  0.0420\n",
      "     67        \u001b[36m0.3000\u001b[0m  0.0390\n",
      "     68        \u001b[36m0.2968\u001b[0m  0.0400\n",
      "     69        \u001b[36m0.2919\u001b[0m  0.0390\n",
      "     70        \u001b[36m0.2853\u001b[0m  0.0400\n",
      "     71        \u001b[36m0.2838\u001b[0m  0.0390\n",
      "     72        \u001b[36m0.2777\u001b[0m  0.0410\n",
      "     73        \u001b[36m0.2745\u001b[0m  0.0400\n",
      "     74        \u001b[36m0.2685\u001b[0m  0.0400\n",
      "     75        \u001b[36m0.2665\u001b[0m  0.0400\n",
      "     76        \u001b[36m0.2623\u001b[0m  0.0420\n",
      "     77        \u001b[36m0.2582\u001b[0m  0.0400\n",
      "     78        \u001b[36m0.2565\u001b[0m  0.0410\n",
      "     79        \u001b[36m0.2484\u001b[0m  0.0400\n",
      "     80        \u001b[36m0.2478\u001b[0m  0.0390\n",
      "     81        \u001b[36m0.2440\u001b[0m  0.0400\n",
      "     82        \u001b[36m0.2421\u001b[0m  0.0390\n",
      "     83        \u001b[36m0.2383\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.2376\u001b[0m  0.0410\n",
      "     85        \u001b[36m0.2357\u001b[0m  0.0390\n",
      "     86        \u001b[36m0.2304\u001b[0m  0.0420\n",
      "     87        \u001b[36m0.2293\u001b[0m  0.0400\n",
      "     88        \u001b[36m0.2260\u001b[0m  0.0410\n",
      "     89        \u001b[36m0.2226\u001b[0m  0.0400\n",
      "     90        \u001b[36m0.2169\u001b[0m  0.0410\n",
      "     91        \u001b[36m0.2081\u001b[0m  0.0400\n",
      "     92        \u001b[36m0.2075\u001b[0m  0.0390\n",
      "     93        \u001b[36m0.2016\u001b[0m  0.0400\n",
      "     94        \u001b[36m0.2009\u001b[0m  0.0400\n",
      "     95        \u001b[36m0.1953\u001b[0m  0.0400\n",
      "     96        \u001b[36m0.1937\u001b[0m  0.0400\n",
      "     97        \u001b[36m0.1918\u001b[0m  0.0390\n",
      "     98        \u001b[36m0.1813\u001b[0m  0.0390\n",
      "     99        0.1870  0.0390\n",
      "    100        0.1859  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1429\u001b[0m  0.0400\n",
      "      2       37.1429  0.0400\n",
      "      3       37.1429  0.0390\n",
      "      4       37.1429  0.0400\n",
      "      5       37.1429  0.0390\n",
      "      6       37.1429  0.0400\n",
      "      7       37.1429  0.0400\n",
      "      8       37.1429  0.0400\n",
      "      9       37.1429  0.0400\n",
      "     10       37.1429  0.0390\n",
      "     11       37.1429  0.0400\n",
      "     12       37.1429  0.0400\n",
      "     13       37.1429  0.0390\n",
      "     14       37.1429  0.0390\n",
      "     15       37.1429  0.0410\n",
      "     16       37.1429  0.0390\n",
      "     17       37.1429  0.0400\n",
      "     18       37.1429  0.0430\n",
      "     19       37.1429  0.0390\n",
      "     20       37.1429  0.0400\n",
      "     21       37.1429  0.0390\n",
      "     22       37.1429  0.0430\n",
      "     23       37.1429  0.0390\n",
      "     24       37.1429  0.0390\n",
      "     25       37.1429  0.0420\n",
      "     26       37.1429  0.0400\n",
      "     27       \u001b[36m10.1885\u001b[0m  0.0390\n",
      "     28        \u001b[36m0.5883\u001b[0m  0.0400\n",
      "     29        \u001b[36m0.5708\u001b[0m  0.0390\n",
      "     30        \u001b[36m0.5382\u001b[0m  0.0410\n",
      "     31        \u001b[36m0.5244\u001b[0m  0.0390\n",
      "     32        \u001b[36m0.5100\u001b[0m  0.0400\n",
      "     33        \u001b[36m0.4972\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.4855\u001b[0m  0.0420\n",
      "     35        \u001b[36m0.4745\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.4635\u001b[0m  0.0420\n",
      "     37        \u001b[36m0.4524\u001b[0m  0.0410\n",
      "     38        \u001b[36m0.4442\u001b[0m  0.0400\n",
      "     39        \u001b[36m0.4365\u001b[0m  0.0410\n",
      "     40        \u001b[36m0.4259\u001b[0m  0.0400\n",
      "     41        \u001b[36m0.4207\u001b[0m  0.0400\n",
      "     42        \u001b[36m0.4134\u001b[0m  0.0390\n",
      "     43        \u001b[36m0.4065\u001b[0m  0.0390\n",
      "     44        \u001b[36m0.3992\u001b[0m  0.0390\n",
      "     45        \u001b[36m0.3897\u001b[0m  0.0390\n",
      "     46        \u001b[36m0.3834\u001b[0m  0.0400\n",
      "     47        \u001b[36m0.3782\u001b[0m  0.0390\n",
      "     48        \u001b[36m0.3742\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.3695\u001b[0m  0.0400\n",
      "     50        \u001b[36m0.3644\u001b[0m  0.0400\n",
      "     51        \u001b[36m0.3600\u001b[0m  0.0390\n",
      "     52        \u001b[36m0.3519\u001b[0m  0.0390\n",
      "     53        \u001b[36m0.3460\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3420\u001b[0m  0.0390\n",
      "     55        \u001b[36m0.3389\u001b[0m  0.0410\n",
      "     56        \u001b[36m0.3330\u001b[0m  0.0390\n",
      "     57        \u001b[36m0.3301\u001b[0m  0.0410\n",
      "     58        \u001b[36m0.3264\u001b[0m  0.0390\n",
      "     59        \u001b[36m0.3212\u001b[0m  0.0400\n",
      "     60        \u001b[36m0.3185\u001b[0m  0.0410\n",
      "     61        \u001b[36m0.3148\u001b[0m  0.0390\n",
      "     62        \u001b[36m0.3117\u001b[0m  0.0400\n",
      "     63        \u001b[36m0.3098\u001b[0m  0.0390\n",
      "     64        \u001b[36m0.3067\u001b[0m  0.0420\n",
      "     65        \u001b[36m0.3024\u001b[0m  0.0390\n",
      "     66        \u001b[36m0.3006\u001b[0m  0.0390\n",
      "     67        \u001b[36m0.2961\u001b[0m  0.0390\n",
      "     68        \u001b[36m0.2944\u001b[0m  0.0390\n",
      "     69        \u001b[36m0.2928\u001b[0m  0.0400\n",
      "     70        \u001b[36m0.2925\u001b[0m  0.0390\n",
      "     71        \u001b[36m0.2917\u001b[0m  0.0390\n",
      "     72        \u001b[36m0.2863\u001b[0m  0.0390\n",
      "     73        \u001b[36m0.2815\u001b[0m  0.0390\n",
      "     74        0.2825  0.0400\n",
      "     75        \u001b[36m0.2777\u001b[0m  0.0400\n",
      "     76        \u001b[36m0.2751\u001b[0m  0.0390\n",
      "     77        \u001b[36m0.2730\u001b[0m  0.0430\n",
      "     78        \u001b[36m0.2701\u001b[0m  0.0400\n",
      "     79        \u001b[36m0.2694\u001b[0m  0.0400\n",
      "     80        \u001b[36m0.2662\u001b[0m  0.0400\n",
      "     81        0.2669  0.0390\n",
      "     82        \u001b[36m0.2627\u001b[0m  0.0410\n",
      "     83        \u001b[36m0.2623\u001b[0m  0.0400\n",
      "     84        \u001b[36m0.2592\u001b[0m  0.0400\n",
      "     85        \u001b[36m0.2572\u001b[0m  0.0400\n",
      "     86        \u001b[36m0.2557\u001b[0m  0.0400\n",
      "     87        \u001b[36m0.2536\u001b[0m  0.0390\n",
      "     88        \u001b[36m0.2509\u001b[0m  0.0390\n",
      "     89        \u001b[36m0.2480\u001b[0m  0.0410\n",
      "     90        0.2507  0.0390\n",
      "     91        \u001b[36m0.2465\u001b[0m  0.0400\n",
      "     92        \u001b[36m0.2445\u001b[0m  0.0400\n",
      "     93        \u001b[36m0.2440\u001b[0m  0.0390\n",
      "     94        \u001b[36m0.2416\u001b[0m  0.0400\n",
      "     95        0.2416  0.0410\n",
      "     96        \u001b[36m0.2386\u001b[0m  0.0390\n",
      "     97        \u001b[36m0.2381\u001b[0m  0.0400\n",
      "     98        \u001b[36m0.2350\u001b[0m  0.0390\n",
      "     99        \u001b[36m0.2335\u001b[0m  0.0440\n",
      "    100        0.2368  0.0565\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0440\n",
      "      2       37.3626  0.0410\n",
      "      3       37.3626  0.0490\n",
      "      4       37.3626  0.0420\n",
      "      5       37.3626  0.0440\n",
      "      6       37.3626  0.0510\n",
      "      7       37.3626  0.0830\n",
      "      8       37.3626  0.0640\n",
      "      9       37.3626  0.0400\n",
      "     10       37.3626  0.0420\n",
      "     11       37.3626  0.0400\n",
      "     12       37.3626  0.0390\n",
      "     13       37.3626  0.0400\n",
      "     14       37.3626  0.0440\n",
      "     15       37.3626  0.0401\n",
      "     16       37.3626  0.0391\n",
      "     17       37.3626  0.0431\n",
      "     18       37.3626  0.0401\n",
      "     19       37.3626  0.0390\n",
      "     20       37.3626  0.0431\n",
      "     21       37.3626  0.0411\n",
      "     22       37.3626  0.0401\n",
      "     23       \u001b[36m18.6579\u001b[0m  0.0401\n",
      "     24        \u001b[36m0.5736\u001b[0m  0.0401\n",
      "     25        \u001b[36m0.5332\u001b[0m  0.0411\n",
      "     26        \u001b[36m0.5072\u001b[0m  0.0431\n",
      "     27        \u001b[36m0.4873\u001b[0m  0.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     28        \u001b[36m0.4715\u001b[0m  0.0401\n",
      "     29        \u001b[36m0.4572\u001b[0m  0.0391\n",
      "     30        \u001b[36m0.4397\u001b[0m  0.0411\n",
      "     31        \u001b[36m0.4310\u001b[0m  0.0406\n",
      "     32        \u001b[36m0.4176\u001b[0m  0.0431\n",
      "     33        \u001b[36m0.4061\u001b[0m  0.0406\n",
      "     34        \u001b[36m0.3971\u001b[0m  0.0401\n",
      "     35        \u001b[36m0.3848\u001b[0m  0.0421\n",
      "     36        \u001b[36m0.3785\u001b[0m  0.0406\n",
      "     37        \u001b[36m0.3654\u001b[0m  0.0411\n",
      "     38        \u001b[36m0.3600\u001b[0m  0.0406\n",
      "     39        \u001b[36m0.3596\u001b[0m  0.0401\n",
      "     40        \u001b[36m0.3454\u001b[0m  0.0401\n",
      "     41        0.3465  0.0401\n",
      "     42        \u001b[36m0.3443\u001b[0m  0.0401\n",
      "     43        \u001b[36m0.3218\u001b[0m  0.0401\n",
      "     44        \u001b[36m0.3210\u001b[0m  0.0411\n",
      "     45        \u001b[36m0.3140\u001b[0m  0.0416\n",
      "     46        \u001b[36m0.3076\u001b[0m  0.0441\n",
      "     47        \u001b[36m0.3026\u001b[0m  0.0446\n",
      "     48        \u001b[36m0.2983\u001b[0m  0.0431\n",
      "     49        \u001b[36m0.2953\u001b[0m  0.0476\n",
      "     50        \u001b[36m0.2847\u001b[0m  0.0461\n",
      "     51        \u001b[36m0.2705\u001b[0m  0.0451\n",
      "     52        0.2723  0.0461\n",
      "     53        \u001b[36m0.2695\u001b[0m  0.0431\n",
      "     54        0.2699  0.0456\n",
      "     55        \u001b[36m0.2579\u001b[0m  0.0446\n",
      "     56        0.2629  0.0456\n",
      "     57        0.2613  0.0471\n",
      "     58        \u001b[36m0.2578\u001b[0m  0.0431\n",
      "     59        \u001b[36m0.2530\u001b[0m  0.0581\n",
      "     60        \u001b[36m0.2509\u001b[0m  0.0491\n",
      "     61        \u001b[36m0.2498\u001b[0m  0.0451\n",
      "     62        \u001b[36m0.2421\u001b[0m  0.0461\n",
      "     63        \u001b[36m0.2418\u001b[0m  0.0431\n",
      "     64        \u001b[36m0.2400\u001b[0m  0.0446\n",
      "     65        \u001b[36m0.2309\u001b[0m  0.0431\n",
      "     66        0.2353  0.0436\n",
      "     67        0.2326  0.0456\n",
      "     68        \u001b[36m0.2281\u001b[0m  0.0411\n",
      "     69        0.2349  0.0396\n",
      "     70        0.2285  0.0401\n",
      "     71        \u001b[36m0.2276\u001b[0m  0.0410\n",
      "     72        \u001b[36m0.2258\u001b[0m  0.0401\n",
      "     73        \u001b[36m0.2238\u001b[0m  0.0411\n",
      "     74        0.2251  0.0406\n",
      "     75        0.2272  0.0426\n",
      "     76        \u001b[36m0.2227\u001b[0m  0.0406\n",
      "     77        \u001b[36m0.2214\u001b[0m  0.0401\n",
      "     78        \u001b[36m0.2189\u001b[0m  0.0441\n",
      "     79        0.2235  0.0406\n",
      "     80        0.2219  0.0411\n",
      "     81        0.2218  0.0396\n",
      "     82        \u001b[36m0.2180\u001b[0m  0.0401\n",
      "     83        0.2190  0.0471\n",
      "     84        \u001b[36m0.2123\u001b[0m  0.0411\n",
      "     85        0.2180  0.0401\n",
      "     86        \u001b[36m0.2036\u001b[0m  0.0396\n",
      "     87        \u001b[36m0.2019\u001b[0m  0.0401\n",
      "     88        \u001b[36m0.1992\u001b[0m  0.0406\n",
      "     89        \u001b[36m0.1911\u001b[0m  0.0411\n",
      "     90        0.2012  0.0421\n",
      "     91        0.2065  0.0391\n",
      "     92        \u001b[36m0.1879\u001b[0m  0.0421\n",
      "     93        \u001b[36m0.1869\u001b[0m  0.0412\n",
      "     94        0.1960  0.0401\n",
      "     95        0.1904  0.0406\n",
      "     96        \u001b[36m0.1861\u001b[0m  0.0411\n",
      "     97        0.1946  0.0421\n",
      "     98        0.1995  0.0396\n",
      "     99        0.1982  0.0401\n",
      "    100        0.1944  0.0416\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0411\n",
      "      2       37.3626  0.0406\n",
      "      3       37.3626  0.0390\n",
      "      4       37.3626  0.0421\n",
      "      5       37.3626  0.0401\n",
      "      6       37.3626  0.0390\n",
      "      7       37.3626  0.0396\n",
      "      8       37.3626  0.0441\n",
      "      9       37.3626  0.0406\n",
      "     10       37.3626  0.0401\n",
      "     11       37.3626  0.0401\n",
      "     12       37.3626  0.0406\n",
      "     13       37.3626  0.0496\n",
      "     14       37.3626  0.0426\n",
      "     15       37.3626  0.0401\n",
      "     16       37.3626  0.0396\n",
      "     17       37.3626  0.0385\n",
      "     18       37.3626  0.0401\n",
      "     19       37.3626  0.0406\n",
      "     20       37.3626  0.0401\n",
      "     21       37.3626  0.0406\n",
      "     22       37.3626  0.0401\n",
      "     23       \u001b[36m18.9149\u001b[0m  0.0411\n",
      "     24        \u001b[36m0.6077\u001b[0m  0.0406\n",
      "     25        \u001b[36m0.5591\u001b[0m  0.0401\n",
      "     26        \u001b[36m0.5399\u001b[0m  0.0416\n",
      "     27        \u001b[36m0.5326\u001b[0m  0.0401\n",
      "     28        \u001b[36m0.5202\u001b[0m  0.0421\n",
      "     29        \u001b[36m0.5139\u001b[0m  0.0431\n",
      "     30        \u001b[36m0.5036\u001b[0m  0.0391\n",
      "     31        \u001b[36m0.4957\u001b[0m  0.0436\n",
      "     32        \u001b[36m0.4834\u001b[0m  0.0391\n",
      "     33        \u001b[36m0.4758\u001b[0m  0.0409\n",
      "     34        \u001b[36m0.4693\u001b[0m  0.0401\n",
      "     35        \u001b[36m0.4612\u001b[0m  0.0391\n",
      "     36        \u001b[36m0.4548\u001b[0m  0.0396\n",
      "     37        \u001b[36m0.4463\u001b[0m  0.0401\n",
      "     38        \u001b[36m0.4382\u001b[0m  0.0411\n",
      "     39        \u001b[36m0.4322\u001b[0m  0.0401\n",
      "     40        \u001b[36m0.4269\u001b[0m  0.0391\n",
      "     41        \u001b[36m0.4223\u001b[0m  0.0401\n",
      "     42        \u001b[36m0.4141\u001b[0m  0.0391\n",
      "     43        \u001b[36m0.4085\u001b[0m  0.0396\n",
      "     44        \u001b[36m0.4018\u001b[0m  0.0421\n",
      "     45        \u001b[36m0.3929\u001b[0m  0.0406\n",
      "     46        \u001b[36m0.3870\u001b[0m  0.0401\n",
      "     47        \u001b[36m0.3780\u001b[0m  0.0401\n",
      "     48        \u001b[36m0.3694\u001b[0m  0.0416\n",
      "     49        \u001b[36m0.3619\u001b[0m  0.0401\n",
      "     50        \u001b[36m0.3506\u001b[0m  0.0406\n",
      "     51        \u001b[36m0.3429\u001b[0m  0.0401\n",
      "     52        \u001b[36m0.3358\u001b[0m  0.0421\n",
      "     53        \u001b[36m0.3289\u001b[0m  0.0401\n",
      "     54        \u001b[36m0.3230\u001b[0m  0.0391\n",
      "     55        \u001b[36m0.3156\u001b[0m  0.0401\n",
      "     56        \u001b[36m0.3065\u001b[0m  0.0401\n",
      "     57        \u001b[36m0.2990\u001b[0m  0.0411\n",
      "     58        \u001b[36m0.2915\u001b[0m  0.0391\n",
      "     59        \u001b[36m0.2885\u001b[0m  0.0391\n",
      "     60        \u001b[36m0.2816\u001b[0m  0.0411\n",
      "     61        \u001b[36m0.2750\u001b[0m  0.0401\n",
      "     62        \u001b[36m0.2683\u001b[0m  0.0401\n",
      "     63        \u001b[36m0.2611\u001b[0m  0.0421\n",
      "     64        \u001b[36m0.2582\u001b[0m  0.0391\n",
      "     65        \u001b[36m0.2572\u001b[0m  0.0506\n",
      "     66        \u001b[36m0.2510\u001b[0m  0.0401\n",
      "     67        \u001b[36m0.2458\u001b[0m  0.0406\n",
      "     68        \u001b[36m0.2411\u001b[0m  0.0401\n",
      "     69        \u001b[36m0.2410\u001b[0m  0.0421\n",
      "     70        \u001b[36m0.2391\u001b[0m  0.0401\n",
      "     71        \u001b[36m0.2370\u001b[0m  0.0411\n",
      "     72        \u001b[36m0.2353\u001b[0m  0.0401\n",
      "     73        0.2370  0.0401\n",
      "     74        \u001b[36m0.2320\u001b[0m  0.0406\n",
      "     75        \u001b[36m0.2286\u001b[0m  0.0401\n",
      "     76        0.2290  0.0421\n",
      "     77        \u001b[36m0.2262\u001b[0m  0.0416\n",
      "     78        \u001b[36m0.2252\u001b[0m  0.0401\n",
      "     79        \u001b[36m0.2238\u001b[0m  0.0426\n",
      "     80        \u001b[36m0.2223\u001b[0m  0.0401\n",
      "     81        \u001b[36m0.2189\u001b[0m  0.0396\n",
      "     82        \u001b[36m0.2152\u001b[0m  0.0401\n",
      "     83        0.2158  0.0401\n",
      "     84        \u001b[36m0.2125\u001b[0m  0.0401\n",
      "     85        \u001b[36m0.2116\u001b[0m  0.0401\n",
      "     86        \u001b[36m0.2097\u001b[0m  0.0401\n",
      "     87        \u001b[36m0.2084\u001b[0m  0.0411\n",
      "     88        \u001b[36m0.2061\u001b[0m  0.0411\n",
      "     89        0.2077  0.0406\n",
      "     90        \u001b[36m0.2050\u001b[0m  0.0401\n",
      "     91        0.2063  0.0491\n",
      "     92        \u001b[36m0.2030\u001b[0m  0.0411\n",
      "     93        0.2050  0.0401\n",
      "     94        0.2035  0.0401\n",
      "     95        \u001b[36m0.2008\u001b[0m  0.0421\n",
      "     96        0.2009  0.0406\n",
      "     97        0.2032  0.0401\n",
      "     98        \u001b[36m0.1994\u001b[0m  0.0421\n",
      "     99        \u001b[36m0.1975\u001b[0m  0.0410\n",
      "    100        0.1998  0.0416\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2807\u001b[0m  0.0411\n",
      "      2       37.2807  0.0401\n",
      "      3       37.2807  0.0396\n",
      "      4       37.2807  0.0401\n",
      "      5       37.2807  0.0396\n",
      "      6       37.2807  0.0411\n",
      "      7       37.2807  0.0413\n",
      "      8       37.2807  0.0406\n",
      "      9       37.2807  0.0401\n",
      "     10       37.2807  0.0406\n",
      "     11       37.2807  0.0401\n",
      "     12       37.2807  0.0406\n",
      "     13       37.2807  0.0401\n",
      "     14       37.2807  0.0411\n",
      "     15       37.2807  0.0441\n",
      "     16       37.2807  0.0411\n",
      "     17       37.2807  0.0406\n",
      "     18       37.2807  0.0416\n",
      "     19       37.2807  0.0391\n",
      "     20       37.2807  0.0416\n",
      "     21       37.2807  0.0391\n",
      "     22       37.2807  0.0426\n",
      "     23       37.2807  0.0401\n",
      "     24       37.2807  0.0411\n",
      "     25       37.2807  0.0401\n",
      "     26       37.2807  0.0411\n",
      "     27       37.2807  0.0416\n",
      "     28       37.2807  0.0411\n",
      "     29       37.2807  0.0426\n",
      "     30       37.2807  0.0401\n",
      "     31       37.2807  0.0426\n",
      "     32       37.2807  0.0401\n",
      "     33       37.2807  0.0401\n",
      "     34       37.2807  0.0426\n",
      "     35       37.2807  0.0401\n",
      "     36       37.2807  0.0406\n",
      "     37       37.2807  0.0421\n",
      "     38       37.2807  0.0451\n",
      "     39       37.2807  0.0697\n",
      "     40       37.2807  0.0692\n",
      "     41       37.2807  0.0787\n",
      "     42       37.2807  0.0622\n",
      "     43       37.2807  0.0692\n",
      "     44       37.2807  0.0772\n",
      "     45       37.2807  0.0622\n",
      "     46       37.2807  0.0757\n",
      "     47       37.2807  0.0511\n",
      "     48       37.2807  0.0752\n",
      "     49       37.2807  0.0657\n",
      "     50       37.2807  0.0768\n",
      "     51       37.2807  0.0521\n",
      "     52       37.2807  0.0456\n",
      "     53       37.2807  0.0466\n",
      "     54       37.2807  0.0561\n",
      "     55       37.2807  0.0463\n",
      "     56       37.2807  0.0454\n",
      "     57       37.2807  0.0466\n",
      "     58       37.2807  0.0532\n",
      "     59       37.2807  0.0456\n",
      "     60       37.2807  0.0446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     61       37.2807  0.0426\n",
      "     62       37.2807  0.0456\n",
      "     63       37.2807  0.0411\n",
      "     64       37.2807  0.0426\n",
      "     65       37.2807  0.0401\n",
      "     66       37.2807  0.0426\n",
      "     67       37.2807  0.0436\n",
      "     68       37.2807  0.0411\n",
      "     69       37.2807  0.0416\n",
      "     70       37.2807  0.0426\n",
      "     71       37.2807  0.0426\n",
      "     72       37.2807  0.0421\n",
      "     73       37.2807  0.0466\n",
      "     74       37.2807  0.0591\n",
      "     75       37.2807  0.0722\n",
      "     76       37.2807  0.0762\n",
      "     77       37.2807  0.0687\n",
      "     78       37.2807  0.0456\n",
      "     79       37.2807  0.0466\n",
      "     80       37.2807  0.0468\n",
      "     81       37.2807  0.0456\n",
      "     82       37.2807  0.0476\n",
      "     83       37.2807  0.0456\n",
      "     84       37.2807  0.0431\n",
      "     85       37.2807  0.0401\n",
      "     86       37.2807  0.0411\n",
      "     87       37.2807  0.0416\n",
      "     88       37.2807  0.0431\n",
      "     89       37.2807  0.0411\n",
      "     90       37.2807  0.0411\n",
      "     91       37.2807  0.0416\n",
      "     92       37.2807  0.0486\n",
      "     93       37.2807  0.0416\n",
      "     94       37.2807  0.0421\n",
      "     95       37.2807  0.0436\n",
      "     96       37.2807  0.0421\n",
      "     97       37.2807  0.0421\n",
      "     98       37.2807  0.0421\n",
      "     99       37.2807  0.0411\n",
      "    100       37.2807  0.0406\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1429\u001b[0m  0.0306\n",
      "      2       37.1429  0.0315\n",
      "      3       37.1429  0.0310\n",
      "      4       37.1429  0.0331\n",
      "      5       37.1429  0.0316\n",
      "      6       37.1429  0.0310\n",
      "      7       37.1429  0.0316\n",
      "      8       37.1429  0.0315\n",
      "      9       37.1429  0.0321\n",
      "     10       37.1429  0.0306\n",
      "     11       37.1429  0.0315\n",
      "     12       37.1429  0.0316\n",
      "     13       37.1429  0.0306\n",
      "     14       37.1429  0.0316\n",
      "     15       37.1429  0.0316\n",
      "     16       37.1429  0.0316\n",
      "     17       37.1429  0.0311\n",
      "     18       37.1429  0.0305\n",
      "     19       37.1429  0.0320\n",
      "     20       37.1429  0.0315\n",
      "     21       37.1429  0.0316\n",
      "     22       37.1429  0.0301\n",
      "     23       37.1429  0.0305\n",
      "     24       37.1429  0.0305\n",
      "     25       37.1429  0.0321\n",
      "     26       37.1429  0.0326\n",
      "     27       37.1429  0.0306\n",
      "     28       37.1429  0.0321\n",
      "     29       37.1429  0.0316\n",
      "     30       37.1429  0.0316\n",
      "     31       37.1429  0.0316\n",
      "     32       37.1429  0.0315\n",
      "     33       37.1429  0.0305\n",
      "     34       37.1429  0.0406\n",
      "     35       37.1429  0.0326\n",
      "     36       37.1429  0.0316\n",
      "     37       37.1429  0.0321\n",
      "     38       37.1429  0.0315\n",
      "     39       37.1429  0.0331\n",
      "     40       37.1429  0.0311\n",
      "     41       37.1429  0.0305\n",
      "     42       37.1429  0.0316\n",
      "     43       37.1429  0.0311\n",
      "     44       37.1429  0.0301\n",
      "     45       37.1429  0.0346\n",
      "     46       37.1429  0.0310\n",
      "     47       37.1429  0.0331\n",
      "     48       37.1429  0.0316\n",
      "     49       37.1429  0.0310\n",
      "     50       37.1429  0.0316\n",
      "     51       37.1429  0.0310\n",
      "     52       37.1429  0.0315\n",
      "     53       37.1429  0.0316\n",
      "     54       37.1429  0.0305\n",
      "     55       37.1429  0.0305\n",
      "     56       37.1429  0.0331\n",
      "     57       37.1429  0.0335\n",
      "     58       37.1429  0.0315\n",
      "     59       37.1429  0.0311\n",
      "     60       37.1429  0.0316\n",
      "     61       37.1429  0.0346\n",
      "     62       37.1429  0.0341\n",
      "     63       37.1429  0.0315\n",
      "     64       37.1429  0.0306\n",
      "     65       37.1429  0.0326\n",
      "     66       37.1429  0.0336\n",
      "     67       37.1429  0.0341\n",
      "     68       37.1429  0.0321\n",
      "     69       37.1429  0.0305\n",
      "     70       37.1429  0.0305\n",
      "     71       37.1429  0.0301\n",
      "     72       37.1429  0.0331\n",
      "     73       37.1429  0.0295\n",
      "     74       37.1429  0.0311\n",
      "     75       37.1429  0.0305\n",
      "     76       37.1429  0.0296\n",
      "     77       37.1429  0.0301\n",
      "     78       37.1429  0.0295\n",
      "     79       37.1429  0.0295\n",
      "     80       37.1429  0.0301\n",
      "     81       37.1429  0.0306\n",
      "     82       37.1429  0.0306\n",
      "     83       37.1429  0.0320\n",
      "     84       37.1429  0.0316\n",
      "     85       37.1429  0.0305\n",
      "     86       37.1429  0.0315\n",
      "     87       37.1429  0.0311\n",
      "     88       37.1429  0.0305\n",
      "     89       37.1429  0.0295\n",
      "     90       37.1429  0.0300\n",
      "     91       37.1429  0.0351\n",
      "     92       37.1429  0.0295\n",
      "     93       37.1429  0.0320\n",
      "     94       37.1429  0.0295\n",
      "     95       37.1429  0.0305\n",
      "     96       37.1429  0.0301\n",
      "     97       37.1429  0.0295\n",
      "     98       37.1429  0.0310\n",
      "     99       37.1429  0.0301\n",
      "    100       37.1429  0.0295\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1429\u001b[0m  0.0295\n",
      "      2       37.1429  0.0301\n",
      "      3       37.1429  0.0295\n",
      "      4       37.1429  0.0305\n",
      "      5       37.1429  0.0316\n",
      "      6       37.1429  0.0301\n",
      "      7       37.1429  0.0305\n",
      "      8       37.1429  0.0316\n",
      "      9       37.1429  0.0301\n",
      "     10       37.1429  0.0296\n",
      "     11       37.1429  0.0295\n",
      "     12       37.1429  0.0300\n",
      "     13       37.1429  0.0295\n",
      "     14       37.1429  0.0295\n",
      "     15       37.1429  0.0301\n",
      "     16       37.1429  0.0305\n",
      "     17       37.1429  0.0295\n",
      "     18       37.1429  0.0300\n",
      "     19       37.1429  0.0296\n",
      "     20       37.1429  0.0305\n",
      "     21       37.1429  0.0306\n",
      "     22       37.1429  0.0311\n",
      "     23       37.1429  0.0295\n",
      "     24       37.1429  0.0295\n",
      "     25       37.1429  0.0311\n",
      "     26       37.1429  0.0295\n",
      "     27       37.1429  0.0305\n",
      "     28       37.1429  0.0301\n",
      "     29       37.1429  0.0305\n",
      "     30       37.1429  0.0316\n",
      "     31       37.1429  0.0310\n",
      "     32       37.1429  0.0305\n",
      "     33       37.1429  0.0315\n",
      "     34       37.1429  0.0301\n",
      "     35       37.1429  0.0306\n",
      "     36       37.1429  0.0325\n",
      "     37       37.1429  0.0305\n",
      "     38       37.1429  0.0295\n",
      "     39       37.1429  0.0305\n",
      "     40       37.1429  0.0305\n",
      "     41       37.1429  0.0300\n",
      "     42       37.1429  0.0295\n",
      "     43       37.1429  0.0305\n",
      "     44       37.1429  0.0305\n",
      "     45       37.1429  0.0305\n",
      "     46       37.1429  0.0305\n",
      "     47       37.1429  0.0306\n",
      "     48       37.1429  0.0295\n",
      "     49       37.1429  0.0295\n",
      "     50       37.1429  0.0301\n",
      "     51       37.1429  0.0305\n",
      "     52       37.1429  0.0305\n",
      "     53       37.1429  0.0295\n",
      "     54       37.1429  0.0301\n",
      "     55       37.1429  0.0305\n",
      "     56       37.1429  0.0295\n",
      "     57       37.1429  0.0300\n",
      "     58       37.1429  0.0311\n",
      "     59       37.1429  0.0295\n",
      "     60       37.1429  0.0301\n",
      "     61       37.1429  0.0295\n",
      "     62       37.1429  0.0306\n",
      "     63       37.1429  0.0305\n",
      "     64       37.1429  0.0305\n",
      "     65       37.1429  0.0305\n",
      "     66       37.1429  0.0306\n",
      "     67       37.1429  0.0295\n",
      "     68       37.1429  0.0305\n",
      "     69       37.1429  0.0295\n",
      "     70       37.1429  0.0305\n",
      "     71       37.1429  0.0305\n",
      "     72       37.1429  0.0295\n",
      "     73       37.1429  0.0305\n",
      "     74       37.1429  0.0300\n",
      "     75       37.1429  0.0295\n",
      "     76       37.1429  0.0316\n",
      "     77       37.1429  0.0305\n",
      "     78       37.1429  0.0295\n",
      "     79       37.1429  0.0346\n",
      "     80       37.1429  0.0305\n",
      "     81       37.1429  0.0295\n",
      "     82       37.1429  0.0311\n",
      "     83       37.1429  0.0315\n",
      "     84       37.1429  0.0306\n",
      "     85       37.1429  0.0295\n",
      "     86       37.1429  0.0311\n",
      "     87       37.1429  0.0305\n",
      "     88       37.1429  0.0305\n",
      "     89       37.1429  0.0305\n",
      "     90       37.1429  0.0305\n",
      "     91       37.1429  0.0305\n",
      "     92       37.1429  0.0301\n",
      "     93       37.1429  0.0306\n",
      "     94       37.1429  0.0316\n",
      "     95       37.1429  0.0300\n",
      "     96       37.1429  0.0315\n",
      "     97       37.1429  0.0306\n",
      "     98       37.1429  0.0301\n",
      "     99       37.1429  0.0305\n",
      "    100       37.1429  0.0315\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0301\n",
      "      2       37.3626  0.0305\n",
      "      3       37.3626  0.0295\n",
      "      4       37.3626  0.0295\n",
      "      5       37.3626  0.0295\n",
      "      6       37.3626  0.0305\n",
      "      7       37.3626  0.0295\n",
      "      8       37.3626  0.0295\n",
      "      9       37.3626  0.0306\n",
      "     10       37.3626  0.0305\n",
      "     11       37.3626  0.0311\n",
      "     12       37.3626  0.0295\n",
      "     13       37.3626  0.0295\n",
      "     14       37.3626  0.0300\n",
      "     15       37.3626  0.0295\n",
      "     16       37.3626  0.0305\n",
      "     17       37.3626  0.0301\n",
      "     18       37.3626  0.0305\n",
      "     19       37.3626  0.0295\n",
      "     20       37.3626  0.0296\n",
      "     21       37.3626  0.0300\n",
      "     22       37.3626  0.0315\n",
      "     23       37.3626  0.0305\n",
      "     24       37.3626  0.0291\n",
      "     25       37.3626  0.0305\n",
      "     26       37.3626  0.0305\n",
      "     27       37.3626  0.0311\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     28       37.3626  0.0306\n",
      "     29       37.3626  0.0305\n",
      "     30       37.3626  0.0301\n",
      "     31       37.3626  0.0315\n",
      "     32       37.3626  0.0295\n",
      "     33       37.3626  0.0306\n",
      "     34       37.3626  0.0305\n",
      "     35       37.3626  0.0295\n",
      "     36       37.3626  0.0305\n",
      "     37       37.3626  0.0295\n",
      "     38       37.3626  0.0305\n",
      "     39       37.3626  0.0305\n",
      "     40       37.3626  0.0311\n",
      "     41       37.3626  0.0295\n",
      "     42       37.3626  0.0285\n",
      "     43       37.3626  0.0311\n",
      "     44       37.3626  0.0295\n",
      "     45       37.3626  0.0295\n",
      "     46       37.3626  0.0300\n",
      "     47       37.3626  0.0305\n",
      "     48       37.3626  0.0295\n",
      "     49       37.3626  0.0301\n",
      "     50       37.3626  0.0306\n",
      "     51       37.3626  0.0310\n",
      "     52       37.3626  0.0305\n",
      "     53       37.3626  0.0300\n",
      "     54       37.3626  0.0305\n",
      "     55       37.3626  0.0305\n",
      "     56       37.3626  0.0310\n",
      "     57       37.3626  0.0336\n",
      "     58       37.3626  0.0295\n",
      "     59       37.3626  0.0321\n",
      "     60       37.3626  0.0305\n",
      "     61       37.3626  0.0325\n",
      "     62       37.3626  0.0301\n",
      "     63       37.3626  0.0306\n",
      "     64       37.3626  0.0295\n",
      "     65       37.3626  0.0310\n",
      "     66       37.3626  0.0285\n",
      "     67       37.3626  0.0306\n",
      "     68       37.3626  0.0295\n",
      "     69       37.3626  0.0306\n",
      "     70       37.3626  0.0295\n",
      "     71       37.3626  0.0295\n",
      "     72       37.3626  0.0311\n",
      "     73       37.3626  0.0336\n",
      "     74       37.3626  0.0295\n",
      "     75       37.3626  0.0311\n",
      "     76       37.3626  0.0295\n",
      "     77       37.3626  0.0295\n",
      "     78       37.3626  0.0320\n",
      "     79       37.3626  0.0295\n",
      "     80       37.3626  0.0295\n",
      "     81       37.3626  0.0310\n",
      "     82       37.3626  0.0305\n",
      "     83       37.3626  0.0295\n",
      "     84       37.3626  0.0301\n",
      "     85       37.3626  0.0305\n",
      "     86       37.3626  0.0316\n",
      "     87       37.3626  0.0305\n",
      "     88       37.3626  0.0301\n",
      "     89       37.3626  0.0305\n",
      "     90       37.3626  0.0305\n",
      "     91       37.3626  0.0326\n",
      "     92       37.3626  0.0305\n",
      "     93       37.3626  0.0305\n",
      "     94       37.3626  0.0311\n",
      "     95       37.3626  0.0306\n",
      "     96       37.3626  0.0295\n",
      "     97       37.3626  0.0301\n",
      "     98       37.3626  0.0295\n",
      "     99       37.3626  0.0305\n",
      "    100       37.3626  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0295\n",
      "      2       37.3626  0.0295\n",
      "      3       37.3626  0.0301\n",
      "      4       37.3626  0.0305\n",
      "      5       37.3626  0.0295\n",
      "      6       37.3626  0.0301\n",
      "      7       37.3626  0.0295\n",
      "      8       37.3626  0.0305\n",
      "      9       37.3626  0.0306\n",
      "     10       37.3626  0.0301\n",
      "     11       37.3626  0.0296\n",
      "     12       37.3626  0.0295\n",
      "     13       37.3626  0.0310\n",
      "     14       37.3626  0.0305\n",
      "     15       37.3626  0.0295\n",
      "     16       37.3626  0.0290\n",
      "     17       37.3626  0.0305\n",
      "     18       37.3626  0.0300\n",
      "     19       37.3626  0.0313\n",
      "     20       37.3626  0.0285\n",
      "     21       37.3626  0.0305\n",
      "     22       37.3626  0.0295\n",
      "     23       37.3626  0.0300\n",
      "     24       37.3626  0.0315\n",
      "     25       37.3626  0.0295\n",
      "     26       37.3626  0.0316\n",
      "     27       37.3626  0.0305\n",
      "     28       37.3626  0.0315\n",
      "     29       37.3626  0.0291\n",
      "     30       37.3626  0.0305\n",
      "     31       37.3626  0.0295\n",
      "     32       37.3626  0.0300\n",
      "     33       37.3626  0.0295\n",
      "     34       37.3626  0.0300\n",
      "     35       37.3626  0.0301\n",
      "     36       37.3626  0.0295\n",
      "     37       37.3626  0.0316\n",
      "     38       37.3626  0.0305\n",
      "     39       37.3626  0.0311\n",
      "     40       37.3626  0.0305\n",
      "     41       37.3626  0.0295\n",
      "     42       37.3626  0.0301\n",
      "     43       37.3626  0.0295\n",
      "     44       37.3626  0.0305\n",
      "     45       37.3626  0.0301\n",
      "     46       37.3626  0.0305\n",
      "     47       37.3626  0.0295\n",
      "     48       37.3626  0.0295\n",
      "     49       37.3626  0.0305\n",
      "     50       37.3626  0.0320\n",
      "     51       37.3626  0.0305\n",
      "     52       37.3626  0.0305\n",
      "     53       37.3626  0.0305\n",
      "     54       37.3626  0.0295\n",
      "     55       37.3626  0.0316\n",
      "     56       37.3626  0.0296\n",
      "     57       37.3626  0.0305\n",
      "     58       37.3626  0.0301\n",
      "     59       37.3626  0.0305\n",
      "     60       37.3626  0.0305\n",
      "     61       37.3626  0.0301\n",
      "     62       37.3626  0.0305\n",
      "     63       37.3626  0.0305\n",
      "     64       37.3626  0.0290\n",
      "     65       37.3626  0.0305\n",
      "     66       37.3626  0.0305\n",
      "     67       37.3626  0.0301\n",
      "     68       37.3626  0.0300\n",
      "     69       37.3626  0.0295\n",
      "     70       37.3626  0.0295\n",
      "     71       37.3626  0.0316\n",
      "     72       37.3626  0.0305\n",
      "     73       37.3626  0.0295\n",
      "     74       37.3626  0.0306\n",
      "     75       37.3626  0.0305\n",
      "     76       37.3626  0.0305\n",
      "     77       37.3626  0.0295\n",
      "     78       37.3626  0.0305\n",
      "     79       37.3626  0.0316\n",
      "     80       37.3626  0.0306\n",
      "     81       37.3626  0.0305\n",
      "     82       37.3626  0.0295\n",
      "     83       37.3626  0.0305\n",
      "     84       37.3626  0.0295\n",
      "     85       37.3626  0.0305\n",
      "     86       37.3626  0.0315\n",
      "     87       37.3626  0.0315\n",
      "     88       37.3626  0.0305\n",
      "     89       37.3626  0.0316\n",
      "     90       37.3626  0.0311\n",
      "     91       37.3626  0.0361\n",
      "     92       37.3626  0.0295\n",
      "     93       37.3626  0.0295\n",
      "     94       37.3626  0.0305\n",
      "     95       37.3626  0.0305\n",
      "     96       37.3626  0.0305\n",
      "     97       37.3626  0.0305\n",
      "     98       37.3626  0.0305\n",
      "     99       37.3626  0.0306\n",
      "    100       37.3626  0.0295\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2807\u001b[0m  0.0285\n",
      "      2       37.2807  0.0301\n",
      "      3       37.2807  0.0295\n",
      "      4       37.2807  0.0305\n",
      "      5       37.2807  0.0320\n",
      "      6       37.2807  0.0305\n",
      "      7       37.2807  0.0305\n",
      "      8       37.2807  0.0296\n",
      "      9       37.2807  0.0295\n",
      "     10       37.2807  0.0296\n",
      "     11       37.2807  0.0305\n",
      "     12       37.2807  0.0301\n",
      "     13       37.2807  0.0295\n",
      "     14       37.2807  0.0305\n",
      "     15       37.2807  0.0305\n",
      "     16       37.2807  0.0320\n",
      "     17       37.2807  0.0316\n",
      "     18       37.2807  0.0296\n",
      "     19       37.2807  0.0305\n",
      "     20       37.2807  0.0295\n",
      "     21       37.2807  0.0311\n",
      "     22       37.2807  0.0295\n",
      "     23       37.2807  0.0295\n",
      "     24       37.2807  0.0305\n",
      "     25       37.2807  0.0300\n",
      "     26       37.2807  0.0305\n",
      "     27       37.2807  0.0295\n",
      "     28       37.2807  0.0300\n",
      "     29       37.2807  0.0305\n",
      "     30       37.2807  0.0305\n",
      "     31       37.2807  0.0310\n",
      "     32       37.2807  0.0295\n",
      "     33       37.2807  0.0300\n",
      "     34       37.2807  0.0301\n",
      "     35       37.2807  0.0295\n",
      "     36       37.2807  0.0307\n",
      "     37       37.2807  0.0300\n",
      "     38       37.2807  0.0305\n",
      "     39       37.2807  0.0305\n",
      "     40       37.2807  0.0305\n",
      "     41       37.2807  0.0300\n",
      "     42       37.2807  0.0305\n",
      "     43       37.2807  0.0295\n",
      "     44       37.2807  0.0321\n",
      "     45       37.2807  0.0305\n",
      "     46       37.2807  0.0295\n",
      "     47       37.2807  0.0321\n",
      "     48       37.2807  0.0306\n",
      "     49       37.2807  0.0301\n",
      "     50       37.2807  0.0351\n",
      "     51       37.2807  0.0315\n",
      "     52       37.2807  0.0326\n",
      "     53       37.2807  0.0300\n",
      "     54       37.2807  0.0295\n",
      "     55       37.2807  0.0326\n",
      "     56       37.2807  0.0346\n",
      "     57       37.2807  0.0295\n",
      "     58       37.2807  0.0295\n",
      "     59       37.2807  0.0311\n",
      "     60       37.2807  0.0341\n",
      "     61       37.2807  0.0285\n",
      "     62       37.2807  0.0306\n",
      "     63       37.2807  0.0301\n",
      "     64       37.2807  0.0305\n",
      "     65       37.2807  0.0295\n",
      "     66       37.2807  0.0330\n",
      "     67       37.2807  0.0336\n",
      "     68       37.2807  0.0326\n",
      "     69       37.2807  0.0290\n",
      "     70       37.2807  0.0295\n",
      "     71       37.2807  0.0295\n",
      "     72       37.2807  0.0300\n",
      "     73       37.2807  0.0305\n",
      "     74       37.2807  0.0295\n",
      "     75       37.2807  0.0300\n",
      "     76       37.2807  0.0295\n",
      "     77       37.2807  0.0295\n",
      "     78       37.2807  0.0301\n",
      "     79       37.2807  0.0305\n",
      "     80       37.2807  0.0295\n",
      "     81       37.2807  0.0305\n",
      "     82       37.2807  0.0301\n",
      "     83       37.2807  0.0305\n",
      "     84       37.2807  0.0316\n",
      "     85       37.2807  0.0300\n",
      "     86       37.2807  0.0296\n",
      "     87       37.2807  0.0305\n",
      "     88       37.2807  0.0310\n",
      "     89       37.2807  0.0316\n",
      "     90       37.2807  0.0305\n",
      "     91       37.2807  0.0300\n",
      "     92       37.2807  0.0316\n",
      "     93       37.2807  0.0305\n",
      "     94       37.2807  0.0301\n",
      "     95       37.2807  0.0306\n",
      "     96       37.2807  0.0305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     97       37.2807  0.0295\n",
      "     98       37.2807  0.0306\n",
      "     99       37.2807  0.0305\n",
      "    100       37.2807  0.0315\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1429\u001b[0m  0.0401\n",
      "      2       37.1429  0.0411\n",
      "      3       37.1429  0.0416\n",
      "      4       37.1429  0.0411\n",
      "      5       37.1429  0.0406\n",
      "      6       37.1429  0.0411\n",
      "      7       37.1429  0.0401\n",
      "      8       37.1429  0.0396\n",
      "      9       37.1429  0.0411\n",
      "     10       37.1429  0.0428\n",
      "     11       37.1429  0.0411\n",
      "     12       37.1429  0.0431\n",
      "     13       37.1429  0.0411\n",
      "     14       37.1429  0.0411\n",
      "     15       37.1429  0.0396\n",
      "     16       37.1429  0.0401\n",
      "     17       37.1429  0.0426\n",
      "     18       37.1429  0.0426\n",
      "     19       37.1429  0.0401\n",
      "     20       37.1429  0.0396\n",
      "     21       37.1429  0.0401\n",
      "     22       37.1429  0.0406\n",
      "     23       37.1429  0.0411\n",
      "     24       37.1429  0.0431\n",
      "     25       37.1429  0.0390\n",
      "     26       37.1429  0.0411\n",
      "     27       37.1429  0.0396\n",
      "     28       37.1429  0.0401\n",
      "     29       \u001b[36m11.8590\u001b[0m  0.0396\n",
      "     30        \u001b[36m0.6318\u001b[0m  0.0411\n",
      "     31        \u001b[36m0.6011\u001b[0m  0.0401\n",
      "     32        \u001b[36m0.5345\u001b[0m  0.0396\n",
      "     33        \u001b[36m0.5246\u001b[0m  0.0401\n",
      "     34        \u001b[36m0.5021\u001b[0m  0.0406\n",
      "     35        \u001b[36m0.4897\u001b[0m  0.0401\n",
      "     36        \u001b[36m0.4828\u001b[0m  0.0421\n",
      "     37        \u001b[36m0.4764\u001b[0m  0.0406\n",
      "     38        \u001b[36m0.4681\u001b[0m  0.0416\n",
      "     39        \u001b[36m0.4598\u001b[0m  0.0406\n",
      "     40        \u001b[36m0.4540\u001b[0m  0.0401\n",
      "     41        \u001b[36m0.4448\u001b[0m  0.0396\n",
      "     42        \u001b[36m0.4397\u001b[0m  0.0421\n",
      "     43        \u001b[36m0.4311\u001b[0m  0.0411\n",
      "     44        \u001b[36m0.4239\u001b[0m  0.0396\n",
      "     45        \u001b[36m0.4167\u001b[0m  0.0421\n",
      "     46        \u001b[36m0.4125\u001b[0m  0.0406\n",
      "     47        \u001b[36m0.4028\u001b[0m  0.0401\n",
      "     48        \u001b[36m0.3999\u001b[0m  0.0416\n",
      "     49        \u001b[36m0.3919\u001b[0m  0.0426\n",
      "     50        \u001b[36m0.3850\u001b[0m  0.0411\n",
      "     51        \u001b[36m0.3807\u001b[0m  0.0401\n",
      "     52        \u001b[36m0.3757\u001b[0m  0.0411\n",
      "     53        \u001b[36m0.3701\u001b[0m  0.0401\n",
      "     54        \u001b[36m0.3670\u001b[0m  0.0403\n",
      "     55        \u001b[36m0.3657\u001b[0m  0.0401\n",
      "     56        \u001b[36m0.3621\u001b[0m  0.0406\n",
      "     57        \u001b[36m0.3566\u001b[0m  0.0411\n",
      "     58        \u001b[36m0.3541\u001b[0m  0.0401\n",
      "     59        \u001b[36m0.3496\u001b[0m  0.0446\n",
      "     60        \u001b[36m0.3394\u001b[0m  0.0436\n",
      "     61        \u001b[36m0.3354\u001b[0m  0.0411\n",
      "     62        \u001b[36m0.3343\u001b[0m  0.0401\n",
      "     63        \u001b[36m0.3305\u001b[0m  0.0396\n",
      "     64        \u001b[36m0.3274\u001b[0m  0.0401\n",
      "     65        \u001b[36m0.3246\u001b[0m  0.0406\n",
      "     66        \u001b[36m0.3209\u001b[0m  0.0411\n",
      "     67        \u001b[36m0.3178\u001b[0m  0.0401\n",
      "     68        \u001b[36m0.3122\u001b[0m  0.0436\n",
      "     69        \u001b[36m0.3065\u001b[0m  0.0401\n",
      "     70        \u001b[36m0.2972\u001b[0m  0.0401\n",
      "     71        \u001b[36m0.2972\u001b[0m  0.0401\n",
      "     72        \u001b[36m0.2945\u001b[0m  0.0421\n",
      "     73        \u001b[36m0.2902\u001b[0m  0.0411\n",
      "     74        \u001b[36m0.2878\u001b[0m  0.0401\n",
      "     75        \u001b[36m0.2851\u001b[0m  0.0401\n",
      "     76        \u001b[36m0.2841\u001b[0m  0.0401\n",
      "     77        \u001b[36m0.2775\u001b[0m  0.0401\n",
      "     78        \u001b[36m0.2750\u001b[0m  0.0411\n",
      "     79        \u001b[36m0.2716\u001b[0m  0.0401\n",
      "     80        \u001b[36m0.2685\u001b[0m  0.0411\n",
      "     81        \u001b[36m0.2681\u001b[0m  0.0436\n",
      "     82        \u001b[36m0.2626\u001b[0m  0.0401\n",
      "     83        \u001b[36m0.2613\u001b[0m  0.0423\n",
      "     84        \u001b[36m0.2569\u001b[0m  0.0421\n",
      "     85        \u001b[36m0.2554\u001b[0m  0.0401\n",
      "     86        \u001b[36m0.2503\u001b[0m  0.0411\n",
      "     87        \u001b[36m0.2490\u001b[0m  0.0411\n",
      "     88        \u001b[36m0.2437\u001b[0m  0.0411\n",
      "     89        0.2451  0.0411\n",
      "     90        \u001b[36m0.2413\u001b[0m  0.0406\n",
      "     91        \u001b[36m0.2372\u001b[0m  0.0411\n",
      "     92        \u001b[36m0.2343\u001b[0m  0.0400\n",
      "     93        \u001b[36m0.2298\u001b[0m  0.0421\n",
      "     94        \u001b[36m0.2284\u001b[0m  0.0411\n",
      "     95        \u001b[36m0.2219\u001b[0m  0.0401\n",
      "     96        \u001b[36m0.2209\u001b[0m  0.0411\n",
      "     97        \u001b[36m0.2184\u001b[0m  0.0411\n",
      "     98        \u001b[36m0.2169\u001b[0m  0.0401\n",
      "     99        \u001b[36m0.2133\u001b[0m  0.0416\n",
      "    100        \u001b[36m0.2117\u001b[0m  0.0406\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1429\u001b[0m  0.0396\n",
      "      2       37.1429  0.0401\n",
      "      3       37.1429  0.0401\n",
      "      4       37.1429  0.0401\n",
      "      5       37.1429  0.0451\n",
      "      6       37.1429  0.0406\n",
      "      7       37.1429  0.0416\n",
      "      8       37.1429  0.0431\n",
      "      9       37.1429  0.0411\n",
      "     10       37.1429  0.0401\n",
      "     11       37.1429  0.0421\n",
      "     12       37.1429  0.0411\n",
      "     13       37.1429  0.0396\n",
      "     14       37.1429  0.0401\n",
      "     15       37.1429  0.0406\n",
      "     16       37.1429  0.0401\n",
      "     17       37.1429  0.0401\n",
      "     18       37.1429  0.0406\n",
      "     19       37.1429  0.0421\n",
      "     20       37.1429  0.0406\n",
      "     21       37.1429  0.0401\n",
      "     22       37.1429  0.0396\n",
      "     23       37.1429  0.0401\n",
      "     24       37.1429  0.0401\n",
      "     25       37.1429  0.0411\n",
      "     26       37.1429  0.0401\n",
      "     27       37.1429  0.0416\n",
      "     28       37.1429  0.0411\n",
      "     29       37.1429  0.0411\n",
      "     30       37.1429  0.0396\n",
      "     31       37.1429  0.0411\n",
      "     32       37.1429  0.0406\n",
      "     33       37.1429  0.0431\n",
      "     34       37.1429  0.0406\n",
      "     35       37.1429  0.0401\n",
      "     36       37.1429  0.0396\n",
      "     37       37.1429  0.0426\n",
      "     38       37.1429  0.0401\n",
      "     39       37.1429  0.0436\n",
      "     40       37.1429  0.0396\n",
      "     41       37.1429  0.0456\n",
      "     42       37.1429  0.0411\n",
      "     43       37.1429  0.0426\n",
      "     44       37.1429  0.0436\n",
      "     45       37.1429  0.0411\n",
      "     46       37.1429  0.0406\n",
      "     47       37.1429  0.0421\n",
      "     48       37.1429  0.0390\n",
      "     49       37.1429  0.0401\n",
      "     50       37.1429  0.0411\n",
      "     51       37.1429  0.0406\n",
      "     52       37.1429  0.0411\n",
      "     53       37.1429  0.0416\n",
      "     54       37.1429  0.0411\n",
      "     55       37.1429  0.0411\n",
      "     56       37.1429  0.0431\n",
      "     57       37.1429  0.0411\n",
      "     58       37.1429  0.0406\n",
      "     59       37.1429  0.0401\n",
      "     60       37.1429  0.0406\n",
      "     61       37.1429  0.0401\n",
      "     62       37.1429  0.0401\n",
      "     63       37.1429  0.0406\n",
      "     64       37.1429  0.0401\n",
      "     65       37.1429  0.0396\n",
      "     66       37.1429  0.0421\n",
      "     67       37.1429  0.0411\n",
      "     68       37.1429  0.0406\n",
      "     69       37.1429  0.0411\n",
      "     70       37.1429  0.0416\n",
      "     71       37.1429  0.0401\n",
      "     72       37.1429  0.0441\n",
      "     73       37.1429  0.0401\n",
      "     74       37.1429  0.0400\n",
      "     75       37.1429  0.0416\n",
      "     76       37.1429  0.0411\n",
      "     77       37.1429  0.0406\n",
      "     78       37.1429  0.0401\n",
      "     79       37.1429  0.0411\n",
      "     80       37.1429  0.0396\n",
      "     81       37.1429  0.0431\n",
      "     82       37.1429  0.0431\n",
      "     83       37.1429  0.0401\n",
      "     84       37.1429  0.0416\n",
      "     85       37.1429  0.0410\n",
      "     86       37.1429  0.0401\n",
      "     87       37.1429  0.0396\n",
      "     88       37.1429  0.0401\n",
      "     89       37.1429  0.0411\n",
      "     90       37.1429  0.0401\n",
      "     91       37.1429  0.0411\n",
      "     92       37.1429  0.0406\n",
      "     93       37.1429  0.0401\n",
      "     94       37.1429  0.0396\n",
      "     95       37.1429  0.0398\n",
      "     96       37.1429  0.0406\n",
      "     97       37.1429  0.0401\n",
      "     98       37.1429  0.0401\n",
      "     99       37.1429  0.0416\n",
      "    100       37.1429  0.0401\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0416\n",
      "      2       37.3626  0.0411\n",
      "      3       37.3626  0.0406\n",
      "      4       37.3626  0.0406\n",
      "      5       37.3626  0.0461\n",
      "      6       37.3626  0.0406\n",
      "      7       37.3626  0.0401\n",
      "      8       37.3626  0.0436\n",
      "      9       37.3626  0.0401\n",
      "     10       37.3626  0.0416\n",
      "     11       37.3626  0.0391\n",
      "     12       37.3626  0.0401\n",
      "     13       37.3626  0.0411\n",
      "     14       37.3626  0.0401\n",
      "     15       37.3626  0.0426\n",
      "     16       37.3626  0.0401\n",
      "     17       37.3626  0.0412\n",
      "     18       37.3626  0.0391\n",
      "     19       37.3626  0.0401\n",
      "     20       37.3626  0.0401\n",
      "     21       37.3626  0.0401\n",
      "     22       37.3626  0.0411\n",
      "     23       37.3626  0.0406\n",
      "     24       37.3626  0.0401\n",
      "     25       37.3626  0.0406\n",
      "     26       37.3626  0.0401\n",
      "     27       37.3626  0.0406\n",
      "     28       \u001b[36m35.5575\u001b[0m  0.0418\n",
      "     29        \u001b[36m0.8921\u001b[0m  0.0431\n",
      "     30        \u001b[36m0.7923\u001b[0m  0.0396\n",
      "     31        \u001b[36m0.5338\u001b[0m  0.0411\n",
      "     32        0.5340  0.0396\n",
      "     33        \u001b[36m0.5208\u001b[0m  0.0411\n",
      "     34        \u001b[36m0.5077\u001b[0m  0.0431\n",
      "     35        \u001b[36m0.4931\u001b[0m  0.0401\n",
      "     36        \u001b[36m0.4825\u001b[0m  0.0391\n",
      "     37        \u001b[36m0.4725\u001b[0m  0.0401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     38        \u001b[36m0.4615\u001b[0m  0.0411\n",
      "     39        \u001b[36m0.4516\u001b[0m  0.0401\n",
      "     40        \u001b[36m0.4395\u001b[0m  0.0401\n",
      "     41        \u001b[36m0.4321\u001b[0m  0.0401\n",
      "     42        \u001b[36m0.4222\u001b[0m  0.0416\n",
      "     43        \u001b[36m0.4122\u001b[0m  0.0401\n",
      "     44        \u001b[36m0.4037\u001b[0m  0.0401\n",
      "     45        \u001b[36m0.3944\u001b[0m  0.0421\n",
      "     46        \u001b[36m0.3869\u001b[0m  0.0401\n",
      "     47        \u001b[36m0.3760\u001b[0m  0.0401\n",
      "     48        \u001b[36m0.3652\u001b[0m  0.0401\n",
      "     49        \u001b[36m0.3602\u001b[0m  0.0401\n",
      "     50        \u001b[36m0.3519\u001b[0m  0.0421\n",
      "     51        \u001b[36m0.3384\u001b[0m  0.0426\n",
      "     52        \u001b[36m0.3343\u001b[0m  0.0401\n",
      "     53        \u001b[36m0.3307\u001b[0m  0.0416\n",
      "     54        \u001b[36m0.3177\u001b[0m  0.0411\n",
      "     55        \u001b[36m0.3097\u001b[0m  0.0401\n",
      "     56        \u001b[36m0.3052\u001b[0m  0.0411\n",
      "     57        \u001b[36m0.3028\u001b[0m  0.0411\n",
      "     58        0.3074  0.0416\n",
      "     59        \u001b[36m0.2933\u001b[0m  0.0406\n",
      "     60        \u001b[36m0.2830\u001b[0m  0.0401\n",
      "     61        \u001b[36m0.2803\u001b[0m  0.0396\n",
      "     62        0.2850  0.0401\n",
      "     63        \u001b[36m0.2757\u001b[0m  0.0406\n",
      "     64        \u001b[36m0.2731\u001b[0m  0.0401\n",
      "     65        \u001b[36m0.2629\u001b[0m  0.0411\n",
      "     66        \u001b[36m0.2620\u001b[0m  0.0396\n",
      "     67        \u001b[36m0.2582\u001b[0m  0.0401\n",
      "     68        \u001b[36m0.2470\u001b[0m  0.0390\n",
      "     69        \u001b[36m0.2456\u001b[0m  0.0411\n",
      "     70        \u001b[36m0.2410\u001b[0m  0.0411\n",
      "     71        0.2432  0.0421\n",
      "     72        \u001b[36m0.2359\u001b[0m  0.0406\n",
      "     73        \u001b[36m0.2329\u001b[0m  0.0406\n",
      "     74        \u001b[36m0.2270\u001b[0m  0.0401\n",
      "     75        \u001b[36m0.2270\u001b[0m  0.0406\n",
      "     76        0.2301  0.0411\n",
      "     77        0.2309  0.0436\n",
      "     78        0.2281  0.0406\n",
      "     79        \u001b[36m0.2220\u001b[0m  0.0411\n",
      "     80        \u001b[36m0.2209\u001b[0m  0.0396\n",
      "     81        \u001b[36m0.2181\u001b[0m  0.0401\n",
      "     82        \u001b[36m0.2147\u001b[0m  0.0401\n",
      "     83        \u001b[36m0.2100\u001b[0m  0.0411\n",
      "     84        0.2163  0.0411\n",
      "     85        0.2112  0.0406\n",
      "     86        \u001b[36m0.2087\u001b[0m  0.0411\n",
      "     87        \u001b[36m0.2072\u001b[0m  0.0406\n",
      "     88        \u001b[36m0.2036\u001b[0m  0.0406\n",
      "     89        0.2076  0.0411\n",
      "     90        0.2062  0.0396\n",
      "     91        \u001b[36m0.1977\u001b[0m  0.0413\n",
      "     92        0.2050  0.0406\n",
      "     93        \u001b[36m0.1958\u001b[0m  0.0401\n",
      "     94        0.2009  0.0401\n",
      "     95        \u001b[36m0.1940\u001b[0m  0.0406\n",
      "     96        \u001b[36m0.1931\u001b[0m  0.0411\n",
      "     97        0.1957  0.0416\n",
      "     98        \u001b[36m0.1908\u001b[0m  0.0411\n",
      "     99        \u001b[36m0.1884\u001b[0m  0.0406\n",
      "    100        0.1923  0.0421\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0416\n",
      "      2       37.3626  0.0426\n",
      "      3       37.3626  0.0401\n",
      "      4       37.3626  0.0396\n",
      "      5       37.3626  0.0411\n",
      "      6       37.3626  0.0411\n",
      "      7       37.3626  0.0441\n",
      "      8       37.3626  0.0401\n",
      "      9       37.3626  0.0421\n",
      "     10       37.3626  0.0401\n",
      "     11       37.3626  0.0406\n",
      "     12       37.3626  0.0401\n",
      "     13       37.3626  0.0416\n",
      "     14       37.3626  0.0401\n",
      "     15       37.3626  0.0401\n",
      "     16       37.3626  0.0396\n",
      "     17       37.3626  0.0401\n",
      "     18       37.3626  0.0406\n",
      "     19       37.3626  0.0391\n",
      "     20       37.3626  0.0411\n",
      "     21       37.3626  0.0396\n",
      "     22       37.3626  0.0401\n",
      "     23       37.3626  0.0416\n",
      "     24       37.3626  0.0401\n",
      "     25       37.3626  0.0401\n",
      "     26       37.3626  0.0406\n",
      "     27       37.3626  0.0421\n",
      "     28       \u001b[36m18.1148\u001b[0m  0.0406\n",
      "     29        \u001b[36m0.6138\u001b[0m  0.0401\n",
      "     30        \u001b[36m0.5714\u001b[0m  0.0423\n",
      "     31        \u001b[36m0.5501\u001b[0m  0.0401\n",
      "     32        \u001b[36m0.5377\u001b[0m  0.0411\n",
      "     33        \u001b[36m0.5289\u001b[0m  0.0396\n",
      "     34        \u001b[36m0.5136\u001b[0m  0.0421\n",
      "     35        \u001b[36m0.5028\u001b[0m  0.0416\n",
      "     36        \u001b[36m0.4951\u001b[0m  0.0411\n",
      "     37        \u001b[36m0.4828\u001b[0m  0.0401\n",
      "     38        \u001b[36m0.4738\u001b[0m  0.0416\n",
      "     39        \u001b[36m0.4658\u001b[0m  0.0401\n",
      "     40        \u001b[36m0.4576\u001b[0m  0.0406\n",
      "     41        \u001b[36m0.4474\u001b[0m  0.0401\n",
      "     42        \u001b[36m0.4402\u001b[0m  0.0401\n",
      "     43        \u001b[36m0.4291\u001b[0m  0.0401\n",
      "     44        \u001b[36m0.4178\u001b[0m  0.0390\n",
      "     45        \u001b[36m0.4068\u001b[0m  0.0406\n",
      "     46        \u001b[36m0.3979\u001b[0m  0.0401\n",
      "     47        \u001b[36m0.3878\u001b[0m  0.0406\n",
      "     48        \u001b[36m0.3780\u001b[0m  0.0411\n",
      "     49        \u001b[36m0.3711\u001b[0m  0.0401\n",
      "     50        \u001b[36m0.3664\u001b[0m  0.0416\n",
      "     51        \u001b[36m0.3606\u001b[0m  0.0411\n",
      "     52        \u001b[36m0.3545\u001b[0m  0.0441\n",
      "     53        \u001b[36m0.3489\u001b[0m  0.0411\n",
      "     54        \u001b[36m0.3442\u001b[0m  0.0406\n",
      "     55        \u001b[36m0.3387\u001b[0m  0.0391\n",
      "     56        \u001b[36m0.3325\u001b[0m  0.0400\n",
      "     57        \u001b[36m0.3275\u001b[0m  0.0396\n",
      "     58        \u001b[36m0.3211\u001b[0m  0.0410\n",
      "     59        \u001b[36m0.3177\u001b[0m  0.0396\n",
      "     60        \u001b[36m0.3123\u001b[0m  0.0411\n",
      "     61        \u001b[36m0.3093\u001b[0m  0.0401\n",
      "     62        \u001b[36m0.3064\u001b[0m  0.0416\n",
      "     63        \u001b[36m0.3029\u001b[0m  0.0401\n",
      "     64        \u001b[36m0.2994\u001b[0m  0.0406\n",
      "     65        \u001b[36m0.2961\u001b[0m  0.0401\n",
      "     66        \u001b[36m0.2930\u001b[0m  0.0406\n",
      "     67        \u001b[36m0.2892\u001b[0m  0.0401\n",
      "     68        \u001b[36m0.2871\u001b[0m  0.0411\n",
      "     69        \u001b[36m0.2859\u001b[0m  0.0406\n",
      "     70        \u001b[36m0.2826\u001b[0m  0.0411\n",
      "     71        \u001b[36m0.2772\u001b[0m  0.0396\n",
      "     72        \u001b[36m0.2733\u001b[0m  0.0401\n",
      "     73        \u001b[36m0.2693\u001b[0m  0.0411\n",
      "     74        0.2709  0.0456\n",
      "     75        \u001b[36m0.2604\u001b[0m  0.0426\n",
      "     76        0.2618  0.0396\n",
      "     77        \u001b[36m0.2509\u001b[0m  0.0411\n",
      "     78        \u001b[36m0.2487\u001b[0m  0.0406\n",
      "     79        \u001b[36m0.2448\u001b[0m  0.0411\n",
      "     80        \u001b[36m0.2435\u001b[0m  0.0436\n",
      "     81        0.2468  0.0406\n",
      "     82        \u001b[36m0.2430\u001b[0m  0.0411\n",
      "     83        \u001b[36m0.2410\u001b[0m  0.0411\n",
      "     84        \u001b[36m0.2386\u001b[0m  0.0401\n",
      "     85        \u001b[36m0.2315\u001b[0m  0.0401\n",
      "     86        0.2357  0.0411\n",
      "     87        0.2327  0.0411\n",
      "     88        0.2328  0.0411\n",
      "     89        \u001b[36m0.2265\u001b[0m  0.0401\n",
      "     90        0.2312  0.0411\n",
      "     91        \u001b[36m0.2250\u001b[0m  0.0421\n",
      "     92        0.2264  0.0401\n",
      "     93        \u001b[36m0.2232\u001b[0m  0.0406\n",
      "     94        0.2366  0.0411\n",
      "     95        \u001b[36m0.2198\u001b[0m  0.0411\n",
      "     96        \u001b[36m0.2195\u001b[0m  0.0421\n",
      "     97        0.2266  0.0406\n",
      "     98        0.2201  0.0421\n",
      "     99        0.2196  0.0416\n",
      "    100        \u001b[36m0.2146\u001b[0m  0.0451\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2807\u001b[0m  0.0391\n",
      "      2       37.2807  0.0401\n",
      "      3       37.2807  0.0401\n",
      "      4       37.2807  0.0421\n",
      "      5       37.2807  0.0416\n",
      "      6       37.2807  0.0401\n",
      "      7       37.2807  0.0451\n",
      "      8       37.2807  0.0401\n",
      "      9       37.2807  0.0401\n",
      "     10       37.2807  0.0441\n",
      "     11       37.2807  0.0411\n",
      "     12       37.2807  0.0471\n",
      "     13       37.2807  0.0441\n",
      "     14       37.2807  0.0431\n",
      "     15       37.2807  0.0426\n",
      "     16       37.2807  0.0421\n",
      "     17       37.2807  0.0411\n",
      "     18       37.2807  0.0409\n",
      "     19       37.2807  0.0401\n",
      "     20       37.2807  0.0416\n",
      "     21       37.2807  0.0431\n",
      "     22       37.2807  0.0421\n",
      "     23       37.2807  0.0406\n",
      "     24       37.2807  0.0401\n",
      "     25       37.2807  0.0406\n",
      "     26       37.2807  0.0391\n",
      "     27       37.2807  0.0401\n",
      "     28       37.2807  0.0406\n",
      "     29       \u001b[36m18.2463\u001b[0m  0.0421\n",
      "     30        \u001b[36m0.8246\u001b[0m  0.0407\n",
      "     31        \u001b[36m0.5949\u001b[0m  0.0411\n",
      "     32        \u001b[36m0.5530\u001b[0m  0.0446\n",
      "     33        \u001b[36m0.5351\u001b[0m  0.0416\n",
      "     34        \u001b[36m0.5313\u001b[0m  0.0399\n",
      "     35        \u001b[36m0.5140\u001b[0m  0.0416\n",
      "     36        \u001b[36m0.5035\u001b[0m  0.0411\n",
      "     37        \u001b[36m0.4939\u001b[0m  0.0396\n",
      "     38        \u001b[36m0.4873\u001b[0m  0.0401\n",
      "     39        \u001b[36m0.4784\u001b[0m  0.0421\n",
      "     40        \u001b[36m0.4734\u001b[0m  0.0401\n",
      "     41        \u001b[36m0.4635\u001b[0m  0.0441\n",
      "     42        \u001b[36m0.4567\u001b[0m  0.0411\n",
      "     43        \u001b[36m0.4514\u001b[0m  0.0401\n",
      "     44        \u001b[36m0.4325\u001b[0m  0.0421\n",
      "     45        \u001b[36m0.4296\u001b[0m  0.0411\n",
      "     46        \u001b[36m0.4172\u001b[0m  0.0441\n",
      "     47        \u001b[36m0.4111\u001b[0m  0.0441\n",
      "     48        \u001b[36m0.4040\u001b[0m  0.0416\n",
      "     49        \u001b[36m0.3971\u001b[0m  0.0426\n",
      "     50        \u001b[36m0.3898\u001b[0m  0.0411\n",
      "     51        \u001b[36m0.3824\u001b[0m  0.0416\n",
      "     52        \u001b[36m0.3701\u001b[0m  0.0401\n",
      "     53        \u001b[36m0.3648\u001b[0m  0.0426\n",
      "     54        \u001b[36m0.3514\u001b[0m  0.0401\n",
      "     55        0.3590  0.0436\n",
      "     56        \u001b[36m0.3412\u001b[0m  0.0406\n",
      "     57        \u001b[36m0.3370\u001b[0m  0.0411\n",
      "     58        \u001b[36m0.3326\u001b[0m  0.0406\n",
      "     59        \u001b[36m0.3183\u001b[0m  0.0401\n",
      "     60        0.3365  0.0436\n",
      "     61        \u001b[36m0.3152\u001b[0m  0.0406\n",
      "     62        \u001b[36m0.3015\u001b[0m  0.0411\n",
      "     63        0.3138  0.0406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     64        \u001b[36m0.2912\u001b[0m  0.0431\n",
      "     65        0.2947  0.0456\n",
      "     66        \u001b[36m0.2872\u001b[0m  0.0456\n",
      "     67        \u001b[36m0.2855\u001b[0m  0.0461\n",
      "     68        \u001b[36m0.2755\u001b[0m  0.0481\n",
      "     69        \u001b[36m0.2656\u001b[0m  0.0456\n",
      "     70        0.2863  0.0476\n",
      "     71        \u001b[36m0.2623\u001b[0m  0.0456\n",
      "     72        \u001b[36m0.2577\u001b[0m  0.0441\n",
      "     73        \u001b[36m0.2569\u001b[0m  0.0446\n",
      "     74        \u001b[36m0.2530\u001b[0m  0.0436\n",
      "     75        \u001b[36m0.2402\u001b[0m  0.0466\n",
      "     76        0.2418  0.0451\n",
      "     77        \u001b[36m0.2352\u001b[0m  0.0441\n",
      "     78        0.2367  0.0511\n",
      "     79        \u001b[36m0.2275\u001b[0m  0.0461\n",
      "     80        \u001b[36m0.2260\u001b[0m  0.0441\n",
      "     81        \u001b[36m0.2215\u001b[0m  0.0451\n",
      "     82        \u001b[36m0.2204\u001b[0m  0.0451\n",
      "     83        \u001b[36m0.2143\u001b[0m  0.0431\n",
      "     84        \u001b[36m0.2139\u001b[0m  0.0471\n",
      "     85        0.2177  0.0446\n",
      "     86        \u001b[36m0.2054\u001b[0m  0.0411\n",
      "     87        0.2072  0.0461\n",
      "     88        \u001b[36m0.2028\u001b[0m  0.0461\n",
      "     89        0.2031  0.0732\n",
      "     90        \u001b[36m0.1966\u001b[0m  0.0416\n",
      "     91        \u001b[36m0.1957\u001b[0m  0.0416\n",
      "     92        \u001b[36m0.1908\u001b[0m  0.0401\n",
      "     93        0.1974  0.0416\n",
      "     94        \u001b[36m0.1873\u001b[0m  0.0411\n",
      "     95        0.1877  0.0431\n",
      "     96        \u001b[36m0.1830\u001b[0m  0.0411\n",
      "     97        0.1836  0.0410\n",
      "     98        \u001b[36m0.1785\u001b[0m  0.0416\n",
      "     99        0.1786  0.0401\n",
      "    100        \u001b[36m0.1765\u001b[0m  0.0418\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1429\u001b[0m  0.0305\n",
      "      2       37.1429  0.0295\n",
      "      3       37.1429  0.0301\n",
      "      4       37.1429  0.0305\n",
      "      5       37.1429  0.0295\n",
      "      6       37.1429  0.0290\n",
      "      7       37.1429  0.0305\n",
      "      8       37.1429  0.0305\n",
      "      9       37.1429  0.0311\n",
      "     10       37.1429  0.0305\n",
      "     11       37.1429  0.0295\n",
      "     12       37.1429  0.0301\n",
      "     13       37.1429  0.0305\n",
      "     14       37.1429  0.0315\n",
      "     15       37.1429  0.0330\n",
      "     16       37.1429  0.0321\n",
      "     17       37.1429  0.0305\n",
      "     18       37.1429  0.0331\n",
      "     19       37.1429  0.0306\n",
      "     20       37.1429  0.0316\n",
      "     21       37.1429  0.0305\n",
      "     22       37.1429  0.0301\n",
      "     23       37.1429  0.0325\n",
      "     24       37.1429  0.0306\n",
      "     25       37.1429  0.0311\n",
      "     26       37.1429  0.0305\n",
      "     27       37.1429  0.0295\n",
      "     28       37.1429  0.0290\n",
      "     29       37.1429  0.0295\n",
      "     30       37.1429  0.0305\n",
      "     31       37.1429  0.0301\n",
      "     32       37.1429  0.0305\n",
      "     33       37.1429  0.0306\n",
      "     34       37.1429  0.0321\n",
      "     35       37.1429  0.0306\n",
      "     36       37.1429  0.0305\n",
      "     37       37.1429  0.0301\n",
      "     38       37.1429  0.0331\n",
      "     39       37.1429  0.0306\n",
      "     40       37.1429  0.0295\n",
      "     41       37.1429  0.0311\n",
      "     42       37.1429  0.0305\n",
      "     43       37.1429  0.0315\n",
      "     44       37.1429  0.0311\n",
      "     45       37.1429  0.0316\n",
      "     46       37.1429  0.0305\n",
      "     47       37.1429  0.0311\n",
      "     48       37.1429  0.0306\n",
      "     49       37.1429  0.0336\n",
      "     50       37.1429  0.0321\n",
      "     51       37.1429  0.0305\n",
      "     52       37.1429  0.0305\n",
      "     53       37.1429  0.0305\n",
      "     54       37.1429  0.0305\n",
      "     55       37.1429  0.0306\n",
      "     56       37.1429  0.0305\n",
      "     57       37.1429  0.0315\n",
      "     58       37.1429  0.0315\n",
      "     59       37.1429  0.0311\n",
      "     60       37.1429  0.0305\n",
      "     61       37.1429  0.0305\n",
      "     62       37.1429  0.0305\n",
      "     63       37.1429  0.0301\n",
      "     64       37.1429  0.0305\n",
      "     65       37.1429  0.0311\n",
      "     66       37.1429  0.0300\n",
      "     67       37.1429  0.0295\n",
      "     68       37.1429  0.0306\n",
      "     69       37.1429  0.0291\n",
      "     70       37.1429  0.0305\n",
      "     71       37.1429  0.0306\n",
      "     72       37.1429  0.0305\n",
      "     73       37.1429  0.0305\n",
      "     74       37.1429  0.0305\n",
      "     75       37.1429  0.0305\n",
      "     76       37.1429  0.0306\n",
      "     77       37.1429  0.0295\n",
      "     78       37.1429  0.0305\n",
      "     79       37.1429  0.0310\n",
      "     80       37.1429  0.0305\n",
      "     81       37.1429  0.0346\n",
      "     82       37.1429  0.0321\n",
      "     83       37.1429  0.0305\n",
      "     84       37.1429  0.0315\n",
      "     85       37.1429  0.0306\n",
      "     86       37.1429  0.0295\n",
      "     87       37.1429  0.0305\n",
      "     88       37.1429  0.0341\n",
      "     89       37.1429  0.0535\n",
      "     90       37.1429  0.0386\n",
      "     91       37.1429  0.0341\n",
      "     92       37.1429  0.0326\n",
      "     93       37.1429  0.0333\n",
      "     94       37.1429  0.0326\n",
      "     95       37.1429  0.0361\n",
      "     96       37.1429  0.0331\n",
      "     97       37.1429  0.0316\n",
      "     98       37.1429  0.0295\n",
      "     99       37.1429  0.0316\n",
      "    100       37.1429  0.0305\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.1429\u001b[0m  0.0295\n",
      "      2       37.1429  0.0311\n",
      "      3       37.1429  0.0336\n",
      "      4       37.1429  0.0305\n",
      "      5       37.1429  0.0311\n",
      "      6       37.1429  0.0306\n",
      "      7       37.1429  0.0316\n",
      "      8       37.1429  0.0300\n",
      "      9       37.1429  0.0315\n",
      "     10       37.1429  0.0310\n",
      "     11       37.1429  0.0321\n",
      "     12       37.1429  0.0313\n",
      "     13       37.1429  0.0295\n",
      "     14       37.1429  0.0300\n",
      "     15       37.1429  0.0310\n",
      "     16       37.1429  0.0326\n",
      "     17       37.1429  0.0301\n",
      "     18       37.1429  0.0305\n",
      "     19       37.1429  0.0305\n",
      "     20       37.1429  0.0306\n",
      "     21       37.1429  0.0306\n",
      "     22       37.1429  0.0315\n",
      "     23       37.1429  0.0306\n",
      "     24       37.1429  0.0316\n",
      "     25       37.1429  0.0295\n",
      "     26       37.1429  0.0305\n",
      "     27       37.1429  0.0321\n",
      "     28       37.1429  0.0305\n",
      "     29       37.1429  0.0305\n",
      "     30       37.1429  0.0310\n",
      "     31       37.1429  0.0305\n",
      "     32       37.1429  0.0305\n",
      "     33       37.1429  0.0301\n",
      "     34       37.1429  0.0305\n",
      "     35       37.1429  0.0306\n",
      "     36       37.1429  0.0300\n",
      "     37       37.1429  0.0295\n",
      "     38       37.1429  0.0310\n",
      "     39       37.1429  0.0306\n",
      "     40       37.1429  0.0301\n",
      "     41       37.1429  0.0315\n",
      "     42       37.1429  0.0386\n",
      "     43       37.1429  0.0316\n",
      "     44       37.1429  0.0315\n",
      "     45       37.1429  0.0315\n",
      "     46       37.1429  0.0305\n",
      "     47       37.1429  0.0305\n",
      "     48       37.1429  0.0305\n",
      "     49       37.1429  0.0300\n",
      "     50       37.1429  0.0331\n",
      "     51       37.1429  0.0305\n",
      "     52       37.1429  0.0300\n",
      "     53       37.1429  0.0305\n",
      "     54       37.1429  0.0306\n",
      "     55       37.1429  0.0301\n",
      "     56       37.1429  0.0311\n",
      "     57       37.1429  0.0305\n",
      "     58       37.1429  0.0310\n",
      "     59       37.1429  0.0305\n",
      "     60       37.1429  0.0305\n",
      "     61       37.1429  0.0290\n",
      "     62       37.1429  0.0305\n",
      "     63       37.1429  0.0300\n",
      "     64       37.1429  0.0305\n",
      "     65       37.1429  0.0300\n",
      "     66       37.1429  0.0305\n",
      "     67       37.1429  0.0305\n",
      "     68       37.1429  0.0301\n",
      "     69       37.1429  0.0315\n",
      "     70       37.1429  0.0316\n",
      "     71       37.1429  0.0310\n",
      "     72       37.1429  0.0315\n",
      "     73       37.1429  0.0305\n",
      "     74       37.1429  0.0306\n",
      "     75       37.1429  0.0316\n",
      "     76       37.1429  0.0305\n",
      "     77       37.1429  0.0300\n",
      "     78       37.1429  0.0305\n",
      "     79       37.1429  0.0341\n",
      "     80       37.1429  0.0311\n",
      "     81       37.1429  0.0306\n",
      "     82       37.1429  0.0326\n",
      "     83       37.1429  0.0316\n",
      "     84       37.1429  0.0356\n",
      "     85       37.1429  0.0331\n",
      "     86       37.1429  0.0311\n",
      "     87       37.1429  0.0315\n",
      "     88       37.1429  0.0305\n",
      "     89       37.1429  0.0301\n",
      "     90       37.1429  0.0320\n",
      "     91       37.1429  0.0306\n",
      "     92       37.1429  0.0305\n",
      "     93       37.1429  0.0296\n",
      "     94       37.1429  0.0305\n",
      "     95       37.1429  0.0296\n",
      "     96       37.1429  0.0301\n",
      "     97       37.1429  0.0321\n",
      "     98       37.1429  0.0305\n",
      "     99       37.1429  0.0317\n",
      "    100       37.1429  0.0306\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0306\n",
      "      2       37.3626  0.0311\n",
      "      3       37.3626  0.0321\n",
      "      4       37.3626  0.0305\n",
      "      5       37.3626  0.0321\n",
      "      6       37.3626  0.0335\n",
      "      7       37.3626  0.0336\n",
      "      8       37.3626  0.0321\n",
      "      9       37.3626  0.0296\n",
      "     10       37.3626  0.0295\n",
      "     11       37.3626  0.0311\n",
      "     12       37.3626  0.0295\n",
      "     13       37.3626  0.0295\n",
      "     14       37.3626  0.0311\n",
      "     15       37.3626  0.0310\n",
      "     16       37.3626  0.0295\n",
      "     17       37.3626  0.0321\n",
      "     18       37.3626  0.0316\n",
      "     19       37.3626  0.0305\n",
      "     20       37.3626  0.0316\n",
      "     21       37.3626  0.0326\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     22       37.3626  0.0305\n",
      "     23       37.3626  0.0315\n",
      "     24       37.3626  0.0306\n",
      "     25       37.3626  0.0311\n",
      "     26       37.3626  0.0305\n",
      "     27       37.3626  0.0301\n",
      "     28       37.3626  0.0305\n",
      "     29       37.3626  0.0305\n",
      "     30       37.3626  0.0301\n",
      "     31       37.3626  0.0306\n",
      "     32       37.3626  0.0306\n",
      "     33       37.3626  0.0311\n",
      "     34       37.3626  0.0306\n",
      "     35       37.3626  0.0305\n",
      "     36       37.3626  0.0300\n",
      "     37       37.3626  0.0315\n",
      "     38       37.3626  0.0306\n",
      "     39       37.3626  0.0331\n",
      "     40       37.3626  0.0315\n",
      "     41       37.3626  0.0306\n",
      "     42       37.3626  0.0300\n",
      "     43       37.3626  0.0305\n",
      "     44       37.3626  0.0305\n",
      "     45       37.3626  0.0305\n",
      "     46       37.3626  0.0301\n",
      "     47       37.3626  0.0306\n",
      "     48       37.3626  0.0305\n",
      "     49       37.3626  0.0316\n",
      "     50       37.3626  0.0305\n",
      "     51       37.3626  0.0305\n",
      "     52       37.3626  0.0311\n",
      "     53       37.3626  0.0295\n",
      "     54       37.3626  0.0305\n",
      "     55       37.3626  0.0305\n",
      "     56       37.3626  0.0300\n",
      "     57       37.3626  0.0295\n",
      "     58       37.3626  0.0306\n",
      "     59       37.3626  0.0305\n",
      "     60       37.3626  0.0315\n",
      "     61       37.3626  0.0295\n",
      "     62       37.3626  0.0351\n",
      "     63       37.3626  0.0316\n",
      "     64       37.3626  0.0331\n",
      "     65       37.3626  0.0326\n",
      "     66       37.3626  0.0326\n",
      "     67       37.3626  0.0321\n",
      "     68       37.3626  0.0331\n",
      "     69       37.3626  0.0361\n",
      "     70       37.3626  0.0311\n",
      "     71       37.3626  0.0305\n",
      "     72       37.3626  0.0315\n",
      "     73       37.3626  0.0320\n",
      "     74       37.3626  0.0315\n",
      "     75       37.3626  0.0315\n",
      "     76       37.3626  0.0295\n",
      "     77       37.3626  0.0305\n",
      "     78       37.3626  0.0305\n",
      "     79       37.3626  0.0305\n",
      "     80       37.3626  0.0311\n",
      "     81       37.3626  0.0305\n",
      "     82       37.3626  0.0305\n",
      "     83       37.3626  0.0325\n",
      "     84       37.3626  0.0305\n",
      "     85       37.3626  0.0321\n",
      "     86       37.3626  0.0305\n",
      "     87       37.3626  0.0305\n",
      "     88       37.3626  0.0305\n",
      "     89       37.3626  0.0305\n",
      "     90       37.3626  0.0305\n",
      "     91       37.3626  0.0321\n",
      "     92       37.3626  0.0311\n",
      "     93       37.3626  0.0315\n",
      "     94       37.3626  0.0305\n",
      "     95       37.3626  0.0315\n",
      "     96       37.3626  0.0306\n",
      "     97       37.3626  0.0326\n",
      "     98       37.3626  0.0316\n",
      "     99       37.3626  0.0295\n",
      "    100       37.3626  0.0341\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0305\n",
      "      2       37.3626  0.0326\n",
      "      3       37.3626  0.0305\n",
      "      4       37.3626  0.0305\n",
      "      5       37.3626  0.0301\n",
      "      6       37.3626  0.0306\n",
      "      7       37.3626  0.0315\n",
      "      8       37.3626  0.0301\n",
      "      9       37.3626  0.0305\n",
      "     10       37.3626  0.0305\n",
      "     11       37.3626  0.0291\n",
      "     12       37.3626  0.0315\n",
      "     13       37.3626  0.0305\n",
      "     14       37.3626  0.0306\n",
      "     15       37.3626  0.0316\n",
      "     16       37.3626  0.0306\n",
      "     17       37.3626  0.0295\n",
      "     18       37.3626  0.0295\n",
      "     19       37.3626  0.0321\n",
      "     20       37.3626  0.0300\n",
      "     21       37.3626  0.0306\n",
      "     22       37.3626  0.0305\n",
      "     23       37.3626  0.0305\n",
      "     24       37.3626  0.0310\n",
      "     25       37.3626  0.0306\n",
      "     26       37.3626  0.0305\n",
      "     27       37.3626  0.0301\n",
      "     28       37.3626  0.0295\n",
      "     29       37.3626  0.0315\n",
      "     30       37.3626  0.0300\n",
      "     31       37.3626  0.0321\n",
      "     32       37.3626  0.0316\n",
      "     33       37.3626  0.0311\n",
      "     34       37.3626  0.0295\n",
      "     35       37.3626  0.0305\n",
      "     36       37.3626  0.0301\n",
      "     37       37.3626  0.0295\n",
      "     38       37.3626  0.0321\n",
      "     39       37.3626  0.0310\n",
      "     40       37.3626  0.0315\n",
      "     41       37.3626  0.0306\n",
      "     42       37.3626  0.0305\n",
      "     43       37.3626  0.0301\n",
      "     44       37.3626  0.0295\n",
      "     45       37.3626  0.0316\n",
      "     46       37.3626  0.0311\n",
      "     47       37.3626  0.0305\n",
      "     48       37.3626  0.0305\n",
      "     49       37.3626  0.0300\n",
      "     50       37.3626  0.0295\n",
      "     51       37.3626  0.0305\n",
      "     52       37.3626  0.0300\n",
      "     53       37.3626  0.0296\n",
      "     54       37.3626  0.0305\n",
      "     55       37.3626  0.0301\n",
      "     56       37.3626  0.0295\n",
      "     57       37.3626  0.0306\n",
      "     58       37.3626  0.0305\n",
      "     59       37.3626  0.0316\n",
      "     60       37.3626  0.0306\n",
      "     61       37.3626  0.0305\n",
      "     62       37.3626  0.0300\n",
      "     63       37.3626  0.0305\n",
      "     64       37.3626  0.0306\n",
      "     65       37.3626  0.0321\n",
      "     66       37.3626  0.0305\n",
      "     67       37.3626  0.0305\n",
      "     68       37.3626  0.0300\n",
      "     69       37.3626  0.0305\n",
      "     70       37.3626  0.0311\n",
      "     71       37.3626  0.0331\n",
      "     72       37.3626  0.0295\n",
      "     73       37.3626  0.0305\n",
      "     74       37.3626  0.0300\n",
      "     75       37.3626  0.0295\n",
      "     76       37.3626  0.0300\n",
      "     77       37.3626  0.0305\n",
      "     78       37.3626  0.0311\n",
      "     79       37.3626  0.0316\n",
      "     80       37.3626  0.0316\n",
      "     81       37.3626  0.0300\n",
      "     82       37.3626  0.0296\n",
      "     83       37.3626  0.0305\n",
      "     84       37.3626  0.0300\n",
      "     85       37.3626  0.0306\n",
      "     86       37.3626  0.0295\n",
      "     87       37.3626  0.0300\n",
      "     88       37.3626  0.0305\n",
      "     89       37.3626  0.0305\n",
      "     90       37.3626  0.0316\n",
      "     91       37.3626  0.0306\n",
      "     92       37.3626  0.0305\n",
      "     93       37.3626  0.0356\n",
      "     94       37.3626  0.0316\n",
      "     95       37.3626  0.0315\n",
      "     96       37.3626  0.0331\n",
      "     97       37.3626  0.0301\n",
      "     98       37.3626  0.0306\n",
      "     99       37.3626  0.0361\n",
      "    100       37.3626  0.0306\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2807\u001b[0m  0.0285\n",
      "      2       37.2807  0.0301\n",
      "      3       37.2807  0.0305\n",
      "      4       37.2807  0.0295\n",
      "      5       37.2807  0.0361\n",
      "      6       37.2807  0.0321\n",
      "      7       37.2807  0.0471\n",
      "      8       37.2807  0.0552\n",
      "      9       37.2807  0.0446\n",
      "     10       37.2807  0.0552\n",
      "     11       37.2807  0.0456\n",
      "     12       37.2807  0.0366\n",
      "     13       37.2807  0.0451\n",
      "     14       37.2807  0.0487\n",
      "     15       37.2807  0.0481\n",
      "     16       37.2807  0.0456\n",
      "     17       37.2807  0.0562\n",
      "     18       37.2807  0.0461\n",
      "     19       37.2807  0.0476\n",
      "     20       37.2807  0.0607\n",
      "     21       37.2807  0.0486\n",
      "     22       37.2807  0.0552\n",
      "     23       37.2807  0.0557\n",
      "     24       37.2807  0.0376\n",
      "     25       37.2807  0.0351\n",
      "     26       37.2807  0.0351\n",
      "     27       37.2807  0.0371\n",
      "     28       37.2807  0.0346\n",
      "     29       37.2807  0.0341\n",
      "     30       37.2807  0.0351\n",
      "     31       37.2807  0.0346\n",
      "     32       37.2807  0.0361\n",
      "     33       37.2807  0.0336\n",
      "     34       37.2807  0.0341\n",
      "     35       37.2807  0.0321\n",
      "     36       37.2807  0.0306\n",
      "     37       37.2807  0.0306\n",
      "     38       37.2807  0.0311\n",
      "     39       37.2807  0.0316\n",
      "     40       37.2807  0.0306\n",
      "     41       37.2807  0.0331\n",
      "     42       37.2807  0.0326\n",
      "     43       37.2807  0.0316\n",
      "     44       37.2807  0.0331\n",
      "     45       37.2807  0.0326\n",
      "     46       37.2807  0.0336\n",
      "     47       37.2807  0.0336\n",
      "     48       37.2807  0.0325\n",
      "     49       37.2807  0.0331\n",
      "     50       37.2807  0.0306\n",
      "     51       37.2807  0.0316\n",
      "     52       37.2807  0.0316\n",
      "     53       37.2807  0.0356\n",
      "     54       37.2807  0.0341\n",
      "     55       37.2807  0.0536\n",
      "     56       37.2807  0.0497\n",
      "     57       37.2807  0.0587\n",
      "     58       37.2807  0.0572\n",
      "     59       37.2807  0.0473\n",
      "     60       37.2807  0.0316\n",
      "     61       37.2807  0.0356\n",
      "     62       37.2807  0.0381\n",
      "     63       37.2807  0.0341\n",
      "     64       37.2807  0.0341\n",
      "     65       37.2807  0.0331\n",
      "     66       37.2807  0.0326\n",
      "     67       37.2807  0.0501\n",
      "     68       37.2807  0.0315\n",
      "     69       37.2807  0.0336\n",
      "     70       37.2807  0.0335\n",
      "     71       37.2807  0.0311\n",
      "     72       37.2807  0.0376\n",
      "     73       37.2807  0.0336\n",
      "     74       37.2807  0.0305\n",
      "     75       37.2807  0.0326\n",
      "     76       37.2807  0.0331\n",
      "     77       37.2807  0.0316\n",
      "     78       37.2807  0.0321\n",
      "     79       37.2807  0.0315\n",
      "     80       37.2807  0.0321\n",
      "     81       37.2807  0.0311\n",
      "     82       37.2807  0.0315\n",
      "     83       37.2807  0.0306\n",
      "     84       37.2807  0.0321\n",
      "     85       37.2807  0.0316\n",
      "     86       37.2807  0.0315\n",
      "     87       37.2807  0.0305\n",
      "     88       37.2807  0.0331\n",
      "     89       37.2807  0.0305\n",
      "     90       37.2807  0.0305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     91       37.2807  0.0311\n",
      "     92       37.2807  0.0321\n",
      "     93       37.2807  0.0316\n",
      "     94       37.2807  0.0321\n",
      "     95       37.2807  0.0321\n",
      "     96       37.2807  0.0336\n",
      "     97       37.2807  0.0321\n",
      "     98       37.2807  0.0326\n",
      "     99       37.2807  0.0326\n",
      "    100       37.2807  0.0321\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m56.2637\u001b[0m  0.0411\n",
      "      2       56.2637  0.0436\n",
      "      3       56.2637  0.0486\n",
      "      4       56.4835  0.0426\n",
      "      5       56.4835  0.0411\n",
      "      6       56.7033  0.0411\n",
      "      7       56.7033  0.0416\n",
      "      8       56.7033  0.0411\n",
      "      9       56.7033  0.0416\n",
      "     10       56.7033  0.0426\n",
      "     11       56.2658  0.0426\n",
      "     12       56.2637  0.0411\n",
      "     13       \u001b[36m55.8465\u001b[0m  0.0421\n",
      "     14       \u001b[36m55.8242\u001b[0m  0.0416\n",
      "     15       55.8242  0.0441\n",
      "     16       55.8242  0.0421\n",
      "     17       55.8242  0.0421\n",
      "     18       55.8242  0.0406\n",
      "     19       55.8242  0.0421\n",
      "     20       55.8242  0.0426\n",
      "     21       55.8242  0.0416\n",
      "     22       55.8242  0.0431\n",
      "     23       55.8242  0.0416\n",
      "     24       55.8242  0.0421\n",
      "     25       55.8242  0.0426\n",
      "     26       55.8242  0.0421\n",
      "     27       55.8242  0.0411\n",
      "     28       55.8242  0.0501\n",
      "     29       55.8242  0.0496\n",
      "     30       56.0647  0.0416\n",
      "     31       56.1247  0.0421\n",
      "     32       56.1031  0.0411\n",
      "     33       55.8242  0.0411\n",
      "     34       55.8242  0.0406\n",
      "     35       55.8242  0.0411\n",
      "     36       55.8242  0.0426\n",
      "     37       55.8242  0.0428\n",
      "     38       \u001b[36m55.7991\u001b[0m  0.0411\n",
      "     39       \u001b[36m55.7844\u001b[0m  0.0426\n",
      "     40       \u001b[36m55.7699\u001b[0m  0.0401\n",
      "     41       \u001b[36m55.7557\u001b[0m  0.0406\n",
      "     42       \u001b[36m55.7417\u001b[0m  0.0421\n",
      "     43       \u001b[36m55.7280\u001b[0m  0.0411\n",
      "     44       \u001b[36m55.7144\u001b[0m  0.0421\n",
      "     45       \u001b[36m55.7012\u001b[0m  0.0441\n",
      "     46       \u001b[36m55.6881\u001b[0m  0.0431\n",
      "     47       \u001b[36m55.6674\u001b[0m  0.0411\n",
      "     48       \u001b[36m55.0041\u001b[0m  0.0416\n",
      "     49       \u001b[36m54.7945\u001b[0m  0.0401\n",
      "     50       \u001b[36m54.7253\u001b[0m  0.0401\n",
      "     51       \u001b[36m54.7253\u001b[0m  0.0406\n",
      "     52       54.7253  0.0406\n",
      "     53       54.7253  0.0406\n",
      "     54       54.7253  0.0401\n",
      "     55       \u001b[36m54.7253\u001b[0m  0.0390\n",
      "     56       54.7253  0.0406\n",
      "     57       54.7253  0.0401\n",
      "     58       54.7253  0.0406\n",
      "     59       \u001b[36m54.7253\u001b[0m  0.0416\n",
      "     60       54.7253  0.0406\n",
      "     61       54.7253  0.0411\n",
      "     62       54.7253  0.0391\n",
      "     63       54.7253  0.0406\n",
      "     64       54.7253  0.0391\n",
      "     65       54.7253  0.0396\n",
      "     66       54.7253  0.0411\n",
      "     67       54.7253  0.0436\n",
      "     68       54.7253  0.0406\n",
      "     69       54.7253  0.0401\n",
      "     70       54.7253  0.0396\n",
      "     71       54.7253  0.0411\n",
      "     72       54.7253  0.0406\n",
      "     73       54.7253  0.0436\n",
      "     74       54.7253  0.0430\n",
      "     75       54.7253  0.0411\n",
      "     76       54.7253  0.0401\n",
      "     77       54.7253  0.0406\n",
      "     78       54.7253  0.0401\n",
      "     79       54.7253  0.0401\n",
      "     80       54.7253  0.0391\n",
      "     81       54.7253  0.0401\n",
      "     82       54.7253  0.0401\n",
      "     83       54.7253  0.0401\n",
      "     84       54.7253  0.0401\n",
      "     85       54.7253  0.0396\n",
      "     86       54.7253  0.0401\n",
      "     87       54.7253  0.0406\n",
      "     88       54.7253  0.0401\n",
      "     89       54.7253  0.0406\n",
      "     90       54.7253  0.0390\n",
      "     91       54.7253  0.0411\n",
      "     92       54.7253  0.0406\n",
      "     93       54.7253  0.0401\n",
      "     94       54.7253  0.0411\n",
      "     95       54.7253  0.0411\n",
      "     96       54.7253  0.0401\n",
      "     97       54.7253  0.0411\n",
      "     98       54.7253  0.0421\n",
      "     99       54.7253  0.0400\n",
      "    100       54.7253  0.0426\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m41.1344\u001b[0m  0.0386\n",
      "      2       \u001b[36m40.6521\u001b[0m  0.0400\n",
      "      3       40.6823  0.0411\n",
      "      4       \u001b[36m40.6358\u001b[0m  0.0401\n",
      "      5       \u001b[36m40.4608\u001b[0m  0.0401\n",
      "      6       \u001b[36m40.0642\u001b[0m  0.0396\n",
      "      7       \u001b[36m39.6036\u001b[0m  0.0413\n",
      "      8       \u001b[36m39.5791\u001b[0m  0.0401\n",
      "      9       39.5811  0.0406\n",
      "     10       39.5803  0.0421\n",
      "     11       39.5794  0.0416\n",
      "     12       39.5797  0.0401\n",
      "     13       39.5793  0.0401\n",
      "     14       39.5792  0.0401\n",
      "     15       \u001b[36m39.5790\u001b[0m  0.0416\n",
      "     16       \u001b[36m39.5788\u001b[0m  0.0391\n",
      "     17       \u001b[36m39.5786\u001b[0m  0.0401\n",
      "     18       \u001b[36m39.5784\u001b[0m  0.0401\n",
      "     19       \u001b[36m39.5783\u001b[0m  0.0411\n",
      "     20       \u001b[36m39.5779\u001b[0m  0.0401\n",
      "     21       39.5784  0.0426\n",
      "     22       39.5976  0.0416\n",
      "     23       \u001b[36m39.5763\u001b[0m  0.0426\n",
      "     24       39.5780  0.0401\n",
      "     25       \u001b[36m39.5759\u001b[0m  0.0406\n",
      "     26       39.5768  0.0391\n",
      "     27       39.5761  0.0401\n",
      "     28       39.5762  0.0386\n",
      "     29       39.5760  0.0401\n",
      "     30       \u001b[36m39.5759\u001b[0m  0.0391\n",
      "     31       \u001b[36m39.5757\u001b[0m  0.0391\n",
      "     32       \u001b[36m39.5756\u001b[0m  0.0390\n",
      "     33       \u001b[36m39.5754\u001b[0m  0.0401\n",
      "     34       \u001b[36m39.5752\u001b[0m  0.0456\n",
      "     35       \u001b[36m39.5751\u001b[0m  0.0396\n",
      "     36       \u001b[36m39.5749\u001b[0m  0.0400\n",
      "     37       \u001b[36m39.5748\u001b[0m  0.0396\n",
      "     38       \u001b[36m39.5746\u001b[0m  0.0401\n",
      "     39       \u001b[36m39.5745\u001b[0m  0.0401\n",
      "     40       \u001b[36m39.5743\u001b[0m  0.0416\n",
      "     41       \u001b[36m39.5741\u001b[0m  0.0401\n",
      "     42       \u001b[36m39.5740\u001b[0m  0.0421\n",
      "     43       \u001b[36m39.5738\u001b[0m  0.0411\n",
      "     44       \u001b[36m39.5737\u001b[0m  0.0411\n",
      "     45       \u001b[36m39.5735\u001b[0m  0.0406\n",
      "     46       \u001b[36m39.5733\u001b[0m  0.0436\n",
      "     47       \u001b[36m39.5732\u001b[0m  0.0406\n",
      "     48       39.5733  0.0391\n",
      "     49       \u001b[36m39.5728\u001b[0m  0.0416\n",
      "     50       \u001b[36m39.5727\u001b[0m  0.0391\n",
      "     51       \u001b[36m39.5725\u001b[0m  0.0401\n",
      "     52       \u001b[36m39.5723\u001b[0m  0.0406\n",
      "     53       \u001b[36m39.5722\u001b[0m  0.0401\n",
      "     54       \u001b[36m39.5720\u001b[0m  0.0406\n",
      "     55       \u001b[36m39.5718\u001b[0m  0.0411\n",
      "     56       \u001b[36m39.5717\u001b[0m  0.0401\n",
      "     57       \u001b[36m39.5715\u001b[0m  0.0406\n",
      "     58       \u001b[36m39.5714\u001b[0m  0.0426\n",
      "     59       \u001b[36m39.5712\u001b[0m  0.0416\n",
      "     60       \u001b[36m39.5710\u001b[0m  0.0401\n",
      "     61       \u001b[36m39.5709\u001b[0m  0.0406\n",
      "     62       \u001b[36m39.5707\u001b[0m  0.0401\n",
      "     63       \u001b[36m39.5706\u001b[0m  0.0411\n",
      "     64       \u001b[36m39.5704\u001b[0m  0.0406\n",
      "     65       \u001b[36m39.5703\u001b[0m  0.0401\n",
      "     66       \u001b[36m39.5701\u001b[0m  0.0416\n",
      "     67       \u001b[36m39.5700\u001b[0m  0.0401\n",
      "     68       \u001b[36m39.5698\u001b[0m  0.0466\n",
      "     69       \u001b[36m39.5697\u001b[0m  0.0416\n",
      "     70       \u001b[36m39.5696\u001b[0m  0.0431\n",
      "     71       \u001b[36m39.5694\u001b[0m  0.0446\n",
      "     72       \u001b[36m39.5693\u001b[0m  0.0411\n",
      "     73       \u001b[36m39.5691\u001b[0m  0.0406\n",
      "     74       \u001b[36m39.5690\u001b[0m  0.0441\n",
      "     75       \u001b[36m39.5688\u001b[0m  0.0400\n",
      "     76       \u001b[36m39.5687\u001b[0m  0.0401\n",
      "     77       \u001b[36m39.5685\u001b[0m  0.0411\n",
      "     78       \u001b[36m39.5684\u001b[0m  0.0406\n",
      "     79       \u001b[36m39.5683\u001b[0m  0.0411\n",
      "     80       \u001b[36m39.5681\u001b[0m  0.0416\n",
      "     81       \u001b[36m39.5680\u001b[0m  0.0421\n",
      "     82       \u001b[36m39.5679\u001b[0m  0.0411\n",
      "     83       \u001b[36m39.5678\u001b[0m  0.0406\n",
      "     84       \u001b[36m39.5676\u001b[0m  0.0411\n",
      "     85       \u001b[36m39.5675\u001b[0m  0.0406\n",
      "     86       \u001b[36m39.5674\u001b[0m  0.0401\n",
      "     87       \u001b[36m39.5673\u001b[0m  0.0416\n",
      "     88       \u001b[36m39.5672\u001b[0m  0.0406\n",
      "     89       \u001b[36m39.5671\u001b[0m  0.0411\n",
      "     90       \u001b[36m39.5670\u001b[0m  0.0406\n",
      "     91       \u001b[36m39.5669\u001b[0m  0.0411\n",
      "     92       \u001b[36m39.5669\u001b[0m  0.0416\n",
      "     93       \u001b[36m39.5668\u001b[0m  0.0436\n",
      "     94       \u001b[36m39.5667\u001b[0m  0.0426\n",
      "     95       \u001b[36m39.5667\u001b[0m  0.0411\n",
      "     96       \u001b[36m39.5666\u001b[0m  0.0421\n",
      "     97       \u001b[36m39.5666\u001b[0m  0.0406\n",
      "     98       \u001b[36m39.5665\u001b[0m  0.0411\n",
      "     99       \u001b[36m39.5665\u001b[0m  0.0461\n",
      "    100       \u001b[36m39.5665\u001b[0m  0.0401\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m58.4003\u001b[0m  0.0396\n",
      "      2       \u001b[36m58.2545\u001b[0m  0.0411\n",
      "      3       \u001b[36m57.3262\u001b[0m  0.0391\n",
      "      4       \u001b[36m56.4367\u001b[0m  0.0396\n",
      "      5       \u001b[36m48.4946\u001b[0m  0.0421\n",
      "      6       \u001b[36m41.3948\u001b[0m  0.0401\n",
      "      7       46.1256  0.0390\n",
      "      8       42.8985  0.0401\n",
      "      9       \u001b[36m40.9303\u001b[0m  0.0396\n",
      "     10       \u001b[36m37.1444\u001b[0m  0.0401\n",
      "     11       37.3642  0.0406\n",
      "     12       37.3642  0.0411\n",
      "     13       37.3642  0.0396\n",
      "     14       37.3641  0.0426\n",
      "     15       37.3641  0.0411\n",
      "     16       37.3641  0.0396\n",
      "     17       37.3641  0.0431\n",
      "     18       37.3634  0.0441\n",
      "     19       37.3626  0.0421\n",
      "     20       37.3626  0.0391\n",
      "     21       37.3626  0.0401\n",
      "     22       37.3626  0.0391\n",
      "     23       37.3626  0.0406\n",
      "     24       37.3626  0.0446\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     25       37.3626  0.0406\n",
      "     26       37.3626  0.0401\n",
      "     27       37.3626  0.0401\n",
      "     28       37.3626  0.0396\n",
      "     29       37.3626  0.0401\n",
      "     30       37.3626  0.0431\n",
      "     31       37.3626  0.0391\n",
      "     32       37.3626  0.0391\n",
      "     33       37.3626  0.0406\n",
      "     34       37.3626  0.0401\n",
      "     35       37.3626  0.0416\n",
      "     36       37.3626  0.0401\n",
      "     37       37.3626  0.0406\n",
      "     38       37.3626  0.0411\n",
      "     39       37.3626  0.0401\n",
      "     40       37.3626  0.0411\n",
      "     41       37.3626  0.0411\n",
      "     42       37.3626  0.0426\n",
      "     43       37.3626  0.0431\n",
      "     44       37.3626  0.0401\n",
      "     45       37.3626  0.0406\n",
      "     46       37.3626  0.0401\n",
      "     47       37.3626  0.0406\n",
      "     48       37.3626  0.0391\n",
      "     49       37.3626  0.0406\n",
      "     50       37.3626  0.0401\n",
      "     51       37.3626  0.0401\n",
      "     52       37.3626  0.0401\n",
      "     53       37.3626  0.0401\n",
      "     54       37.3626  0.0431\n",
      "     55       37.3626  0.0411\n",
      "     56       37.3626  0.0408\n",
      "     57       37.3626  0.0396\n",
      "     58       37.3626  0.0400\n",
      "     59       37.3626  0.0401\n",
      "     60       37.3626  0.0401\n",
      "     61       37.3626  0.0401\n",
      "     62       37.3626  0.0421\n",
      "     63       37.3626  0.0411\n",
      "     64       37.3626  0.0401\n",
      "     65       37.3626  0.0411\n",
      "     66       37.3626  0.0421\n",
      "     67       37.3626  0.0426\n",
      "     68       37.3626  0.0401\n",
      "     69       37.3626  0.0411\n",
      "     70       37.3626  0.0401\n",
      "     71       37.3626  0.0416\n",
      "     72       37.3626  0.0401\n",
      "     73       37.3626  0.0401\n",
      "     74       37.3626  0.0401\n",
      "     75       37.3626  0.0421\n",
      "     76       37.3626  0.0400\n",
      "     77       37.3626  0.0466\n",
      "     78       37.3626  0.0421\n",
      "     79       37.3626  0.0401\n",
      "     80       37.3626  0.0401\n",
      "     81       37.3626  0.0396\n",
      "     82       37.3626  0.0411\n",
      "     83       37.3626  0.0386\n",
      "     84       37.3626  0.0401\n",
      "     85       37.3626  0.0416\n",
      "     86       37.3682  0.0421\n",
      "     87       37.3626  0.0401\n",
      "     88       37.3626  0.0416\n",
      "     89       37.3626  0.0401\n",
      "     90       37.3626  0.0411\n",
      "     91       37.3626  0.0496\n",
      "     92       37.3626  0.0411\n",
      "     93       37.3626  0.0411\n",
      "     94       37.3626  0.0401\n",
      "     95       37.3626  0.0406\n",
      "     96       37.3626  0.0401\n",
      "     97       37.3626  0.0405\n",
      "     98       37.3626  0.0411\n",
      "     99       37.3626  0.0401\n",
      "    100       37.3626  0.0406\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m62.6374\u001b[0m  0.0400\n",
      "      2       62.6374  0.0416\n",
      "      3       62.6374  0.0401\n",
      "      4       62.6374  0.0396\n",
      "      5       62.6374  0.0401\n",
      "      6       62.6374  0.0401\n",
      "      7       62.6374  0.0401\n",
      "      8       62.6374  0.0401\n",
      "      9       62.6374  0.0396\n",
      "     10       62.6374  0.0411\n",
      "     11       62.6374  0.0411\n",
      "     12       62.6374  0.0396\n",
      "     13       62.6374  0.0416\n",
      "     14       62.6374  0.0406\n",
      "     15       62.6374  0.0431\n",
      "     16       62.6374  0.0396\n",
      "     17       62.6374  0.0401\n",
      "     18       62.6374  0.0401\n",
      "     19       62.6374  0.0396\n",
      "     20       62.6374  0.0401\n",
      "     21       \u001b[36m62.6012\u001b[0m  0.0396\n",
      "     22       \u001b[36m62.4344\u001b[0m  0.0390\n",
      "     23       \u001b[36m61.8806\u001b[0m  0.0401\n",
      "     24       \u001b[36m61.2083\u001b[0m  0.0396\n",
      "     25       \u001b[36m52.5383\u001b[0m  0.0401\n",
      "     26       \u001b[36m13.3590\u001b[0m  0.0431\n",
      "     27        \u001b[36m7.8213\u001b[0m  0.0411\n",
      "     28        \u001b[36m5.0090\u001b[0m  0.0400\n",
      "     29        \u001b[36m4.5221\u001b[0m  0.0406\n",
      "     30        \u001b[36m4.0346\u001b[0m  0.0411\n",
      "     31        \u001b[36m3.5818\u001b[0m  0.0396\n",
      "     32        \u001b[36m2.9247\u001b[0m  0.0401\n",
      "     33        \u001b[36m2.8701\u001b[0m  0.0396\n",
      "     34        3.1767  0.0401\n",
      "     35        3.2757  0.0401\n",
      "     36        3.2977  0.0406\n",
      "     37        3.2834  0.0401\n",
      "     38        3.2875  0.0406\n",
      "     39        3.2950  0.0446\n",
      "     40        3.1234  0.0436\n",
      "     41        3.2588  0.0401\n",
      "     42        3.2658  0.0391\n",
      "     43        3.2610  0.0396\n",
      "     44        3.2635  0.0401\n",
      "     45        3.2630  0.0406\n",
      "     46        3.2828  0.0411\n",
      "     47        3.2776  0.0401\n",
      "     48        3.2730  0.0406\n",
      "     49        3.0825  0.0391\n",
      "     50        3.0451  0.0406\n",
      "     51        3.0714  0.0411\n",
      "     52        3.0634  0.0401\n",
      "     53        3.0475  0.0386\n",
      "     54        2.8876  0.0411\n",
      "     55        3.0366  0.0396\n",
      "     56        3.0350  0.0401\n",
      "     57        3.0401  0.0411\n",
      "     58        3.0371  0.0436\n",
      "     59        3.0291  0.0431\n",
      "     60        3.0169  0.0396\n",
      "     61        2.9939  0.0421\n",
      "     62        2.9906  0.0416\n",
      "     63        2.9975  0.0431\n",
      "     64        2.9750  0.0401\n",
      "     65        \u001b[36m1.4474\u001b[0m  0.0401\n",
      "     66        2.9853  0.0426\n",
      "     67        3.2366  0.0415\n",
      "     68        3.0099  0.0420\n",
      "     69        2.9885  0.0441\n",
      "     70        3.0066  0.0401\n",
      "     71        2.9966  0.0411\n",
      "     72        2.9856  0.0406\n",
      "     73        2.9837  0.0411\n",
      "     74        2.9730  0.0396\n",
      "     75        2.9605  0.0421\n",
      "     76        2.9640  0.0416\n",
      "     77        2.9465  0.0401\n",
      "     78        2.9396  0.0401\n",
      "     79        2.9372  0.0406\n",
      "     80        2.9489  0.0411\n",
      "     81        2.9473  0.0406\n",
      "     82        2.9426  0.0421\n",
      "     83        2.9395  0.0411\n",
      "     84        2.9385  0.0416\n",
      "     85        2.9353  0.0401\n",
      "     86        2.9331  0.0406\n",
      "     87        2.9309  0.0441\n",
      "     88        2.9286  0.0461\n",
      "     89        2.9271  0.0401\n",
      "     90        2.9246  0.0401\n",
      "     91        2.9202  0.0396\n",
      "     92        2.9204  0.0401\n",
      "     93        2.9185  0.0416\n",
      "     94        2.9165  0.0406\n",
      "     95        2.9143  0.0406\n",
      "     96        2.9030  0.0411\n",
      "     97        2.9093  0.0401\n",
      "     98        2.9088  0.0406\n",
      "     99        2.9071  0.0416\n",
      "    100        2.9052  0.0401\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2807\u001b[0m  0.0401\n",
      "      2       37.2807  0.0411\n",
      "      3       37.2807  0.0401\n",
      "      4       37.2807  0.0411\n",
      "      5       37.2807  0.0396\n",
      "      6       37.2807  0.0401\n",
      "      7       37.2807  0.0406\n",
      "      8       37.2807  0.0416\n",
      "      9       37.2807  0.0421\n",
      "     10       37.2807  0.0406\n",
      "     11       37.2807  0.0431\n",
      "     12       37.2807  0.0401\n",
      "     13       37.2807  0.0391\n",
      "     14       37.2807  0.0391\n",
      "     15       37.2807  0.0411\n",
      "     16       37.2807  0.0411\n",
      "     17       37.2807  0.0391\n",
      "     18       37.2807  0.0401\n",
      "     19       37.2807  0.0401\n",
      "     20       37.2807  0.0391\n",
      "     21       37.2807  0.0401\n",
      "     22       37.2807  0.0401\n",
      "     23       37.2807  0.0401\n",
      "     24       \u001b[36m23.1352\u001b[0m  0.0401\n",
      "     25        \u001b[36m3.4694\u001b[0m  0.0401\n",
      "     26        \u001b[36m3.1976\u001b[0m  0.0390\n",
      "     27        \u001b[36m3.1920\u001b[0m  0.0411\n",
      "     28        3.1923  0.0401\n",
      "     29        \u001b[36m3.1918\u001b[0m  0.0406\n",
      "     30        \u001b[36m3.1912\u001b[0m  0.0401\n",
      "     31        \u001b[36m3.1908\u001b[0m  0.0466\n",
      "     32        \u001b[36m3.1904\u001b[0m  0.0391\n",
      "     33        \u001b[36m3.1900\u001b[0m  0.0411\n",
      "     34        \u001b[36m3.1897\u001b[0m  0.0411\n",
      "     35        \u001b[36m3.1894\u001b[0m  0.0411\n",
      "     36        \u001b[36m3.1891\u001b[0m  0.0416\n",
      "     37        \u001b[36m3.1889\u001b[0m  0.0411\n",
      "     38        \u001b[36m3.1887\u001b[0m  0.0401\n",
      "     39        \u001b[36m3.1885\u001b[0m  0.0396\n",
      "     40        \u001b[36m3.1884\u001b[0m  0.0401\n",
      "     41        \u001b[36m3.1883\u001b[0m  0.0406\n",
      "     42        \u001b[36m3.1882\u001b[0m  0.0401\n",
      "     43        \u001b[36m3.1881\u001b[0m  0.0406\n",
      "     44        \u001b[36m3.1880\u001b[0m  0.0401\n",
      "     45        3.2087  0.0401\n",
      "     46        \u001b[36m3.1875\u001b[0m  0.0401\n",
      "     47        \u001b[36m3.1864\u001b[0m  0.0411\n",
      "     48        \u001b[36m3.1854\u001b[0m  0.0401\n",
      "     49        \u001b[36m3.1849\u001b[0m  0.0401\n",
      "     50        \u001b[36m3.1843\u001b[0m  0.0401\n",
      "     51        \u001b[36m3.1842\u001b[0m  0.0401\n",
      "     52        \u001b[36m3.1842\u001b[0m  0.0401\n",
      "     53        \u001b[36m3.1841\u001b[0m  0.0406\n",
      "     54        \u001b[36m3.1834\u001b[0m  0.0421\n",
      "     55        \u001b[36m3.1832\u001b[0m  0.0411\n",
      "     56        3.1834  0.0416\n",
      "     57        \u001b[36m3.1832\u001b[0m  0.0411\n",
      "     58        3.1833  0.0406\n",
      "     59        \u001b[36m3.1831\u001b[0m  0.0401\n",
      "     60        3.1832  0.0451\n",
      "     61        \u001b[36m3.1825\u001b[0m  0.0411\n",
      "     62        3.1829  0.0390\n",
      "     63        3.1826  0.0396\n",
      "     64        3.1829  0.0391\n",
      "     65        3.1829  0.0396\n",
      "     66        \u001b[36m3.1823\u001b[0m  0.0411\n",
      "     67        3.1827  0.0391\n",
      "     68        \u001b[36m3.1821\u001b[0m  0.0406\n",
      "     69        \u001b[36m3.1820\u001b[0m  0.0401\n",
      "     70        \u001b[36m3.1819\u001b[0m  0.0396\n",
      "     71        \u001b[36m3.1818\u001b[0m  0.0451\n",
      "     72        3.1823  0.0396\n",
      "     73        \u001b[36m3.1818\u001b[0m  0.0401\n",
      "     74        \u001b[36m3.1816\u001b[0m  0.0401\n",
      "     75        3.1816  0.0406\n",
      "     76        3.1819  0.0400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     77        \u001b[36m3.1814\u001b[0m  0.0406\n",
      "     78        3.1825  0.0411\n",
      "     79        3.1817  0.0401\n",
      "     80        3.1829  0.0406\n",
      "     81        3.1818  0.0401\n",
      "     82        \u001b[36m3.1814\u001b[0m  0.0416\n",
      "     83        3.1815  0.0411\n",
      "     84        \u001b[36m3.1813\u001b[0m  0.0406\n",
      "     85        \u001b[36m3.1812\u001b[0m  0.0431\n",
      "     86        \u001b[36m3.1811\u001b[0m  0.0411\n",
      "     87        3.1813  0.0406\n",
      "     88        3.1818  0.0401\n",
      "     89        3.1814  0.0411\n",
      "     90        3.1812  0.0408\n",
      "     91        3.1812  0.0411\n",
      "     92        3.1811  0.0396\n",
      "     93        3.1828  0.0401\n",
      "     94        3.1812  0.0406\n",
      "     95        3.1818  0.0401\n",
      "     96        3.1812  0.0421\n",
      "     97        \u001b[36m3.1810\u001b[0m  0.0401\n",
      "     98        \u001b[36m3.1810\u001b[0m  0.0401\n",
      "     99        \u001b[36m3.1809\u001b[0m  0.0406\n",
      "    100        3.1809  0.0411\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m39.7802\u001b[0m  0.0291\n",
      "      2       39.7802  0.0305\n",
      "      3       39.7802  0.0306\n",
      "      4       39.7802  0.0301\n",
      "      5       39.7802  0.0331\n",
      "      6       39.7802  0.0325\n",
      "      7       39.7802  0.0300\n",
      "      8       39.7802  0.0295\n",
      "      9       39.7802  0.0295\n",
      "     10       39.7802  0.0295\n",
      "     11       39.7802  0.0346\n",
      "     12       39.7802  0.0295\n",
      "     13       39.7802  0.0305\n",
      "     14       39.7802  0.0301\n",
      "     15       39.7802  0.0305\n",
      "     16       39.7802  0.0300\n",
      "     17       39.7802  0.0301\n",
      "     18       39.7802  0.0295\n",
      "     19       39.7802  0.0305\n",
      "     20       39.7802  0.0301\n",
      "     21       39.7802  0.0305\n",
      "     22       39.7802  0.0305\n",
      "     23       39.7802  0.0301\n",
      "     24       39.7802  0.0295\n",
      "     25       39.7802  0.0300\n",
      "     26       39.7802  0.0306\n",
      "     27       39.7802  0.0290\n",
      "     28       39.7802  0.0305\n",
      "     29       39.7802  0.0305\n",
      "     30       39.7802  0.0311\n",
      "     31       39.7802  0.0305\n",
      "     32       39.7802  0.0295\n",
      "     33       39.7802  0.0301\n",
      "     34       39.7802  0.0300\n",
      "     35       39.7802  0.0305\n",
      "     36       39.7802  0.0311\n",
      "     37       39.7802  0.0305\n",
      "     38       39.7802  0.0295\n",
      "     39       39.7802  0.0331\n",
      "     40       39.7802  0.0300\n",
      "     41       39.7802  0.0306\n",
      "     42       39.7802  0.0295\n",
      "     43       39.7802  0.0366\n",
      "     44       39.7802  0.0305\n",
      "     45       39.7802  0.0305\n",
      "     46       39.7802  0.0300\n",
      "     47       39.7802  0.0295\n",
      "     48       39.7802  0.0306\n",
      "     49       39.7802  0.0301\n",
      "     50       39.7802  0.0295\n",
      "     51       39.7802  0.0315\n",
      "     52       39.7802  0.0300\n",
      "     53       39.7802  0.0306\n",
      "     54       39.7802  0.0295\n",
      "     55       39.7802  0.0311\n",
      "     56       39.7802  0.0295\n",
      "     57       39.7802  0.0295\n",
      "     58       39.7802  0.0305\n",
      "     59       39.7802  0.0296\n",
      "     60       39.7802  0.0295\n",
      "     61       39.7802  0.0295\n",
      "     62       39.7802  0.0301\n",
      "     63       39.7802  0.0305\n",
      "     64       39.7802  0.0305\n",
      "     65       39.7802  0.0301\n",
      "     66       39.7802  0.0305\n",
      "     67       39.7802  0.0295\n",
      "     68       39.7802  0.0301\n",
      "     69       39.7802  0.0305\n",
      "     70       39.7802  0.0310\n",
      "     71       39.7802  0.0311\n",
      "     72       39.7802  0.0305\n",
      "     73       39.7802  0.0295\n",
      "     74       39.7802  0.0306\n",
      "     75       39.7802  0.0301\n",
      "     76       39.7802  0.0351\n",
      "     77       39.7802  0.0305\n",
      "     78       39.7802  0.0305\n",
      "     79       39.7802  0.0306\n",
      "     80       39.7802  0.0305\n",
      "     81       39.7802  0.0311\n",
      "     82       39.7802  0.0306\n",
      "     83       39.7802  0.0315\n",
      "     84       39.7802  0.0331\n",
      "     85       39.7802  0.0305\n",
      "     86       39.7802  0.0300\n",
      "     87       39.7802  0.0300\n",
      "     88       39.7802  0.0295\n",
      "     89       39.7802  0.0305\n",
      "     90       39.7802  0.0330\n",
      "     91       39.7802  0.0305\n",
      "     92       39.7802  0.0315\n",
      "     93       39.7802  0.0311\n",
      "     94       39.7802  0.0305\n",
      "     95       39.7802  0.0295\n",
      "     96       39.7802  0.0305\n",
      "     97       39.7802  0.0305\n",
      "     98       39.7802  0.0305\n",
      "     99       39.7802  0.0315\n",
      "    100       39.7802  0.0316\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m69.4566\u001b[0m  0.0305\n",
      "      2       \u001b[36m62.8571\u001b[0m  0.0306\n",
      "      3       62.8571  0.0325\n",
      "      4       62.8571  0.0295\n",
      "      5       62.8571  0.0305\n",
      "      6       62.8571  0.0317\n",
      "      7       62.8571  0.0315\n",
      "      8       62.8571  0.0336\n",
      "      9       62.8571  0.0305\n",
      "     10       62.8571  0.0305\n",
      "     11       62.8571  0.0305\n",
      "     12       62.8571  0.0305\n",
      "     13       62.8571  0.0295\n",
      "     14       62.8571  0.0295\n",
      "     15       62.8571  0.0305\n",
      "     16       62.8571  0.0295\n",
      "     17       62.8571  0.0295\n",
      "     18       62.8571  0.0295\n",
      "     19       62.8571  0.0285\n",
      "     20       62.8571  0.0300\n",
      "     21       62.8571  0.0305\n",
      "     22       62.8571  0.0306\n",
      "     23       62.8571  0.0305\n",
      "     24       62.8571  0.0295\n",
      "     25       62.8571  0.0291\n",
      "     26       62.8571  0.0305\n",
      "     27       62.8571  0.0305\n",
      "     28       62.8571  0.0300\n",
      "     29       62.8571  0.0386\n",
      "     30       62.8571  0.0326\n",
      "     31       62.8571  0.0336\n",
      "     32       62.8571  0.0341\n",
      "     33       62.8571  0.0336\n",
      "     34       62.8571  0.0341\n",
      "     35       62.8571  0.0336\n",
      "     36       62.8571  0.0310\n",
      "     37       62.8571  0.0326\n",
      "     38       62.8571  0.0325\n",
      "     39       62.8571  0.0356\n",
      "     40       62.8571  0.0336\n",
      "     41       62.8571  0.0326\n",
      "     42       62.8571  0.0336\n",
      "     43       62.8571  0.0310\n",
      "     44       62.8571  0.0321\n",
      "     45       62.8571  0.0305\n",
      "     46       62.8571  0.0311\n",
      "     47       62.8571  0.0305\n",
      "     48       62.8571  0.0305\n",
      "     49       62.8571  0.0331\n",
      "     50       62.8571  0.0305\n",
      "     51       62.8571  0.0315\n",
      "     52       62.8571  0.0331\n",
      "     53       62.8571  0.0325\n",
      "     54       62.8571  0.0331\n",
      "     55       62.8571  0.0371\n",
      "     56       62.8571  0.0311\n",
      "     57       62.8571  0.0326\n",
      "     58       62.8571  0.0311\n",
      "     59       62.8571  0.0316\n",
      "     60       62.8571  0.0311\n",
      "     61       62.8571  0.0361\n",
      "     62       62.8571  0.0401\n",
      "     63       62.8571  0.0386\n",
      "     64       62.8571  0.0336\n",
      "     65       62.8571  0.0325\n",
      "     66       62.8571  0.0316\n",
      "     67       62.8571  0.0316\n",
      "     68       62.8571  0.0316\n",
      "     69       62.8571  0.0627\n",
      "     70       62.8571  0.0426\n",
      "     71       62.8571  0.0341\n",
      "     72       62.8571  0.0351\n",
      "     73       62.8571  0.0346\n",
      "     74       62.8571  0.0371\n",
      "     75       62.8571  0.0331\n",
      "     76       62.8571  0.0296\n",
      "     77       62.8571  0.0290\n",
      "     78       62.8571  0.0305\n",
      "     79       62.8571  0.0295\n",
      "     80       62.8571  0.0290\n",
      "     81       62.8571  0.0305\n",
      "     82       62.8571  0.0305\n",
      "     83       62.8571  0.0300\n",
      "     84       62.8571  0.0295\n",
      "     85       62.8571  0.0306\n",
      "     86       62.8571  0.0316\n",
      "     87       62.8571  0.0321\n",
      "     88       62.8571  0.0316\n",
      "     89       62.8571  0.0396\n",
      "     90       62.8571  0.0481\n",
      "     91       62.8571  0.0451\n",
      "     92       62.8571  0.0341\n",
      "     93       62.8571  0.0326\n",
      "     94       62.8571  0.0310\n",
      "     95       62.8571  0.0306\n",
      "     96       62.8571  0.0305\n",
      "     97       62.8571  0.0295\n",
      "     98       62.8571  0.0341\n",
      "     99       62.8571  0.0316\n",
      "    100       62.8571  0.0316\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0295\n",
      "      2       37.3626  0.0311\n",
      "      3       37.3626  0.0295\n",
      "      4       37.3626  0.0290\n",
      "      5       37.3626  0.0316\n",
      "      6       37.3626  0.0295\n",
      "      7       37.3626  0.0290\n",
      "      8       37.3626  0.0295\n",
      "      9       37.3626  0.0305\n",
      "     10       37.3626  0.0300\n",
      "     11       37.3626  0.0315\n",
      "     12       37.3626  0.0295\n",
      "     13       37.3626  0.0331\n",
      "     14       37.3626  0.0295\n",
      "     15       37.3626  0.0295\n",
      "     16       37.3626  0.0300\n",
      "     17       37.3626  0.0305\n",
      "     18       37.3626  0.0300\n",
      "     19       37.3626  0.0295\n",
      "     20       37.3626  0.0301\n",
      "     21       37.3626  0.0295\n",
      "     22       37.3626  0.0315\n",
      "     23       37.3626  0.0300\n",
      "     24       37.3626  0.0306\n",
      "     25       37.3626  0.0295\n",
      "     26       37.3626  0.0291\n",
      "     27       37.3626  0.0331\n",
      "     28       37.3626  0.0381\n",
      "     29       37.3626  0.0361\n",
      "     30       37.3626  0.0321\n",
      "     31       37.3626  0.0295\n",
      "     32       37.3626  0.0300\n",
      "     33       37.3626  0.0295\n",
      "     34       37.3626  0.0295\n",
      "     35       37.3626  0.0302\n",
      "     36       37.3626  0.0295\n",
      "     37       37.3626  0.0295\n",
      "     38       37.3626  0.0295\n",
      "     39       37.3626  0.0295\n",
      "     40       37.3626  0.0295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     41       37.3626  0.0290\n",
      "     42       37.3626  0.0295\n",
      "     43       37.3626  0.0295\n",
      "     44       37.3626  0.0305\n",
      "     45       37.3626  0.0331\n",
      "     46       37.3626  0.0306\n",
      "     47       37.3626  0.0296\n",
      "     48       37.3626  0.0301\n",
      "     49       37.3626  0.0305\n",
      "     50       37.3626  0.0295\n",
      "     51       37.3626  0.0305\n",
      "     52       37.3626  0.0305\n",
      "     53       37.3626  0.0295\n",
      "     54       37.3626  0.0305\n",
      "     55       37.3626  0.0295\n",
      "     56       37.3626  0.0321\n",
      "     57       37.3626  0.0326\n",
      "     58       37.3626  0.0321\n",
      "     59       37.3626  0.0306\n",
      "     60       37.3626  0.0316\n",
      "     61       37.3626  0.0321\n",
      "     62       37.3626  0.0326\n",
      "     63       37.3626  0.0305\n",
      "     64       37.3626  0.0311\n",
      "     65       37.3626  0.0295\n",
      "     66       37.3626  0.0305\n",
      "     67       37.3626  0.0291\n",
      "     68       37.3626  0.0305\n",
      "     69       37.3626  0.0295\n",
      "     70       37.3626  0.0305\n",
      "     71       37.3626  0.0295\n",
      "     72       37.3626  0.0295\n",
      "     73       37.3626  0.0305\n",
      "     74       37.3626  0.0300\n",
      "     75       37.3626  0.0295\n",
      "     76       37.3626  0.0305\n",
      "     77       37.3626  0.0295\n",
      "     78       37.3626  0.0305\n",
      "     79       37.3626  0.0305\n",
      "     80       37.3626  0.0295\n",
      "     81       37.3626  0.0305\n",
      "     82       37.3626  0.0295\n",
      "     83       37.3626  0.0306\n",
      "     84       37.3626  0.0300\n",
      "     85       37.3626  0.0306\n",
      "     86       37.3626  0.0305\n",
      "     87       37.3626  0.0295\n",
      "     88       37.3626  0.0315\n",
      "     89       37.3626  0.0305\n",
      "     90       37.3626  0.0301\n",
      "     91       37.3626  0.0305\n",
      "     92       37.3626  0.0295\n",
      "     93       37.3626  0.0305\n",
      "     94       37.3626  0.0336\n",
      "     95       37.3626  0.0316\n",
      "     96       37.3626  0.0316\n",
      "     97       37.3626  0.0295\n",
      "     98       37.3626  0.0315\n",
      "     99       37.3626  0.0305\n",
      "    100       37.3626  0.0315\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m31.1437\u001b[0m  0.0305\n",
      "      2       37.3626  0.0311\n",
      "      3       37.3626  0.0295\n",
      "      4       37.3626  0.0305\n",
      "      5       37.3626  0.0316\n",
      "      6       37.3626  0.0315\n",
      "      7       37.3626  0.0316\n",
      "      8       37.3626  0.0336\n",
      "      9       37.3626  0.0315\n",
      "     10       37.3626  0.0316\n",
      "     11       37.3626  0.0321\n",
      "     12       37.3626  0.0301\n",
      "     13       37.3626  0.0316\n",
      "     14       37.3626  0.0311\n",
      "     15       37.3626  0.0305\n",
      "     16       37.3626  0.0306\n",
      "     17       37.3626  0.0356\n",
      "     18       37.3626  0.0325\n",
      "     19       37.3626  0.0351\n",
      "     20       37.3626  0.0306\n",
      "     21       37.3626  0.0305\n",
      "     22       37.3626  0.0295\n",
      "     23       37.3626  0.0311\n",
      "     24       37.3626  0.0300\n",
      "     25       37.3626  0.0325\n",
      "     26       37.3626  0.0316\n",
      "     27       37.3626  0.0295\n",
      "     28       37.3626  0.0295\n",
      "     29       37.3626  0.0295\n",
      "     30       37.3626  0.0300\n",
      "     31       37.3626  0.0295\n",
      "     32       37.3626  0.0285\n",
      "     33       37.3626  0.0290\n",
      "     34       37.3626  0.0295\n",
      "     35       37.3626  0.0295\n",
      "     36       37.3626  0.0310\n",
      "     37       37.3626  0.0295\n",
      "     38       37.3626  0.0295\n",
      "     39       37.3626  0.0295\n",
      "     40       37.3626  0.0361\n",
      "     41       37.3626  0.0331\n",
      "     42       37.3626  0.0306\n",
      "     43       37.3626  0.0295\n",
      "     44       37.3626  0.0306\n",
      "     45       37.3626  0.0305\n",
      "     46       37.3626  0.0310\n",
      "     47       37.3626  0.0310\n",
      "     48       37.3626  0.0315\n",
      "     49       37.3626  0.0301\n",
      "     50       37.3626  0.0295\n",
      "     51       37.3626  0.0305\n",
      "     52       37.3626  0.0300\n",
      "     53       37.3626  0.0305\n",
      "     54       37.3626  0.0331\n",
      "     55       37.3626  0.0597\n",
      "     56       37.3626  0.0627\n",
      "     57       37.3626  0.0638\n",
      "     58       37.3626  0.0496\n",
      "     59       37.3626  0.0351\n",
      "     60       37.3626  0.0306\n",
      "     61       37.3626  0.0311\n",
      "     62       37.3626  0.0305\n",
      "     63       37.3626  0.0295\n",
      "     64       37.3626  0.0326\n",
      "     65       37.3626  0.0295\n",
      "     66       37.3626  0.0305\n",
      "     67       37.3626  0.0311\n",
      "     68       37.3626  0.0316\n",
      "     69       37.3626  0.0331\n",
      "     70       37.3626  0.0341\n",
      "     71       37.3626  0.0361\n",
      "     72       37.3626  0.0326\n",
      "     73       37.3626  0.0311\n",
      "     74       37.3626  0.0446\n",
      "     75       37.3626  0.0325\n",
      "     76       37.3626  0.0386\n",
      "     77       37.3626  0.0331\n",
      "     78       37.3626  0.0371\n",
      "     79       37.3626  0.0411\n",
      "     80       37.3626  0.0325\n",
      "     81       37.3626  0.0366\n",
      "     82       37.3626  0.0325\n",
      "     83       37.3626  0.0491\n",
      "     84       37.3626  0.0341\n",
      "     85       37.3626  0.0451\n",
      "     86       37.3626  0.0562\n",
      "     87       37.3626  0.0331\n",
      "     88       37.3626  0.0371\n",
      "     89       37.3626  0.0315\n",
      "     90       37.3626  0.0376\n",
      "     91       37.3626  0.0311\n",
      "     92       37.3626  0.0305\n",
      "     93       37.3626  0.0421\n",
      "     94       37.3626  0.0311\n",
      "     95       37.3626  0.0326\n",
      "     96       37.3626  0.0305\n",
      "     97       37.3626  0.0301\n",
      "     98       37.3626  0.0316\n",
      "     99       37.3626  0.0295\n",
      "    100       37.3626  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.5000\u001b[0m  0.0311\n",
      "      2       37.5000  0.0345\n",
      "      3       37.5000  0.0311\n",
      "      4       37.5000  0.0295\n",
      "      5       37.5000  0.0295\n",
      "      6       37.5000  0.0321\n",
      "      7       37.5000  0.0316\n",
      "      8       37.5000  0.0305\n",
      "      9       37.5000  0.0471\n",
      "     10       37.5000  0.0391\n",
      "     11       37.5000  0.0386\n",
      "     12       37.5000  0.0336\n",
      "     13       37.5000  0.0401\n",
      "     14       37.5000  0.0503\n",
      "     15       37.5000  0.0426\n",
      "     16       37.5000  0.0321\n",
      "     17       37.5000  0.0401\n",
      "     18       37.5000  0.0316\n",
      "     19       37.5000  0.0521\n",
      "     20       37.5000  0.0381\n",
      "     21       37.5000  0.0371\n",
      "     22       37.5000  0.0391\n",
      "     23       37.5000  0.0336\n",
      "     24       37.5000  0.0356\n",
      "     25       37.5000  0.0326\n",
      "     26       37.5000  0.0326\n",
      "     27       37.5000  0.0306\n",
      "     28       37.5000  0.0448\n",
      "     29       37.5000  0.0376\n",
      "     30       37.5000  0.0341\n",
      "     31       37.5000  0.0316\n",
      "     32       37.5000  0.0361\n",
      "     33       37.5000  0.0295\n",
      "     34       37.5000  0.0306\n",
      "     35       37.5000  0.0305\n",
      "     36       37.5000  0.0310\n",
      "     37       37.5000  0.0290\n",
      "     38       37.5000  0.0310\n",
      "     39       37.5000  0.0310\n",
      "     40       37.5000  0.0340\n",
      "     41       37.5000  0.0310\n",
      "     42       37.5000  0.0295\n",
      "     43       37.5000  0.0300\n",
      "     44       37.5000  0.0300\n",
      "     45       37.5000  0.0305\n",
      "     46       37.5000  0.0300\n",
      "     47       37.5000  0.0300\n",
      "     48       37.5000  0.0295\n",
      "     49       37.5000  0.0310\n",
      "     50       37.5000  0.0310\n",
      "     51       37.5000  0.0295\n",
      "     52       37.5000  0.0300\n",
      "     53       37.5000  0.0300\n",
      "     54       37.5000  0.0305\n",
      "     55       37.5000  0.0300\n",
      "     56       37.5000  0.0310\n",
      "     57       37.5000  0.0300\n",
      "     58       37.5000  0.0305\n",
      "     59       37.5000  0.0320\n",
      "     60       37.5000  0.0350\n",
      "     61       37.5000  0.0305\n",
      "     62       37.5000  0.0310\n",
      "     63       37.5000  0.0300\n",
      "     64       37.5000  0.0295\n",
      "     65       37.5000  0.0300\n",
      "     66       37.5000  0.0310\n",
      "     67       37.5000  0.0305\n",
      "     68       37.5000  0.0300\n",
      "     69       37.5000  0.0300\n",
      "     70       37.5000  0.0295\n",
      "     71       37.5000  0.0320\n",
      "     72       37.5000  0.0310\n",
      "     73       37.5000  0.0305\n",
      "     74       37.5000  0.0430\n",
      "     75       37.5000  0.0510\n",
      "     76       37.5000  0.0420\n",
      "     77       37.5000  0.0460\n",
      "     78       37.5000  0.0485\n",
      "     79       37.5000  0.0330\n",
      "     80       37.5000  0.0395\n",
      "     81       37.5000  0.0520\n",
      "     82       37.5000  0.0425\n",
      "     83       37.5000  0.0430\n",
      "     84       37.5000  0.0505\n",
      "     85       37.5000  0.0450\n",
      "     86       37.5000  0.0430\n",
      "     87       37.5000  0.0525\n",
      "     88       37.5000  0.0450\n",
      "     89       37.5000  0.0440\n",
      "     90       37.5000  0.0585\n",
      "     91       37.5000  0.0430\n",
      "     92       37.5000  0.0330\n",
      "     93       37.5000  0.0325\n",
      "     94       37.5000  0.0370\n",
      "     95       37.5000  0.0320\n",
      "     96       37.5000  0.0405\n",
      "     97       37.5000  0.0340\n",
      "     98       37.5000  0.0330\n",
      "     99       37.5000  0.0325\n",
      "    100       37.5000  0.0320\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.4175\u001b[0m  0.0435\n",
      "      2       \u001b[36m37.3626\u001b[0m  0.0440\n",
      "      3       37.3626  0.0405\n",
      "      4       37.3626  0.0420\n",
      "      5       37.3626  0.0410\n",
      "      6       37.3626  0.0405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7       37.3626  0.0400\n",
      "      8       \u001b[36m37.1429\u001b[0m  0.0405\n",
      "      9       37.1429  0.0410\n",
      "     10       \u001b[36m36.9231\u001b[0m  0.0410\n",
      "     11       36.9231  0.0435\n",
      "     12       36.9231  0.0430\n",
      "     13       37.1429  0.0415\n",
      "     14       \u001b[36m36.7033\u001b[0m  0.0470\n",
      "     15       37.1429  0.0425\n",
      "     16       38.0220  0.0410\n",
      "     17       38.0220  0.0445\n",
      "     18       38.0220  0.0570\n",
      "     19       38.0220  0.0635\n",
      "     20       38.2418  0.0570\n",
      "     21       38.4615  0.0625\n",
      "     22       38.4615  0.0480\n",
      "     23       38.4615  0.0420\n",
      "     24       38.4615  0.0430\n",
      "     25       38.4615  0.0435\n",
      "     26       38.4615  0.0440\n",
      "     27       38.4615  0.0445\n",
      "     28       38.4615  0.0440\n",
      "     29       38.4821  0.0415\n",
      "     30       38.4615  0.0410\n",
      "     31       38.4615  0.0450\n",
      "     32       38.4615  0.0415\n",
      "     33       38.4615  0.0430\n",
      "     34       38.4615  0.0425\n",
      "     35       38.4615  0.0430\n",
      "     36       38.4615  0.0428\n",
      "     37       38.4615  0.0420\n",
      "     38       38.4325  0.0425\n",
      "     39       38.4105  0.0430\n",
      "     40       38.3897  0.0440\n",
      "     41       38.3693  0.0425\n",
      "     42       38.3492  0.0420\n",
      "     43       38.3294  0.0425\n",
      "     44       38.0880  0.0420\n",
      "     45       38.0220  0.0425\n",
      "     46       38.0220  0.0420\n",
      "     47       38.0220  0.0430\n",
      "     48       38.0220  0.0415\n",
      "     49       38.0220  0.0430\n",
      "     50       38.0220  0.0425\n",
      "     51       38.0220  0.0450\n",
      "     52       38.0220  0.0425\n",
      "     53       38.0220  0.0430\n",
      "     54       38.0220  0.0430\n",
      "     55       38.0220  0.0430\n",
      "     56       38.0220  0.0440\n",
      "     57       38.0220  0.0430\n",
      "     58       38.0220  0.0440\n",
      "     59       38.0220  0.0430\n",
      "     60       38.0220  0.0420\n",
      "     61       38.0220  0.0450\n",
      "     62       38.0220  0.0430\n",
      "     63       38.0220  0.0440\n",
      "     64       38.0220  0.0430\n",
      "     65       38.0220  0.0430\n",
      "     66       38.0220  0.0440\n",
      "     67       38.0220  0.0440\n",
      "     68       38.0220  0.0430\n",
      "     69       38.0220  0.0420\n",
      "     70       38.0220  0.0420\n",
      "     71       38.0220  0.0430\n",
      "     72       38.0220  0.0450\n",
      "     73       38.0220  0.0440\n",
      "     74       38.0220  0.0430\n",
      "     75       38.0220  0.0430\n",
      "     76       38.0220  0.0430\n",
      "     77       38.0220  0.0430\n",
      "     78       38.0220  0.0430\n",
      "     79       38.0220  0.0450\n",
      "     80       38.0220  0.0440\n",
      "     81       38.0220  0.0440\n",
      "     82       38.0220  0.0430\n",
      "     83       38.0220  0.0430\n",
      "     84       38.0220  0.0430\n",
      "     85       38.0220  0.0440\n",
      "     86       38.0220  0.0450\n",
      "     87       38.0220  0.0430\n",
      "     88       38.0220  0.0440\n",
      "     89       38.0220  0.0430\n",
      "     90       38.0220  0.0430\n",
      "     91       38.0220  0.0420\n",
      "     92       38.0220  0.0430\n",
      "     93       38.0220  0.0420\n",
      "     94       38.0220  0.0420\n",
      "     95       38.0220  0.0440\n",
      "     96       38.0220  0.0420\n",
      "     97       38.0220  0.0420\n",
      "     98       38.0220  0.0430\n",
      "     99       38.0220  0.0440\n",
      "    100       38.0220  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m36.9231\u001b[0m  0.0400\n",
      "      2       36.9231  0.0410\n",
      "      3       36.9231  0.0400\n",
      "      4       37.1429  0.0420\n",
      "      5       37.1429  0.0390\n",
      "      6       38.0220  0.0410\n",
      "      7       37.1429  0.0390\n",
      "      8       37.1429  0.0400\n",
      "      9       37.1429  0.0420\n",
      "     10       37.1429  0.0400\n",
      "     11       37.1429  0.0390\n",
      "     12       37.1429  0.0400\n",
      "     13       37.1429  0.0400\n",
      "     14       37.1429  0.0390\n",
      "     15       37.1429  0.0400\n",
      "     16       37.1429  0.0400\n",
      "     17       37.1429  0.0400\n",
      "     18       37.1429  0.0400\n",
      "     19       37.1429  0.0400\n",
      "     20       37.1429  0.0400\n",
      "     21       37.1429  0.0400\n",
      "     22       37.1429  0.0400\n",
      "     23       37.1429  0.0420\n",
      "     24       37.1429  0.0400\n",
      "     25       37.1429  0.0410\n",
      "     26       37.1429  0.0400\n",
      "     27       37.1429  0.0410\n",
      "     28       37.1429  0.0410\n",
      "     29       37.1429  0.0400\n",
      "     30       37.1429  0.0400\n",
      "     31       37.1429  0.0400\n",
      "     32       37.1429  0.0400\n",
      "     33       37.1429  0.0410\n",
      "     34       37.1429  0.0410\n",
      "     35       37.1429  0.0400\n",
      "     36       37.1429  0.0420\n",
      "     37       37.1429  0.0400\n",
      "     38       37.1429  0.0410\n",
      "     39       37.1429  0.0410\n",
      "     40       37.1429  0.0410\n",
      "     41       37.1429  0.0410\n",
      "     42       37.1429  0.0410\n",
      "     43       37.1429  0.0460\n",
      "     44       37.1429  0.0410\n",
      "     45       37.1429  0.0410\n",
      "     46       37.1429  0.0410\n",
      "     47       37.1429  0.0430\n",
      "     48       37.1429  0.0410\n",
      "     49       37.1429  0.0420\n",
      "     50       37.1429  0.0430\n",
      "     51       37.1429  0.0420\n",
      "     52       37.1429  0.0400\n",
      "     53       \u001b[36m35.7335\u001b[0m  0.0410\n",
      "     54       37.1429  0.0400\n",
      "     55       37.1429  0.0390\n",
      "     56       37.1429  0.0400\n",
      "     57       37.1429  0.0400\n",
      "     58       37.1429  0.0420\n",
      "     59       37.1429  0.0400\n",
      "     60       37.1429  0.0400\n",
      "     61       37.1429  0.0400\n",
      "     62       37.1429  0.0400\n",
      "     63       37.1429  0.0400\n",
      "     64       37.1429  0.0400\n",
      "     65       37.1429  0.0410\n",
      "     66       37.1429  0.0400\n",
      "     67       37.1429  0.0410\n",
      "     68       37.1429  0.0410\n",
      "     69       37.1429  0.0400\n",
      "     70       37.1429  0.0400\n",
      "     71       37.1429  0.0400\n",
      "     72       37.1429  0.0400\n",
      "     73       37.1429  0.0410\n",
      "     74       37.1429  0.0400\n",
      "     75       37.1429  0.0410\n",
      "     76       37.1429  0.0400\n",
      "     77       37.1429  0.0410\n",
      "     78       37.1429  0.0410\n",
      "     79       37.1429  0.0410\n",
      "     80       37.1429  0.0400\n",
      "     81       37.1429  0.0400\n",
      "     82       37.1429  0.0420\n",
      "     83       37.1429  0.0400\n",
      "     84       37.1429  0.0400\n",
      "     85       37.1429  0.0400\n",
      "     86       37.1429  0.0400\n",
      "     87       37.1429  0.0400\n",
      "     88       37.1429  0.0410\n",
      "     89       37.1429  0.0400\n",
      "     90       37.1429  0.0410\n",
      "     91       37.1429  0.0410\n",
      "     92       37.1429  0.0410\n",
      "     93       37.1429  0.0400\n",
      "     94       37.1429  0.0410\n",
      "     95       37.1429  0.0420\n",
      "     96       37.1429  0.0400\n",
      "     97       37.1429  0.0410\n",
      "     98       37.1429  0.0410\n",
      "     99       37.1429  0.0410\n",
      "    100       37.1429  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m48.1319\u001b[0m  0.0400\n",
      "      2       48.3516  0.0400\n",
      "      3       49.0110  0.0410\n",
      "      4       48.1319  0.0400\n",
      "      5       \u001b[36m47.8835\u001b[0m  0.0410\n",
      "      6       47.9517  0.0420\n",
      "      7       47.9410  0.0400\n",
      "      8       48.5714  0.0400\n",
      "      9       48.2298  0.0400\n",
      "     10       \u001b[36m47.4725\u001b[0m  0.0400\n",
      "     11       \u001b[36m43.0561\u001b[0m  0.0400\n",
      "     12       43.0769  0.0390\n",
      "     13       43.0769  0.0400\n",
      "     14       \u001b[36m42.8571\u001b[0m  0.0400\n",
      "     15       42.9779  0.0400\n",
      "     16       \u001b[36m41.9971\u001b[0m  0.0390\n",
      "     17       \u001b[36m41.9780\u001b[0m  0.0400\n",
      "     18       41.9780  0.0400\n",
      "     19       41.9780  0.0440\n",
      "     20       41.9780  0.0400\n",
      "     21       41.9780  0.0400\n",
      "     22       41.9780  0.0400\n",
      "     23       41.9780  0.0410\n",
      "     24       41.9780  0.0400\n",
      "     25       41.9780  0.0390\n",
      "     26       41.9780  0.0410\n",
      "     27       41.9780  0.0400\n",
      "     28       41.9780  0.0390\n",
      "     29       41.9780  0.0400\n",
      "     30       41.9780  0.0400\n",
      "     31       41.9780  0.0410\n",
      "     32       41.9780  0.0390\n",
      "     33       41.9780  0.0390\n",
      "     34       41.9780  0.0400\n",
      "     35       41.9780  0.0400\n",
      "     36       41.9780  0.0390\n",
      "     37       41.9780  0.0390\n",
      "     38       41.9780  0.0410\n",
      "     39       41.9780  0.0390\n",
      "     40       41.9780  0.0410\n",
      "     41       41.9780  0.0410\n",
      "     42       41.9780  0.0400\n",
      "     43       41.9780  0.0400\n",
      "     44       41.9780  0.0410\n",
      "     45       41.9780  0.0400\n",
      "     46       41.9780  0.0400\n",
      "     47       41.9780  0.0400\n",
      "     48       41.9780  0.0410\n",
      "     49       41.9780  0.0400\n",
      "     50       50.2747  0.0410\n",
      "     51       45.9114  0.0400\n",
      "     52       \u001b[36m35.7243\u001b[0m  0.0410\n",
      "     53       39.7802  0.0400\n",
      "     54       39.5604  0.0400\n",
      "     55       39.4997  0.0420\n",
      "     56       38.9517  0.0400\n",
      "     57       38.6813  0.0390\n",
      "     58       38.6813  0.0400\n",
      "     59       38.6813  0.0400\n",
      "     60       38.6813  0.0400\n",
      "     61       38.6813  0.0390\n",
      "     62       38.6813  0.0410\n",
      "     63       38.6813  0.0400\n",
      "     64       38.6813  0.0400\n",
      "     65       38.6813  0.0400\n",
      "     66       38.6813  0.0410\n",
      "     67       38.6813  0.0400\n",
      "     68       38.6813  0.0410\n",
      "     69       38.6813  0.0400\n",
      "     70       38.6813  0.0400\n",
      "     71       38.6813  0.0440\n",
      "     72       38.6813  0.0410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     73       38.6813  0.0420\n",
      "     74       38.6813  0.0400\n",
      "     75       38.6813  0.0420\n",
      "     76       38.6813  0.0400\n",
      "     77       38.6813  0.0410\n",
      "     78       38.6813  0.0410\n",
      "     79       38.6813  0.0410\n",
      "     80       38.6813  0.0420\n",
      "     81       38.6813  0.0400\n",
      "     82       38.6813  0.0420\n",
      "     83       38.6813  0.0410\n",
      "     84       38.6813  0.0410\n",
      "     85       38.6813  0.0410\n",
      "     86       38.6813  0.0410\n",
      "     87       38.6813  0.0410\n",
      "     88       38.6813  0.0410\n",
      "     89       38.6813  0.0400\n",
      "     90       38.6813  0.0420\n",
      "     91       38.6813  0.0410\n",
      "     92       38.6813  0.0410\n",
      "     93       38.6813  0.0410\n",
      "     94       38.6813  0.0410\n",
      "     95       38.6813  0.0420\n",
      "     96       38.6813  0.0420\n",
      "     97       38.6813  0.0410\n",
      "     98       38.6813  0.0410\n",
      "     99       38.6813  0.0410\n",
      "    100       38.6813  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m40.8791\u001b[0m  0.0390\n",
      "      2       40.8791  0.0400\n",
      "      3       41.4857  0.0400\n",
      "      4       41.0989  0.0410\n",
      "      5       \u001b[36m40.4446\u001b[0m  0.0400\n",
      "      6       41.0989  0.0400\n",
      "      7       40.8791  0.0390\n",
      "      8       40.8791  0.0400\n",
      "      9       40.8791  0.0410\n",
      "     10       40.8791  0.0390\n",
      "     11       40.8791  0.0400\n",
      "     12       40.8791  0.0400\n",
      "     13       40.8791  0.0400\n",
      "     14       40.6593  0.0400\n",
      "     15       40.6593  0.0400\n",
      "     16       40.6593  0.0390\n",
      "     17       40.8791  0.0400\n",
      "     18       41.5385  0.0390\n",
      "     19       41.7582  0.0400\n",
      "     20       40.6951  0.0390\n",
      "     21       \u001b[36m40.0000\u001b[0m  0.0410\n",
      "     22       40.0000  0.0400\n",
      "     23       40.0000  0.0400\n",
      "     24       40.0000  0.0400\n",
      "     25       40.0000  0.0400\n",
      "     26       \u001b[36m39.5946\u001b[0m  0.0400\n",
      "     27       \u001b[36m39.5604\u001b[0m  0.0390\n",
      "     28       39.5604  0.0410\n",
      "     29       39.5604  0.0410\n",
      "     30       39.5604  0.0390\n",
      "     31       39.5604  0.0400\n",
      "     32       39.5604  0.0400\n",
      "     33       39.6558  0.0400\n",
      "     34       39.7305  0.0400\n",
      "     35       39.7802  0.0400\n",
      "     36       39.7802  0.0390\n",
      "     37       39.7802  0.0400\n",
      "     38       39.7802  0.0400\n",
      "     39       39.7802  0.0400\n",
      "     40       39.7802  0.0410\n",
      "     41       39.7802  0.0400\n",
      "     42       39.7802  0.0400\n",
      "     43       39.7802  0.0400\n",
      "     44       39.7802  0.0410\n",
      "     45       39.7802  0.0410\n",
      "     46       39.7802  0.0410\n",
      "     47       39.7802  0.0400\n",
      "     48       39.7802  0.0420\n",
      "     49       39.7802  0.0400\n",
      "     50       39.7802  0.0400\n",
      "     51       39.7802  0.0400\n",
      "     52       39.7802  0.0410\n",
      "     53       39.7802  0.0410\n",
      "     54       39.7802  0.0400\n",
      "     55       39.7802  0.0420\n",
      "     56       39.7802  0.0400\n",
      "     57       39.7802  0.0400\n",
      "     58       39.7802  0.0410\n",
      "     59       39.7802  0.0400\n",
      "     60       39.7802  0.0400\n",
      "     61       39.7802  0.0410\n",
      "     62       39.7802  0.0410\n",
      "     63       39.7802  0.0410\n",
      "     64       39.7802  0.0410\n",
      "     65       39.7802  0.0410\n",
      "     66       39.7802  0.0400\n",
      "     67       39.7802  0.0410\n",
      "     68       39.7802  0.0400\n",
      "     69       39.7802  0.0420\n",
      "     70       39.7802  0.0410\n",
      "     71       39.7802  0.0410\n",
      "     72       39.7802  0.0410\n",
      "     73       39.7802  0.0410\n",
      "     74       39.7802  0.0410\n",
      "     75       39.7802  0.0410\n",
      "     76       39.7802  0.0410\n",
      "     77       39.7802  0.0410\n",
      "     78       39.7802  0.0430\n",
      "     79       39.7802  0.0410\n",
      "     80       39.7802  0.0410\n",
      "     81       39.7802  0.0410\n",
      "     82       39.7802  0.0410\n",
      "     83       39.7802  0.0410\n",
      "     84       39.7802  0.0420\n",
      "     85       39.7802  0.0410\n",
      "     86       39.7802  0.0410\n",
      "     87       39.7802  0.0410\n",
      "     88       39.7802  0.0410\n",
      "     89       39.7802  0.0410\n",
      "     90       39.7802  0.0410\n",
      "     91       39.7802  0.0410\n",
      "     92       39.7802  0.0420\n",
      "     93       39.7802  0.0410\n",
      "     94       39.7802  0.0420\n",
      "     95       39.7802  0.0420\n",
      "     96       39.7802  0.0420\n",
      "     97       39.7802  0.0410\n",
      "     98       39.7802  0.0420\n",
      "     99       39.7802  0.0420\n",
      "    100       39.7802  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m62.7193\u001b[0m  0.0410\n",
      "      2       62.7193  0.0400\n",
      "      3       62.7193  0.0400\n",
      "      4       62.7193  0.0390\n",
      "      5       62.7193  0.0400\n",
      "      6       62.7193  0.0390\n",
      "      7       62.7193  0.0400\n",
      "      8       62.7193  0.0390\n",
      "      9       62.7193  0.0400\n",
      "     10       62.7193  0.0400\n",
      "     11       62.7193  0.0410\n",
      "     12       62.7193  0.0400\n",
      "     13       62.7193  0.0400\n",
      "     14       62.7193  0.0390\n",
      "     15       62.7193  0.0400\n",
      "     16       \u001b[36m62.6444\u001b[0m  0.0400\n",
      "     17       \u001b[36m62.6115\u001b[0m  0.0410\n",
      "     18       63.4399  0.0430\n",
      "     19       \u001b[36m62.0004\u001b[0m  0.0400\n",
      "     20       \u001b[36m60.8844\u001b[0m  0.0400\n",
      "     21       \u001b[36m53.3188\u001b[0m  0.0400\n",
      "     22       59.1191  0.0400\n",
      "     23       55.8855  0.0390\n",
      "     24       \u001b[36m52.1973\u001b[0m  0.0390\n",
      "     25       \u001b[36m51.0714\u001b[0m  0.0400\n",
      "     26       \u001b[36m51.0714\u001b[0m  0.0420\n",
      "     27       51.0714  0.0390\n",
      "     28       51.0714  0.0390\n",
      "     29       51.0714  0.0400\n",
      "     30       51.0714  0.0400\n",
      "     31       51.0714  0.0400\n",
      "     32       \u001b[36m51.0713\u001b[0m  0.0400\n",
      "     33       \u001b[36m51.0712\u001b[0m  0.0400\n",
      "     34       51.0712  0.0400\n",
      "     35       51.0712  0.0390\n",
      "     36       51.0712  0.0400\n",
      "     37       51.0712  0.0420\n",
      "     38       51.0712  0.0400\n",
      "     39       51.0712  0.0400\n",
      "     40       51.0712  0.0400\n",
      "     41       51.0713  0.0410\n",
      "     42       51.0713  0.0400\n",
      "     43       51.0713  0.0420\n",
      "     44       51.0713  0.0410\n",
      "     45       \u001b[36m51.0035\u001b[0m  0.0410\n",
      "     46       \u001b[36m50.9603\u001b[0m  0.0400\n",
      "     47       \u001b[36m50.8971\u001b[0m  0.0390\n",
      "     48       \u001b[36m49.7124\u001b[0m  0.0400\n",
      "     49       49.7129  0.0400\n",
      "     50       49.7136  0.0410\n",
      "     51       49.7144  0.0400\n",
      "     52       49.7151  0.0400\n",
      "     53       49.7158  0.0400\n",
      "     54       49.7166  0.0400\n",
      "     55       49.7174  0.0400\n",
      "     56       49.7181  0.0400\n",
      "     57       49.7189  0.0410\n",
      "     58       49.7197  0.0400\n",
      "     59       49.7204  0.0400\n",
      "     60       49.7212  0.0410\n",
      "     61       49.7220  0.0410\n",
      "     62       49.7228  0.0400\n",
      "     63       49.7236  0.0400\n",
      "     64       49.7244  0.0400\n",
      "     65       49.7252  0.0400\n",
      "     66       49.7261  0.0410\n",
      "     67       49.7269  0.0410\n",
      "     68       49.7278  0.0410\n",
      "     69       49.7286  0.0390\n",
      "     70       49.7293  0.0410\n",
      "     71       49.7299  0.0400\n",
      "     72       49.7301  0.0410\n",
      "     73       49.7300  0.0400\n",
      "     74       49.7298  0.0420\n",
      "     75       49.7296  0.0420\n",
      "     76       49.7294  0.0410\n",
      "     77       49.7292  0.0440\n",
      "     78       49.7289  0.0400\n",
      "     79       49.7287  0.0400\n",
      "     80       49.7284  0.0410\n",
      "     81       49.7282  0.0410\n",
      "     82       49.7279  0.0400\n",
      "     83       49.7276  0.0400\n",
      "     84       49.7273  0.0420\n",
      "     85       49.7270  0.0400\n",
      "     86       49.7267  0.0410\n",
      "     87       49.7264  0.0410\n",
      "     88       49.7260  0.0410\n",
      "     89       49.7257  0.0410\n",
      "     90       49.7253  0.0420\n",
      "     91       49.7250  0.0420\n",
      "     92       49.7245  0.0410\n",
      "     93       49.7240  0.0410\n",
      "     94       49.7235  0.0410\n",
      "     95       49.7230  0.0410\n",
      "     96       49.7225  0.0400\n",
      "     97       49.7220  0.0420\n",
      "     98       49.7215  0.0410\n",
      "     99       49.7210  0.0430\n",
      "    100       49.7206  0.0420\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m62.8571\u001b[0m  0.0290\n",
      "      2       62.8571  0.0300\n",
      "      3       62.8571  0.0300\n",
      "      4       62.8571  0.0290\n",
      "      5       62.8571  0.0300\n",
      "      6       62.8571  0.0300\n",
      "      7       62.8571  0.0300\n",
      "      8       62.8571  0.0300\n",
      "      9       62.8571  0.0300\n",
      "     10       62.8571  0.0300\n",
      "     11       62.8571  0.0300\n",
      "     12       62.8571  0.0320\n",
      "     13       62.8571  0.0310\n",
      "     14       62.8571  0.0310\n",
      "     15       62.8571  0.0310\n",
      "     16       62.8571  0.0300\n",
      "     17       62.8571  0.0300\n",
      "     18       62.8571  0.0300\n",
      "     19       62.8571  0.0310\n",
      "     20       62.8571  0.0300\n",
      "     21       62.8571  0.0310\n",
      "     22       62.8571  0.0300\n",
      "     23       62.8571  0.0300\n",
      "     24       62.8571  0.0340\n",
      "     25       62.8571  0.0300\n",
      "     26       62.8571  0.0300\n",
      "     27       62.8571  0.0290\n",
      "     28       62.8571  0.0300\n",
      "     29       62.8571  0.0300\n",
      "     30       62.8571  0.0300\n",
      "     31       62.8571  0.0310\n",
      "     32       62.8571  0.0300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     33       62.8571  0.0290\n",
      "     34       62.8571  0.0300\n",
      "     35       62.8571  0.0290\n",
      "     36       62.8571  0.0290\n",
      "     37       62.8571  0.0300\n",
      "     38       62.8571  0.0290\n",
      "     39       62.8571  0.0310\n",
      "     40       62.8571  0.0300\n",
      "     41       62.8571  0.0300\n",
      "     42       62.8571  0.0310\n",
      "     43       62.8571  0.0310\n",
      "     44       62.8571  0.0300\n",
      "     45       62.8571  0.0300\n",
      "     46       62.8571  0.0300\n",
      "     47       62.8571  0.0300\n",
      "     48       62.8571  0.0300\n",
      "     49       62.8571  0.0300\n",
      "     50       62.8571  0.0300\n",
      "     51       62.8571  0.0300\n",
      "     52       62.8571  0.0290\n",
      "     53       62.8571  0.0300\n",
      "     54       62.8571  0.0300\n",
      "     55       62.8571  0.0300\n",
      "     56       62.8571  0.0300\n",
      "     57       62.8571  0.0300\n",
      "     58       62.8571  0.0330\n",
      "     59       62.8571  0.0300\n",
      "     60       62.8571  0.0300\n",
      "     61       62.8571  0.0310\n",
      "     62       62.8571  0.0310\n",
      "     63       62.8571  0.0310\n",
      "     64       62.8571  0.0300\n",
      "     65       62.8571  0.0300\n",
      "     66       62.8571  0.0300\n",
      "     67       62.8571  0.0300\n",
      "     68       62.8571  0.0300\n",
      "     69       62.8571  0.0300\n",
      "     70       62.8571  0.0300\n",
      "     71       62.8571  0.0300\n",
      "     72       62.8571  0.0300\n",
      "     73       62.8571  0.0300\n",
      "     74       62.8571  0.0300\n",
      "     75       62.8571  0.0310\n",
      "     76       62.8571  0.0300\n",
      "     77       62.8571  0.0310\n",
      "     78       62.8571  0.0300\n",
      "     79       62.8571  0.0300\n",
      "     80       62.8571  0.0290\n",
      "     81       62.8571  0.0300\n",
      "     82       62.8571  0.0310\n",
      "     83       62.8571  0.0300\n",
      "     84       62.8571  0.0300\n",
      "     85       62.8571  0.0310\n",
      "     86       62.8571  0.0300\n",
      "     87       62.8571  0.0310\n",
      "     88       62.8571  0.0300\n",
      "     89       62.8571  0.0300\n",
      "     90       62.8571  0.0310\n",
      "     91       62.8571  0.0300\n",
      "     92       62.8571  0.0300\n",
      "     93       62.8571  0.0300\n",
      "     94       62.8571  0.0300\n",
      "     95       62.8571  0.0310\n",
      "     96       62.8571  0.0300\n",
      "     97       62.8571  0.0300\n",
      "     98       62.8571  0.0300\n",
      "     99       62.8571  0.0310\n",
      "    100       62.8571  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m34.7782\u001b[0m  0.0290\n",
      "      2       37.1429  0.0300\n",
      "      3       37.1429  0.0300\n",
      "      4       37.1429  0.0300\n",
      "      5       37.1429  0.0290\n",
      "      6       37.1429  0.0300\n",
      "      7       37.1429  0.0300\n",
      "      8       37.1429  0.0310\n",
      "      9       37.1429  0.0290\n",
      "     10       37.1429  0.0300\n",
      "     11       37.1429  0.0300\n",
      "     12       37.1429  0.0300\n",
      "     13       37.1429  0.0300\n",
      "     14       37.1429  0.0300\n",
      "     15       37.1429  0.0310\n",
      "     16       37.1429  0.0300\n",
      "     17       37.1429  0.0300\n",
      "     18       37.1429  0.0300\n",
      "     19       37.1429  0.0300\n",
      "     20       37.1429  0.0300\n",
      "     21       37.1429  0.0310\n",
      "     22       37.1429  0.0310\n",
      "     23       37.1429  0.0300\n",
      "     24       37.1429  0.0300\n",
      "     25       37.1429  0.0300\n",
      "     26       37.1429  0.0300\n",
      "     27       37.1429  0.0300\n",
      "     28       37.1429  0.0300\n",
      "     29       37.1429  0.0300\n",
      "     30       37.1429  0.0290\n",
      "     31       37.1429  0.0300\n",
      "     32       37.1429  0.0300\n",
      "     33       37.1429  0.0300\n",
      "     34       37.1429  0.0300\n",
      "     35       37.1429  0.0300\n",
      "     36       37.1429  0.0300\n",
      "     37       37.1429  0.0350\n",
      "     38       37.1429  0.0320\n",
      "     39       37.1429  0.0320\n",
      "     40       37.1429  0.0300\n",
      "     41       37.1429  0.0300\n",
      "     42       37.1429  0.0300\n",
      "     43       37.1429  0.0320\n",
      "     44       37.1429  0.0300\n",
      "     45       37.1429  0.0300\n",
      "     46       37.1429  0.0300\n",
      "     47       37.1429  0.0300\n",
      "     48       37.1429  0.0300\n",
      "     49       37.1429  0.0310\n",
      "     50       37.1429  0.0300\n",
      "     51       37.1429  0.0300\n",
      "     52       37.1429  0.0300\n",
      "     53       37.1429  0.0310\n",
      "     54       37.1429  0.0300\n",
      "     55       37.1429  0.0310\n",
      "     56       37.1429  0.0300\n",
      "     57       37.1429  0.0300\n",
      "     58       37.1429  0.0360\n",
      "     59       37.1429  0.0310\n",
      "     60       37.1429  0.0320\n",
      "     61       37.1429  0.0290\n",
      "     62       37.1429  0.0300\n",
      "     63       37.1429  0.0350\n",
      "     64       37.1429  0.0300\n",
      "     65       37.1429  0.0330\n",
      "     66       37.1429  0.0290\n",
      "     67       37.1429  0.0300\n",
      "     68       37.1429  0.0300\n",
      "     69       37.1429  0.0300\n",
      "     70       37.1429  0.0320\n",
      "     71       37.1429  0.0310\n",
      "     72       37.1429  0.0300\n",
      "     73       37.1429  0.0310\n",
      "     74       37.1429  0.0300\n",
      "     75       37.1429  0.0310\n",
      "     76       37.1429  0.0300\n",
      "     77       37.1429  0.0300\n",
      "     78       37.1429  0.0340\n",
      "     79       37.1429  0.0310\n",
      "     80       37.1429  0.0300\n",
      "     81       37.1429  0.0300\n",
      "     82       37.1429  0.0310\n",
      "     83       37.1429  0.0300\n",
      "     84       37.1429  0.0310\n",
      "     85       37.1429  0.0300\n",
      "     86       37.1429  0.0300\n",
      "     87       37.1429  0.0310\n",
      "     88       37.1429  0.0300\n",
      "     89       37.1429  0.0300\n",
      "     90       37.1429  0.0310\n",
      "     91       37.1429  0.0310\n",
      "     92       37.1429  0.0300\n",
      "     93       37.1429  0.0300\n",
      "     94       37.1429  0.0310\n",
      "     95       37.1429  0.0300\n",
      "     96       37.1429  0.0300\n",
      "     97       37.1429  0.0300\n",
      "     98       37.1429  0.0310\n",
      "     99       37.1429  0.0310\n",
      "    100       37.1429  0.0290\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.3626\u001b[0m  0.0290\n",
      "      2       37.3626  0.0310\n",
      "      3       37.3626  0.0300\n",
      "      4       37.3626  0.0300\n",
      "      5       37.3626  0.0300\n",
      "      6       37.3626  0.0290\n",
      "      7       37.3626  0.0310\n",
      "      8       37.3626  0.0300\n",
      "      9       37.3626  0.0290\n",
      "     10       37.3626  0.0300\n",
      "     11       37.3626  0.0300\n",
      "     12       37.3626  0.0300\n",
      "     13       37.3626  0.0300\n",
      "     14       37.3626  0.0300\n",
      "     15       37.3626  0.0310\n",
      "     16       37.3626  0.0300\n",
      "     17       37.3626  0.0310\n",
      "     18       37.3626  0.0300\n",
      "     19       37.3626  0.0300\n",
      "     20       37.3626  0.0300\n",
      "     21       37.3626  0.0290\n",
      "     22       37.3626  0.0290\n",
      "     23       37.3626  0.0300\n",
      "     24       37.3626  0.0300\n",
      "     25       37.3626  0.0310\n",
      "     26       37.3626  0.0300\n",
      "     27       37.3626  0.0300\n",
      "     28       37.3626  0.0300\n",
      "     29       37.3626  0.0290\n",
      "     30       37.3626  0.0300\n",
      "     31       37.3626  0.0310\n",
      "     32       37.3626  0.0290\n",
      "     33       37.3626  0.0300\n",
      "     34       37.3626  0.0300\n",
      "     35       37.3626  0.0300\n",
      "     36       37.3626  0.0300\n",
      "     37       37.3626  0.0300\n",
      "     38       37.3626  0.0300\n",
      "     39       37.3626  0.0300\n",
      "     40       37.3626  0.0300\n",
      "     41       37.3626  0.0300\n",
      "     42       37.3626  0.0300\n",
      "     43       37.3626  0.0350\n",
      "     44       37.3626  0.0300\n",
      "     45       37.3626  0.0300\n",
      "     46       37.3626  0.0300\n",
      "     47       37.3626  0.0300\n",
      "     48       37.3626  0.0300\n",
      "     49       37.3626  0.0300\n",
      "     50       37.3626  0.0300\n",
      "     51       37.3626  0.0310\n",
      "     52       37.3626  0.0300\n",
      "     53       37.3626  0.0300\n",
      "     54       37.3626  0.0300\n",
      "     55       37.3626  0.0310\n",
      "     56       37.3626  0.0300\n",
      "     57       37.3626  0.0310\n",
      "     58       37.3626  0.0300\n",
      "     59       37.3626  0.0300\n",
      "     60       37.3626  0.0300\n",
      "     61       37.3626  0.0300\n",
      "     62       37.3626  0.0300\n",
      "     63       37.3626  0.0300\n",
      "     64       37.3626  0.0290\n",
      "     65       37.3626  0.0300\n",
      "     66       37.3626  0.0320\n",
      "     67       37.3626  0.0320\n",
      "     68       37.3626  0.0300\n",
      "     69       37.3626  0.0310\n",
      "     70       37.3626  0.0300\n",
      "     71       37.3626  0.0300\n",
      "     72       37.3626  0.0300\n",
      "     73       37.3626  0.0310\n",
      "     74       37.3626  0.0300\n",
      "     75       37.3626  0.0300\n",
      "     76       37.3626  0.0310\n",
      "     77       37.3626  0.0290\n",
      "     78       37.3626  0.0310\n",
      "     79       37.3626  0.0300\n",
      "     80       37.3626  0.0310\n",
      "     81       37.3626  0.0300\n",
      "     82       37.3626  0.0300\n",
      "     83       37.3626  0.0300\n",
      "     84       37.3626  0.0300\n",
      "     85       37.3626  0.0300\n",
      "     86       37.3626  0.0300\n",
      "     87       37.3626  0.0300\n",
      "     88       37.3626  0.0300\n",
      "     89       37.3626  0.0310\n",
      "     90       37.3626  0.0290\n",
      "     91       37.3626  0.0310\n",
      "     92       37.3626  0.0300\n",
      "     93       37.3626  0.0300\n",
      "     94       37.3626  0.0310\n",
      "     95       37.3626  0.0300\n",
      "     96       37.3626  0.0300\n",
      "     97       37.3626  0.0300\n",
      "     98       37.3626  0.0300\n",
      "     99       37.3626  0.0290\n",
      "    100       37.3626  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m34.1239\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2       39.1209  0.0300\n",
      "      3       39.1209  0.0300\n",
      "      4       39.1209  0.0300\n",
      "      5       39.1209  0.0300\n",
      "      6       39.1209  0.0300\n",
      "      7       39.1209  0.0300\n",
      "      8       39.1209  0.0300\n",
      "      9       39.1209  0.0300\n",
      "     10       39.1209  0.0300\n",
      "     11       39.1209  0.0300\n",
      "     12       39.1209  0.0310\n",
      "     13       39.1209  0.0290\n",
      "     14       39.1209  0.0300\n",
      "     15       39.1209  0.0290\n",
      "     16       39.1209  0.0310\n",
      "     17       39.1209  0.0300\n",
      "     18       39.1209  0.0290\n",
      "     19       39.1209  0.0300\n",
      "     20       39.1209  0.0300\n",
      "     21       39.1209  0.0300\n",
      "     22       39.1209  0.0300\n",
      "     23       39.1209  0.0300\n",
      "     24       39.1209  0.0300\n",
      "     25       39.1209  0.0290\n",
      "     26       39.1209  0.0290\n",
      "     27       39.1209  0.0310\n",
      "     28       39.1209  0.0300\n",
      "     29       39.1209  0.0290\n",
      "     30       39.1209  0.0320\n",
      "     31       39.1209  0.0300\n",
      "     32       39.1209  0.0300\n",
      "     33       39.1209  0.0300\n",
      "     34       39.1209  0.0300\n",
      "     35       39.1209  0.0300\n",
      "     36       39.1209  0.0300\n",
      "     37       39.1209  0.0300\n",
      "     38       39.1209  0.0300\n",
      "     39       39.1209  0.0300\n",
      "     40       39.1209  0.0300\n",
      "     41       39.1209  0.0310\n",
      "     42       39.1209  0.0290\n",
      "     43       39.1209  0.0310\n",
      "     44       39.1209  0.0290\n",
      "     45       39.1209  0.0310\n",
      "     46       39.1209  0.0300\n",
      "     47       39.1209  0.0310\n",
      "     48       39.1209  0.0300\n",
      "     49       39.1209  0.0290\n",
      "     50       39.1209  0.0310\n",
      "     51       39.1209  0.0290\n",
      "     52       39.1209  0.0300\n",
      "     53       39.1209  0.0290\n",
      "     54       39.1209  0.0310\n",
      "     55       39.1209  0.0310\n",
      "     56       39.1209  0.0300\n",
      "     57       39.1209  0.0300\n",
      "     58       39.1209  0.0300\n",
      "     59       39.1209  0.0300\n",
      "     60       39.1209  0.0300\n",
      "     61       39.1209  0.0290\n",
      "     62       39.1209  0.0300\n",
      "     63       39.1209  0.0300\n",
      "     64       39.1209  0.0300\n",
      "     65       39.1209  0.0300\n",
      "     66       39.1209  0.0300\n",
      "     67       39.1209  0.0300\n",
      "     68       39.1209  0.0300\n",
      "     69       39.1209  0.0290\n",
      "     70       39.1209  0.0310\n",
      "     71       39.1209  0.0300\n",
      "     72       39.1209  0.0300\n",
      "     73       39.1209  0.0300\n",
      "     74       39.1209  0.0300\n",
      "     75       39.1209  0.0300\n",
      "     76       39.1209  0.0310\n",
      "     77       39.1209  0.0300\n",
      "     78       39.1209  0.0300\n",
      "     79       39.1209  0.0300\n",
      "     80       39.1209  0.0300\n",
      "     81       39.1209  0.0300\n",
      "     82       39.1209  0.0300\n",
      "     83       39.1209  0.0300\n",
      "     84       39.1209  0.0300\n",
      "     85       39.1209  0.0300\n",
      "     86       39.1209  0.0310\n",
      "     87       39.1209  0.0310\n",
      "     88       39.1209  0.0300\n",
      "     89       39.1209  0.0310\n",
      "     90       39.1209  0.0300\n",
      "     91       39.1209  0.0300\n",
      "     92       39.1209  0.0290\n",
      "     93       39.1209  0.0290\n",
      "     94       39.1209  0.0300\n",
      "     95       39.1209  0.0300\n",
      "     96       39.1209  0.0290\n",
      "     97       39.1209  0.0300\n",
      "     98       39.1209  0.0300\n",
      "     99       39.1209  0.0300\n",
      "    100       39.1209  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m62.7193\u001b[0m  0.0290\n",
      "      2       62.7193  0.0300\n",
      "      3       62.7193  0.0300\n",
      "      4       62.7193  0.0300\n",
      "      5       62.7193  0.0300\n",
      "      6       62.7193  0.0310\n",
      "      7       62.7193  0.0300\n",
      "      8       62.7193  0.0310\n",
      "      9       62.7193  0.0340\n",
      "     10       62.7193  0.0300\n",
      "     11       62.7193  0.0300\n",
      "     12       62.7193  0.0300\n",
      "     13       62.7193  0.0300\n",
      "     14       62.7193  0.0300\n",
      "     15       62.7193  0.0290\n",
      "     16       62.7193  0.0300\n",
      "     17       62.7193  0.0300\n",
      "     18       62.7193  0.0310\n",
      "     19       62.7193  0.0310\n",
      "     20       62.7193  0.0290\n",
      "     21       62.7193  0.0300\n",
      "     22       62.7193  0.0300\n",
      "     23       62.7193  0.0300\n",
      "     24       62.7193  0.0300\n",
      "     25       62.7193  0.0300\n",
      "     26       62.7193  0.0300\n",
      "     27       62.7193  0.0300\n",
      "     28       62.7193  0.0310\n",
      "     29       62.7193  0.0300\n",
      "     30       62.7193  0.0300\n",
      "     31       62.7193  0.0310\n",
      "     32       62.7193  0.0310\n",
      "     33       62.7193  0.0300\n",
      "     34       62.7193  0.0290\n",
      "     35       62.7193  0.0290\n",
      "     36       62.7193  0.0300\n",
      "     37       62.7193  0.0300\n",
      "     38       62.7193  0.0300\n",
      "     39       62.7193  0.0300\n",
      "     40       62.7193  0.0300\n",
      "     41       62.7193  0.0310\n",
      "     42       62.7193  0.0290\n",
      "     43       62.7193  0.0330\n",
      "     44       62.7193  0.0310\n",
      "     45       62.7193  0.0300\n",
      "     46       62.7193  0.0300\n",
      "     47       62.7193  0.0300\n",
      "     48       62.7193  0.0300\n",
      "     49       62.7193  0.0300\n",
      "     50       62.7193  0.0310\n",
      "     51       62.7193  0.0300\n",
      "     52       62.7193  0.0310\n",
      "     53       62.7193  0.0300\n",
      "     54       62.7193  0.0300\n",
      "     55       62.7193  0.0300\n",
      "     56       62.7193  0.0300\n",
      "     57       62.7193  0.0300\n",
      "     58       62.7193  0.0330\n",
      "     59       62.7193  0.0300\n",
      "     60       62.7193  0.0310\n",
      "     61       62.7193  0.0300\n",
      "     62       62.7193  0.0300\n",
      "     63       62.7193  0.0300\n",
      "     64       62.7193  0.0310\n",
      "     65       62.7193  0.0310\n",
      "     66       62.7193  0.0300\n",
      "     67       62.7193  0.0300\n",
      "     68       62.7193  0.0310\n",
      "     69       62.7193  0.0310\n",
      "     70       62.7193  0.0310\n",
      "     71       62.7193  0.0300\n",
      "     72       62.7193  0.0300\n",
      "     73       62.7193  0.0300\n",
      "     74       62.7193  0.0300\n",
      "     75       62.7193  0.0310\n",
      "     76       62.7193  0.0300\n",
      "     77       62.7193  0.0310\n",
      "     78       62.7193  0.0300\n",
      "     79       62.7193  0.0310\n",
      "     80       62.7193  0.0290\n",
      "     81       62.7193  0.0300\n",
      "     82       62.7193  0.0300\n",
      "     83       62.7193  0.0310\n",
      "     84       62.7193  0.0320\n",
      "     85       62.7193  0.0290\n",
      "     86       62.7193  0.0300\n",
      "     87       62.7193  0.0310\n",
      "     88       62.7193  0.0290\n",
      "     89       62.7193  0.0300\n",
      "     90       62.7193  0.0310\n",
      "     91       62.7193  0.0300\n",
      "     92       62.7193  0.0310\n",
      "     93       62.7193  0.0300\n",
      "     94       62.7193  0.0300\n",
      "     95       62.7193  0.0300\n",
      "     96       62.7193  0.0310\n",
      "     97       62.7193  0.0300\n",
      "     98       62.7193  0.0300\n",
      "     99       62.7193  0.0300\n",
      "    100       62.7193  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9584\u001b[0m  0.0470\n",
      "      2        \u001b[36m0.8913\u001b[0m  0.0390\n",
      "      3        \u001b[36m0.8310\u001b[0m  0.0400\n",
      "      4        \u001b[36m0.7812\u001b[0m  0.0400\n",
      "      5        \u001b[36m0.7426\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.7141\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.6942\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.6811\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6732\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6688\u001b[0m  0.0390\n",
      "     11        \u001b[36m0.6664\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6651\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6644\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6640\u001b[0m  0.0400\n",
      "     15        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     16        \u001b[36m0.6635\u001b[0m  0.0400\n",
      "     17        \u001b[36m0.6634\u001b[0m  0.0390\n",
      "     18        \u001b[36m0.6633\u001b[0m  0.0390\n",
      "     19        \u001b[36m0.6633\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6632\u001b[0m  0.0390\n",
      "     21        \u001b[36m0.6632\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6632\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6631\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.6631\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.6631\u001b[0m  0.0390\n",
      "     26        \u001b[36m0.6631\u001b[0m  0.0400\n",
      "     27        \u001b[36m0.6631\u001b[0m  0.0400\n",
      "     28        \u001b[36m0.6631\u001b[0m  0.0400\n",
      "     29        \u001b[36m0.6630\u001b[0m  0.0390\n",
      "     30        \u001b[36m0.6630\u001b[0m  0.0390\n",
      "     31        \u001b[36m0.6630\u001b[0m  0.0400\n",
      "     32        \u001b[36m0.6630\u001b[0m  0.0400\n",
      "     33        0.6631  0.0390\n",
      "     34        \u001b[36m0.6630\u001b[0m  0.0400\n",
      "     35        \u001b[36m0.6630\u001b[0m  0.0400\n",
      "     36        \u001b[36m0.6630\u001b[0m  0.0400\n",
      "     37        \u001b[36m0.6630\u001b[0m  0.0390\n",
      "     38        0.6645  0.0410\n",
      "     39        0.6659  0.0400\n",
      "     40        0.6653  0.0390\n",
      "     41        \u001b[36m0.6604\u001b[0m  0.0390\n",
      "     42        \u001b[36m0.6584\u001b[0m  0.0390\n",
      "     43        0.6611  0.0420\n",
      "     44        \u001b[36m0.6513\u001b[0m  0.0390\n",
      "     45        0.6529  0.0400\n",
      "     46        \u001b[36m0.6427\u001b[0m  0.0390\n",
      "     47        0.6444  0.0390\n",
      "     48        \u001b[36m0.6394\u001b[0m  0.0390\n",
      "     49        \u001b[36m0.6299\u001b[0m  0.0390\n",
      "     50        0.6516  0.0400\n",
      "     51        0.6313  0.0400\n",
      "     52        \u001b[36m0.6279\u001b[0m  0.0400\n",
      "     53        \u001b[36m0.6206\u001b[0m  0.0400\n",
      "     54        0.6477  0.0400\n",
      "     55        0.6456  0.0400\n",
      "     56        0.6462  0.0400\n",
      "     57        0.6444  0.0420\n",
      "     58        0.6439  0.0400\n",
      "     59        0.6428  0.0400\n",
      "     60        0.6406  0.0390\n",
      "     61        0.6322  0.0400\n",
      "     62        \u001b[36m0.6106\u001b[0m  0.0410\n",
      "     63        \u001b[36m0.6083\u001b[0m  0.0390\n",
      "     64        \u001b[36m0.6021\u001b[0m  0.0400\n",
      "     65        0.6114  0.0410\n",
      "     66        \u001b[36m0.6007\u001b[0m  0.0410\n",
      "     67        0.6012  0.0400\n",
      "     68        \u001b[36m0.5917\u001b[0m  0.0390\n",
      "     69        0.6368  0.0400\n",
      "     70        0.6375  0.0390\n",
      "     71        0.6340  0.0400\n",
      "     72        0.6419  0.0410\n",
      "     73        0.6155  0.0400\n",
      "     74        \u001b[36m0.5858\u001b[0m  0.0400\n",
      "     75        0.5989  0.0400\n",
      "     76        0.5999  0.0390\n",
      "     77        0.5940  0.0400\n",
      "     78        0.5956  0.0400\n",
      "     79        0.6006  0.0400\n",
      "     80        0.5920  0.0410\n",
      "     81        0.5882  0.0400\n",
      "     82        0.6110  0.0400\n",
      "     83        0.5867  0.0420\n",
      "     84        \u001b[36m0.5814\u001b[0m  0.0390\n",
      "     85        0.5834  0.0410\n",
      "     86        \u001b[36m0.5792\u001b[0m  0.0400\n",
      "     87        \u001b[36m0.5744\u001b[0m  0.0410\n",
      "     88        0.5974  0.0400\n",
      "     89        0.6119  0.0400\n",
      "     90        0.6025  0.0400\n",
      "     91        0.5831  0.0400\n",
      "     92        0.5933  0.0400\n",
      "     93        0.5758  0.0400\n",
      "     94        0.5793  0.0400\n",
      "     95        0.5873  0.0400\n",
      "     96        \u001b[36m0.5688\u001b[0m  0.0440\n",
      "     97        0.5770  0.0400\n",
      "     98        0.5775  0.0400\n",
      "     99        0.5691  0.0400\n",
      "    100        \u001b[36m0.5685\u001b[0m  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2781\u001b[0m  0.0400\n",
      "      2        \u001b[36m1.1901\u001b[0m  0.0390\n",
      "      3        \u001b[36m1.1013\u001b[0m  0.0400\n",
      "      4        \u001b[36m1.0128\u001b[0m  0.0400\n",
      "      5        \u001b[36m0.9190\u001b[0m  0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.8133\u001b[0m  0.0410\n",
      "      7        \u001b[36m0.7229\u001b[0m  0.0390\n",
      "      8        \u001b[36m0.6821\u001b[0m  0.0390\n",
      "      9        \u001b[36m0.6707\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6675\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6663\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6658\u001b[0m  0.0390\n",
      "     13        \u001b[36m0.6654\u001b[0m  0.0390\n",
      "     14        \u001b[36m0.6652\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6651\u001b[0m  0.0390\n",
      "     16        \u001b[36m0.6650\u001b[0m  0.0390\n",
      "     17        \u001b[36m0.6649\u001b[0m  0.0390\n",
      "     18        \u001b[36m0.6648\u001b[0m  0.0390\n",
      "     19        \u001b[36m0.6647\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6647\u001b[0m  0.0390\n",
      "     21        \u001b[36m0.6646\u001b[0m  0.0400\n",
      "     22        \u001b[36m0.6646\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6646\u001b[0m  0.0390\n",
      "     24        \u001b[36m0.6645\u001b[0m  0.0390\n",
      "     25        \u001b[36m0.6645\u001b[0m  0.0390\n",
      "     26        \u001b[36m0.6645\u001b[0m  0.0400\n",
      "     27        \u001b[36m0.6644\u001b[0m  0.0390\n",
      "     28        0.6645  0.0400\n",
      "     29        \u001b[36m0.6644\u001b[0m  0.0400\n",
      "     30        \u001b[36m0.6643\u001b[0m  0.0390\n",
      "     31        \u001b[36m0.6643\u001b[0m  0.0400\n",
      "     32        0.6643  0.0390\n",
      "     33        0.6643  0.0400\n",
      "     34        0.6643  0.0390\n",
      "     35        0.6643  0.0410\n",
      "     36        \u001b[36m0.6643\u001b[0m  0.0400\n",
      "     37        \u001b[36m0.6643\u001b[0m  0.0390\n",
      "     38        \u001b[36m0.6642\u001b[0m  0.0400\n",
      "     39        \u001b[36m0.6642\u001b[0m  0.0390\n",
      "     40        \u001b[36m0.6642\u001b[0m  0.0400\n",
      "     41        \u001b[36m0.6642\u001b[0m  0.0380\n",
      "     42        \u001b[36m0.6642\u001b[0m  0.0400\n",
      "     43        \u001b[36m0.6642\u001b[0m  0.0390\n",
      "     44        \u001b[36m0.6642\u001b[0m  0.0400\n",
      "     45        \u001b[36m0.6641\u001b[0m  0.0390\n",
      "     46        \u001b[36m0.6641\u001b[0m  0.0400\n",
      "     47        \u001b[36m0.6641\u001b[0m  0.0390\n",
      "     48        \u001b[36m0.6641\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.6641\u001b[0m  0.0400\n",
      "     50        \u001b[36m0.6641\u001b[0m  0.0390\n",
      "     51        \u001b[36m0.6641\u001b[0m  0.0400\n",
      "     52        \u001b[36m0.6641\u001b[0m  0.0390\n",
      "     53        \u001b[36m0.6641\u001b[0m  0.0400\n",
      "     54        \u001b[36m0.6640\u001b[0m  0.0410\n",
      "     55        \u001b[36m0.6640\u001b[0m  0.0410\n",
      "     56        \u001b[36m0.6640\u001b[0m  0.0390\n",
      "     57        \u001b[36m0.6640\u001b[0m  0.0400\n",
      "     58        \u001b[36m0.6640\u001b[0m  0.0390\n",
      "     59        \u001b[36m0.6640\u001b[0m  0.0390\n",
      "     60        \u001b[36m0.6640\u001b[0m  0.0400\n",
      "     61        \u001b[36m0.6640\u001b[0m  0.0410\n",
      "     62        \u001b[36m0.6639\u001b[0m  0.0410\n",
      "     63        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     64        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     65        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     66        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     67        \u001b[36m0.6639\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     69        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     70        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     71        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     72        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     73        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     74        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     75        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     76        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     77        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     78        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     79        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     80        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     81        \u001b[36m0.6637\u001b[0m  0.0405\n",
      "     82        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     83        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     84        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     85        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     86        \u001b[36m0.6637\u001b[0m  0.0410\n",
      "     87        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     88        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     89        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     90        \u001b[36m0.6636\u001b[0m  0.0390\n",
      "     91        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     92        \u001b[36m0.6636\u001b[0m  0.0390\n",
      "     93        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     94        \u001b[36m0.6636\u001b[0m  0.0390\n",
      "     95        \u001b[36m0.6636\u001b[0m  0.0410\n",
      "     96        \u001b[36m0.6636\u001b[0m  0.0390\n",
      "     97        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     98        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     99        \u001b[36m0.6636\u001b[0m  0.0390\n",
      "    100        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.6689\u001b[0m  0.0400\n",
      "      2        \u001b[36m1.5576\u001b[0m  0.0400\n",
      "      3        \u001b[36m1.4376\u001b[0m  0.0430\n",
      "      4        \u001b[36m1.2933\u001b[0m  0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m1.0707\u001b[0m  0.0410\n",
      "      6        \u001b[36m0.7957\u001b[0m  0.0410\n",
      "      7        \u001b[36m0.6864\u001b[0m  0.0390\n",
      "      8        \u001b[36m0.6709\u001b[0m  0.0390\n",
      "      9        \u001b[36m0.6691\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6685\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6681\u001b[0m  0.0390\n",
      "     12        \u001b[36m0.6678\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6676\u001b[0m  0.0410\n",
      "     14        \u001b[36m0.6675\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6674\u001b[0m  0.0390\n",
      "     16        \u001b[36m0.6673\u001b[0m  0.0390\n",
      "     17        \u001b[36m0.6672\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6671\u001b[0m  0.0390\n",
      "     19        \u001b[36m0.6670\u001b[0m  0.0390\n",
      "     20        \u001b[36m0.6670\u001b[0m  0.0400\n",
      "     21        \u001b[36m0.6669\u001b[0m  0.0400\n",
      "     22        \u001b[36m0.6669\u001b[0m  0.0390\n",
      "     23        \u001b[36m0.6669\u001b[0m  0.0390\n",
      "     24        \u001b[36m0.6668\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.6667\u001b[0m  0.0390\n",
      "     26        0.6668  0.0390\n",
      "     27        0.6667  0.0400\n",
      "     28        0.6667  0.0400\n",
      "     29        \u001b[36m0.6667\u001b[0m  0.0400\n",
      "     30        \u001b[36m0.6667\u001b[0m  0.0390\n",
      "     31        \u001b[36m0.6666\u001b[0m  0.0390\n",
      "     32        \u001b[36m0.6666\u001b[0m  0.0400\n",
      "     33        \u001b[36m0.6665\u001b[0m  0.0390\n",
      "     34        0.6666  0.0400\n",
      "     35        0.6666  0.0390\n",
      "     36        \u001b[36m0.6665\u001b[0m  0.0410\n",
      "     37        \u001b[36m0.6665\u001b[0m  0.0400\n",
      "     38        \u001b[36m0.6665\u001b[0m  0.0390\n",
      "     39        \u001b[36m0.6665\u001b[0m  0.0400\n",
      "     40        \u001b[36m0.6665\u001b[0m  0.0390\n",
      "     41        \u001b[36m0.6664\u001b[0m  0.0390\n",
      "     42        \u001b[36m0.6664\u001b[0m  0.0390\n",
      "     43        \u001b[36m0.6663\u001b[0m  0.0410\n",
      "     44        \u001b[36m0.6662\u001b[0m  0.0400\n",
      "     45        \u001b[36m0.6661\u001b[0m  0.0400\n",
      "     46        \u001b[36m0.6659\u001b[0m  0.0400\n",
      "     47        \u001b[36m0.6657\u001b[0m  0.0400\n",
      "     48        \u001b[36m0.6656\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.6655\u001b[0m  0.0400\n",
      "     50        \u001b[36m0.6655\u001b[0m  0.0390\n",
      "     51        \u001b[36m0.6654\u001b[0m  0.0390\n",
      "     52        \u001b[36m0.6653\u001b[0m  0.0400\n",
      "     53        \u001b[36m0.6653\u001b[0m  0.0400\n",
      "     54        \u001b[36m0.6652\u001b[0m  0.0420\n",
      "     55        \u001b[36m0.6651\u001b[0m  0.0390\n",
      "     56        \u001b[36m0.6651\u001b[0m  0.0400\n",
      "     57        \u001b[36m0.6650\u001b[0m  0.0390\n",
      "     58        \u001b[36m0.6649\u001b[0m  0.0400\n",
      "     59        \u001b[36m0.6649\u001b[0m  0.0400\n",
      "     60        \u001b[36m0.6648\u001b[0m  0.0390\n",
      "     61        \u001b[36m0.6647\u001b[0m  0.0410\n",
      "     62        \u001b[36m0.6647\u001b[0m  0.0400\n",
      "     63        \u001b[36m0.6646\u001b[0m  0.0390\n",
      "     64        \u001b[36m0.6646\u001b[0m  0.0400\n",
      "     65        \u001b[36m0.6645\u001b[0m  0.0390\n",
      "     66        \u001b[36m0.6645\u001b[0m  0.0400\n",
      "     67        \u001b[36m0.6644\u001b[0m  0.0390\n",
      "     68        \u001b[36m0.6643\u001b[0m  0.0400\n",
      "     69        \u001b[36m0.6643\u001b[0m  0.0400\n",
      "     70        \u001b[36m0.6642\u001b[0m  0.0410\n",
      "     71        0.6668  0.0430\n",
      "     72        0.6644  0.0390\n",
      "     73        \u001b[36m0.6642\u001b[0m  0.0400\n",
      "     74        \u001b[36m0.6641\u001b[0m  0.0400\n",
      "     75        \u001b[36m0.6640\u001b[0m  0.0390\n",
      "     76        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     77        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     78        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     79        \u001b[36m0.6637\u001b[0m  0.0420\n",
      "     80        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     81        \u001b[36m0.6636\u001b[0m  0.0390\n",
      "     82        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     83        \u001b[36m0.6635\u001b[0m  0.0390\n",
      "     84        \u001b[36m0.6634\u001b[0m  0.0400\n",
      "     85        \u001b[36m0.6634\u001b[0m  0.0410\n",
      "     86        \u001b[36m0.6633\u001b[0m  0.0400\n",
      "     87        \u001b[36m0.6633\u001b[0m  0.0390\n",
      "     88        \u001b[36m0.6632\u001b[0m  0.0400\n",
      "     89        \u001b[36m0.6632\u001b[0m  0.0400\n",
      "     90        \u001b[36m0.6631\u001b[0m  0.0400\n",
      "     91        \u001b[36m0.6631\u001b[0m  0.0390\n",
      "     92        \u001b[36m0.6630\u001b[0m  0.0400\n",
      "     93        \u001b[36m0.6630\u001b[0m  0.0390\n",
      "     94        \u001b[36m0.6629\u001b[0m  0.0390\n",
      "     95        \u001b[36m0.6629\u001b[0m  0.0400\n",
      "     96        \u001b[36m0.6629\u001b[0m  0.0400\n",
      "     97        \u001b[36m0.6628\u001b[0m  0.0390\n",
      "     98        \u001b[36m0.6628\u001b[0m  0.0400\n",
      "     99        \u001b[36m0.6627\u001b[0m  0.0400\n",
      "    100        \u001b[36m0.6627\u001b[0m  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.6291\u001b[0m  0.0410\n",
      "      2        \u001b[36m1.5218\u001b[0m  0.0410\n",
      "      3        \u001b[36m1.4116\u001b[0m  0.0390\n",
      "      4        \u001b[36m1.2972\u001b[0m  0.0400\n",
      "      5        \u001b[36m1.1621\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.9628\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.7552\u001b[0m  0.0390\n",
      "      8        \u001b[36m0.6841\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6729\u001b[0m  0.0390\n",
      "     10        \u001b[36m0.6709\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6700\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6695\u001b[0m  0.0390\n",
      "     13        \u001b[36m0.6692\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6689\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6687\u001b[0m  0.0390\n",
      "     16        \u001b[36m0.6686\u001b[0m  0.0390\n",
      "     17        \u001b[36m0.6684\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6683\u001b[0m  0.0390\n",
      "     19        \u001b[36m0.6682\u001b[0m  0.0410\n",
      "     20        0.6686  0.0390\n",
      "     21        \u001b[36m0.6681\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6680\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6679\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.6679\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.6678\u001b[0m  0.0390\n",
      "     26        \u001b[36m0.6678\u001b[0m  0.0390\n",
      "     27        \u001b[36m0.6677\u001b[0m  0.0390\n",
      "     28        \u001b[36m0.6677\u001b[0m  0.0390\n",
      "     29        \u001b[36m0.6592\u001b[0m  0.0400\n",
      "     30        0.6725  0.0400\n",
      "     31        0.6695  0.0390\n",
      "     32        0.6695  0.0400\n",
      "     33        0.6670  0.0400\n",
      "     34        0.6670  0.0390\n",
      "     35        0.6669  0.0410\n",
      "     36        0.6668  0.0390\n",
      "     37        0.6667  0.0390\n",
      "     38        0.6667  0.0400\n",
      "     39        0.6666  0.0390\n",
      "     40        0.6666  0.0390\n",
      "     41        0.6665  0.0390\n",
      "     42        0.6665  0.0390\n",
      "     43        0.6664  0.0400\n",
      "     44        0.6664  0.0390\n",
      "     45        0.6664  0.0390\n",
      "     46        0.6663  0.0400\n",
      "     47        0.6663  0.0400\n",
      "     48        0.6663  0.0400\n",
      "     49        0.6663  0.0400\n",
      "     50        0.6662  0.0390\n",
      "     51        0.6662  0.0400\n",
      "     52        0.6662  0.0400\n",
      "     53        0.6662  0.0400\n",
      "     54        0.6661  0.0410\n",
      "     55        0.6661  0.0390\n",
      "     56        0.6661  0.0390\n",
      "     57        0.6661  0.0390\n",
      "     58        0.6660  0.0400\n",
      "     59        0.6660  0.0400\n",
      "     60        0.6660  0.0400\n",
      "     61        0.6660  0.0390\n",
      "     62        0.6660  0.0400\n",
      "     63        0.6659  0.0390\n",
      "     64        0.6659  0.0400\n",
      "     65        0.6659  0.0380\n",
      "     66        0.6659  0.0430\n",
      "     67        0.6658  0.0400\n",
      "     68        0.6658  0.0390\n",
      "     69        0.6658  0.0440\n",
      "     70        0.6658  0.0410\n",
      "     71        0.6658  0.0390\n",
      "     72        0.6657  0.0390\n",
      "     73        0.6657  0.0390\n",
      "     74        0.6657  0.0400\n",
      "     75        0.6657  0.0390\n",
      "     76        0.6657  0.0400\n",
      "     77        0.6656  0.0400\n",
      "     78        0.6656  0.0400\n",
      "     79        0.6656  0.0420\n",
      "     80        0.6656  0.0400\n",
      "     81        0.6656  0.0390\n",
      "     82        0.6655  0.0390\n",
      "     83        0.6655  0.0400\n",
      "     84        0.6655  0.0400\n",
      "     85        0.6655  0.0400\n",
      "     86        0.6655  0.0390\n",
      "     87        0.6654  0.0400\n",
      "     88        0.6654  0.0390\n",
      "     89        0.6654  0.0400\n",
      "     90        0.6654  0.0400\n",
      "     91        0.6654  0.0400\n",
      "     92        0.6653  0.0400\n",
      "     93        0.6653  0.0390\n",
      "     94        0.6653  0.0400\n",
      "     95        0.6653  0.0400\n",
      "     96        0.6653  0.0400\n",
      "     97        0.6652  0.0400\n",
      "     98        0.6652  0.0390\n",
      "     99        0.6652  0.0440\n",
      "    100        0.6652  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2314\u001b[0m  0.0400\n",
      "      2        \u001b[36m1.1372\u001b[0m  0.0400\n",
      "      3        \u001b[36m1.0458\u001b[0m  0.0410\n",
      "      4        \u001b[36m0.9579\u001b[0m  0.0400\n",
      "      5        \u001b[36m0.8714\u001b[0m  0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.7857\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.7181\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.6839\u001b[0m  0.0410\n",
      "      9        \u001b[36m0.6716\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6674\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6659\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6652\u001b[0m  0.0390\n",
      "     13        \u001b[36m0.6648\u001b[0m  0.0390\n",
      "     14        \u001b[36m0.6646\u001b[0m  0.0390\n",
      "     15        0.6658  0.0390\n",
      "     16        \u001b[36m0.6644\u001b[0m  0.0390\n",
      "     17        \u001b[36m0.6643\u001b[0m  0.0390\n",
      "     18        \u001b[36m0.6643\u001b[0m  0.0400\n",
      "     19        \u001b[36m0.6642\u001b[0m  0.0380\n",
      "     20        \u001b[36m0.6642\u001b[0m  0.0390\n",
      "     21        \u001b[36m0.6642\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6641\u001b[0m  0.0390\n",
      "     23        \u001b[36m0.6641\u001b[0m  0.0390\n",
      "     24        \u001b[36m0.6641\u001b[0m  0.0410\n",
      "     25        \u001b[36m0.6641\u001b[0m  0.0390\n",
      "     26        \u001b[36m0.6641\u001b[0m  0.0400\n",
      "     27        \u001b[36m0.6641\u001b[0m  0.0400\n",
      "     28        \u001b[36m0.6641\u001b[0m  0.0420\n",
      "     29        0.6658  0.0410\n",
      "     30        \u001b[36m0.6640\u001b[0m  0.0390\n",
      "     31        0.6640  0.0390\n",
      "     32        \u001b[36m0.6640\u001b[0m  0.0400\n",
      "     33        \u001b[36m0.6640\u001b[0m  0.0390\n",
      "     34        \u001b[36m0.6640\u001b[0m  0.0400\n",
      "     35        \u001b[36m0.6640\u001b[0m  0.0400\n",
      "     36        \u001b[36m0.6640\u001b[0m  0.0400\n",
      "     37        0.6642  0.0400\n",
      "     38        0.6641  0.0400\n",
      "     39        0.6647  0.0390\n",
      "     40        0.6641  0.0390\n",
      "     41        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     42        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     43        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     44        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     45        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     46        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     47        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     48        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     50        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     51        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     52        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     53        \u001b[36m0.6639\u001b[0m  0.0420\n",
      "     54        \u001b[36m0.6639\u001b[0m  0.0390\n",
      "     55        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     56        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     57        \u001b[36m0.6639\u001b[0m  0.0400\n",
      "     58        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     59        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     60        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     61        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     62        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     63        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     64        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     65        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     66        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     67        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     68        \u001b[36m0.6638\u001b[0m  0.0410\n",
      "     69        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     70        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     71        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     72        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     73        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     74        \u001b[36m0.6638\u001b[0m  0.0430\n",
      "     75        \u001b[36m0.6638\u001b[0m  0.0400\n",
      "     76        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     77        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     78        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     79        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     80        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     81        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     82        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     83        \u001b[36m0.6637\u001b[0m  0.0410\n",
      "     84        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     85        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     86        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     87        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     88        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     89        \u001b[36m0.6637\u001b[0m  0.0390\n",
      "     90        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     91        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     92        \u001b[36m0.6637\u001b[0m  0.0410\n",
      "     93        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     94        \u001b[36m0.6636\u001b[0m  0.0410\n",
      "     95        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     96        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     97        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "     98        \u001b[36m0.6636\u001b[0m  0.0410\n",
      "     99        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "    100        \u001b[36m0.6636\u001b[0m  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2864\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.2393\u001b[0m  0.0310\n",
      "      3        \u001b[36m1.1936\u001b[0m  0.0300\n",
      "      4        \u001b[36m1.1494\u001b[0m  0.0300\n",
      "      5        \u001b[36m1.1070\u001b[0m  0.0330\n",
      "      6        \u001b[36m1.0664\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.0277\u001b[0m  0.0300\n",
      "      8        \u001b[36m0.9912\u001b[0m  0.0300\n",
      "      9        \u001b[36m0.9568\u001b[0m  0.0300\n",
      "     10        \u001b[36m0.9247\u001b[0m  0.0310\n",
      "     11        \u001b[36m0.8949\u001b[0m  0.0290\n",
      "     12        \u001b[36m0.8674\u001b[0m  0.0300\n",
      "     13        \u001b[36m0.8423\u001b[0m  0.0290\n",
      "     14        \u001b[36m0.8194\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.7988\u001b[0m  0.0300\n",
      "     16        \u001b[36m0.7803\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.7638\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.7492\u001b[0m  0.0290\n",
      "     19        \u001b[36m0.7364\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.7252\u001b[0m  0.0290\n",
      "     21        \u001b[36m0.7155\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.7070\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.6998\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.6935\u001b[0m  0.0380\n",
      "     25        \u001b[36m0.6882\u001b[0m  0.0310\n",
      "     26        \u001b[36m0.6837\u001b[0m  0.0310\n",
      "     27        \u001b[36m0.6799\u001b[0m  0.0310\n",
      "     28        \u001b[36m0.6766\u001b[0m  0.0290\n",
      "     29        \u001b[36m0.6739\u001b[0m  0.0290\n",
      "     30        \u001b[36m0.6716\u001b[0m  0.0300\n",
      "     31        \u001b[36m0.6696\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.6680\u001b[0m  0.0290\n",
      "     33        \u001b[36m0.6667\u001b[0m  0.0310\n",
      "     34        \u001b[36m0.6655\u001b[0m  0.0310\n",
      "     35        \u001b[36m0.6646\u001b[0m  0.0290\n",
      "     36        \u001b[36m0.6638\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6632\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6626\u001b[0m  0.0290\n",
      "     39        \u001b[36m0.6622\u001b[0m  0.0290\n",
      "     40        \u001b[36m0.6618\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6612\u001b[0m  0.0310\n",
      "     43        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6608\u001b[0m  0.0290\n",
      "     45        \u001b[36m0.6607\u001b[0m  0.0290\n",
      "     46        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6605\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.6604\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6603\u001b[0m  0.0290\n",
      "     51        \u001b[36m0.6602\u001b[0m  0.0290\n",
      "     52        \u001b[36m0.6602\u001b[0m  0.0290\n",
      "     53        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6601\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6601\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6601\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.6601\u001b[0m  0.0290\n",
      "     58        \u001b[36m0.6601\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6601\u001b[0m  0.0290\n",
      "     60        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6600\u001b[0m  0.0290\n",
      "     64        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6600\u001b[0m  0.0310\n",
      "     68        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.6600\u001b[0m  0.0320\n",
      "     72        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6600\u001b[0m  0.0310\n",
      "     75        \u001b[36m0.6600\u001b[0m  0.0310\n",
      "     76        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6600\u001b[0m  0.0290\n",
      "     79        \u001b[36m0.6600\u001b[0m  0.0310\n",
      "     80        \u001b[36m0.6600\u001b[0m  0.0290\n",
      "     81        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     83        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6600\u001b[0m  0.0290\n",
      "     85        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.6600\u001b[0m  0.0310\n",
      "     88        \u001b[36m0.6600\u001b[0m  0.0310\n",
      "     89        0.6600  0.0300\n",
      "     90        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     91        0.6600  0.0300\n",
      "     92        0.6600  0.0300\n",
      "     93        \u001b[36m0.6600\u001b[0m  0.0310\n",
      "     94        0.6600  0.0300\n",
      "     95        0.6600  0.0290\n",
      "     96        0.6600  0.0300\n",
      "     97        \u001b[36m0.6600\u001b[0m  0.0300\n",
      "     98        0.6600  0.0310\n",
      "     99        0.6600  0.0350\n",
      "    100        0.6600  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.6493\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.5955\u001b[0m  0.0290\n",
      "      3        \u001b[36m1.5423\u001b[0m  0.0300\n",
      "      4        \u001b[36m1.4899\u001b[0m  0.0290\n",
      "      5        \u001b[36m1.4383\u001b[0m  0.0300\n",
      "      6        \u001b[36m1.3875\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.3378\u001b[0m  0.0320\n",
      "      8        \u001b[36m1.2893\u001b[0m  0.0290\n",
      "      9        \u001b[36m1.2420\u001b[0m  0.0290\n",
      "     10        \u001b[36m1.1961\u001b[0m  0.0300\n",
      "     11        \u001b[36m1.1518\u001b[0m  0.0300\n",
      "     12        \u001b[36m1.1092\u001b[0m  0.0290\n",
      "     13        \u001b[36m1.0684\u001b[0m  0.0310\n",
      "     14        \u001b[36m1.0296\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.9929\u001b[0m  0.0300\n",
      "     16        \u001b[36m0.9584\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.9261\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.8961\u001b[0m  0.0290\n",
      "     19        \u001b[36m0.8685\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.8432\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.8202\u001b[0m  0.0290\n",
      "     22        \u001b[36m0.7994\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.7808\u001b[0m  0.0290\n",
      "     24        \u001b[36m0.7642\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.7496\u001b[0m  0.0290\n",
      "     26        \u001b[36m0.7367\u001b[0m  0.0310\n",
      "     27        \u001b[36m0.7254\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.7156\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.7071\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.6998\u001b[0m  0.0290\n",
      "     31        \u001b[36m0.6936\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.6883\u001b[0m  0.0300\n",
      "     33        \u001b[36m0.6837\u001b[0m  0.0290\n",
      "     34        \u001b[36m0.6799\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.6766\u001b[0m  0.0290\n",
      "     36        \u001b[36m0.6739\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6716\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6697\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6681\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.6667\u001b[0m  0.0310\n",
      "     41        \u001b[36m0.6656\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6646\u001b[0m  0.0290\n",
      "     43        \u001b[36m0.6639\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6632\u001b[0m  0.0290\n",
      "     45        \u001b[36m0.6627\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6623\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6619\u001b[0m  0.0290\n",
      "     48        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6611\u001b[0m  0.0290\n",
      "     51        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6608\u001b[0m  0.0310\n",
      "     53        \u001b[36m0.6607\u001b[0m  0.0310\n",
      "     54        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6605\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6605\u001b[0m  0.0290\n",
      "     57        \u001b[36m0.6604\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.6604\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6603\u001b[0m  0.0290\n",
      "     61        \u001b[36m0.6603\u001b[0m  0.0290\n",
      "     62        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6602\u001b[0m  0.0330\n",
      "     68        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     72        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     74        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6602\u001b[0m  0.0290\n",
      "     80        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6602\u001b[0m  0.0290\n",
      "     82        0.6602  0.0300\n",
      "     83        0.6602  0.0290\n",
      "     84        0.6602  0.0300\n",
      "     85        0.6602  0.0300\n",
      "     86        0.6602  0.0300\n",
      "     87        0.6602  0.0290\n",
      "     88        0.6602  0.0300\n",
      "     89        0.6602  0.0300\n",
      "     90        0.6602  0.0300\n",
      "     91        0.6602  0.0300\n",
      "     92        0.6602  0.0300\n",
      "     93        0.6602  0.0290\n",
      "     94        0.6602  0.0290\n",
      "     95        0.6602  0.0300\n",
      "     96        0.6602  0.0310\n",
      "     97        0.6602  0.0300\n",
      "     98        0.6602  0.0300\n",
      "     99        0.6602  0.0290\n",
      "    100        0.6602  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.8890\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.8331\u001b[0m  0.0300\n",
      "      3        \u001b[36m1.7775\u001b[0m  0.0310\n",
      "      4        \u001b[36m1.7224\u001b[0m  0.0300\n",
      "      5        \u001b[36m1.6677\u001b[0m  0.0340\n",
      "      6        \u001b[36m1.6135\u001b[0m  0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.5599\u001b[0m  0.0300\n",
      "      8        \u001b[36m1.5071\u001b[0m  0.0290\n",
      "      9        \u001b[36m1.4550\u001b[0m  0.0290\n",
      "     10        \u001b[36m1.4039\u001b[0m  0.0300\n",
      "     11        \u001b[36m1.3537\u001b[0m  0.0300\n",
      "     12        \u001b[36m1.3047\u001b[0m  0.0300\n",
      "     13        \u001b[36m1.2569\u001b[0m  0.0300\n",
      "     14        \u001b[36m1.2105\u001b[0m  0.0300\n",
      "     15        \u001b[36m1.1657\u001b[0m  0.0300\n",
      "     16        \u001b[36m1.1225\u001b[0m  0.0300\n",
      "     17        \u001b[36m1.0812\u001b[0m  0.0300\n",
      "     18        \u001b[36m1.0418\u001b[0m  0.0300\n",
      "     19        \u001b[36m1.0044\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.9693\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.9363\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.9057\u001b[0m  0.0290\n",
      "     23        \u001b[36m0.8774\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.8515\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.8279\u001b[0m  0.0290\n",
      "     26        \u001b[36m0.8065\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.7873\u001b[0m  0.0290\n",
      "     28        \u001b[36m0.7701\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.7549\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.7416\u001b[0m  0.0300\n",
      "     31        \u001b[36m0.7298\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.7196\u001b[0m  0.0290\n",
      "     33        \u001b[36m0.7108\u001b[0m  0.0290\n",
      "     34        \u001b[36m0.7031\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.6966\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.6910\u001b[0m  0.0310\n",
      "     37        \u001b[36m0.6862\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6821\u001b[0m  0.0310\n",
      "     39        \u001b[36m0.6787\u001b[0m  0.0290\n",
      "     40        \u001b[36m0.6758\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6734\u001b[0m  0.0290\n",
      "     42        \u001b[36m0.6713\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.6696\u001b[0m  0.0290\n",
      "     44        \u001b[36m0.6682\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6670\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6660\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6651\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.6645\u001b[0m  0.0290\n",
      "     49        \u001b[36m0.6639\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6634\u001b[0m  0.0300\n",
      "     51        \u001b[36m0.6630\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6627\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6624\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6622\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6620\u001b[0m  0.0290\n",
      "     56        \u001b[36m0.6619\u001b[0m  0.0310\n",
      "     57        \u001b[36m0.6617\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.6616\u001b[0m  0.0290\n",
      "     59        \u001b[36m0.6615\u001b[0m  0.0290\n",
      "     60        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6614\u001b[0m  0.0310\n",
      "     63        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6613\u001b[0m  0.0290\n",
      "     66        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.6612\u001b[0m  0.0310\n",
      "     72        \u001b[36m0.6612\u001b[0m  0.0320\n",
      "     73        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     75        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     77        \u001b[36m0.6612\u001b[0m  0.0320\n",
      "     78        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     80        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     83        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     86        0.6612  0.0290\n",
      "     87        0.6612  0.0300\n",
      "     88        0.6612  0.0300\n",
      "     89        0.6612  0.0300\n",
      "     90        0.6612  0.0300\n",
      "     91        0.6612  0.0300\n",
      "     92        0.6612  0.0290\n",
      "     93        0.6612  0.0310\n",
      "     94        0.6612  0.0300\n",
      "     95        0.6612  0.0300\n",
      "     96        0.6612  0.0310\n",
      "     97        0.6612  0.0300\n",
      "     98        0.6612  0.0300\n",
      "     99        0.6612  0.0300\n",
      "    100        0.6612  0.0290\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2543\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.2077\u001b[0m  0.0360\n",
      "      3        \u001b[36m1.1627\u001b[0m  0.0330\n",
      "      4        \u001b[36m1.1194\u001b[0m  0.0300\n",
      "      5        \u001b[36m1.0779\u001b[0m  0.0340\n",
      "      6        \u001b[36m1.0384\u001b[0m  0.0290"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        \u001b[36m1.0010\u001b[0m  0.0290\n",
      "      8        \u001b[36m0.9658\u001b[0m  0.0290\n",
      "      9        \u001b[36m0.9329\u001b[0m  0.0290\n",
      "     10        \u001b[36m0.9024\u001b[0m  0.0300\n",
      "     11        \u001b[36m0.8742\u001b[0m  0.0300\n",
      "     12        \u001b[36m0.8484\u001b[0m  0.0290\n",
      "     13        \u001b[36m0.8249\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.8037\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.7846\u001b[0m  0.0300\n",
      "     16        \u001b[36m0.7677\u001b[0m  0.0290\n",
      "     17        \u001b[36m0.7527\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.7395\u001b[0m  0.0300\n",
      "     19        \u001b[36m0.7280\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.7180\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.7093\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.7018\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.6954\u001b[0m  0.0290\n",
      "     24        \u001b[36m0.6900\u001b[0m  0.0310\n",
      "     25        \u001b[36m0.6853\u001b[0m  0.0310\n",
      "     26        \u001b[36m0.6814\u001b[0m  0.0310\n",
      "     27        \u001b[36m0.6781\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.6753\u001b[0m  0.0310\n",
      "     29        \u001b[36m0.6729\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.6709\u001b[0m  0.0300\n",
      "     31        \u001b[36m0.6693\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.6679\u001b[0m  0.0290\n",
      "     33        \u001b[36m0.6667\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.6658\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.6650\u001b[0m  0.0290\n",
      "     36        \u001b[36m0.6643\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6638\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6633\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6630\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.6626\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6624\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6622\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.6620\u001b[0m  0.0290\n",
      "     44        \u001b[36m0.6619\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6617\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6614\u001b[0m  0.0290\n",
      "     51        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     58        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6612\u001b[0m  0.0310\n",
      "     62        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     63        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     65        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6612\u001b[0m  0.0310\n",
      "     70        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     71        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     76        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     78        0.6612  0.0340\n",
      "     79        0.6612  0.0300\n",
      "     80        0.6612  0.0290\n",
      "     81        0.6612  0.0300\n",
      "     82        0.6612  0.0300\n",
      "     83        0.6612  0.0300\n",
      "     84        0.6612  0.0300\n",
      "     85        0.6612  0.0300\n",
      "     86        0.6612  0.0310\n",
      "     87        0.6612  0.0310\n",
      "     88        0.6612  0.0300\n",
      "     89        0.6612  0.0290\n",
      "     90        0.6612  0.0310\n",
      "     91        0.6612  0.0290\n",
      "     92        0.6612  0.0310\n",
      "     93        0.6612  0.0300\n",
      "     94        0.6612  0.0310\n",
      "     95        0.6612  0.0290\n",
      "     96        0.6612  0.0300\n",
      "     97        0.6612  0.0300\n",
      "     98        0.6612  0.0300\n",
      "     99        0.6612  0.0310\n",
      "    100        0.6612  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.3332\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.2856\u001b[0m  0.0300\n",
      "      3        \u001b[36m1.2393\u001b[0m  0.0290\n",
      "      4        \u001b[36m1.1943\u001b[0m  0.0300\n",
      "      5        \u001b[36m1.1510\u001b[0m  0.0300\n",
      "      6        \u001b[36m1.1093\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.0694\u001b[0m  0.0300\n",
      "      8        \u001b[36m1.0314\u001b[0m  0.0300\n",
      "      9        \u001b[36m0.9954\u001b[0m  0.0290\n",
      "     10        \u001b[36m0.9616\u001b[0m  0.0300\n",
      "     11        \u001b[36m0.9299\u001b[0m  0.0300\n",
      "     12        \u001b[36m0.9005\u001b[0m  0.0300\n",
      "     13        \u001b[36m0.8733\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.8484\u001b[0m  0.0330\n",
      "     15        \u001b[36m0.8257\u001b[0m  0.0300\n",
      "     16        \u001b[36m0.8051\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.7866\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.7701\u001b[0m  0.0290\n",
      "     19        \u001b[36m0.7554\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.7425\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.7311\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.7211\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.7125\u001b[0m  0.0310\n",
      "     24        \u001b[36m0.7050\u001b[0m  0.0310\n",
      "     25        \u001b[36m0.6985\u001b[0m  0.0290\n",
      "     26        \u001b[36m0.6929\u001b[0m  0.0310\n",
      "     27        \u001b[36m0.6882\u001b[0m  0.0290\n",
      "     28        \u001b[36m0.6841\u001b[0m  0.0290\n",
      "     29        \u001b[36m0.6806\u001b[0m  0.0290\n",
      "     30        \u001b[36m0.6776\u001b[0m  0.0300\n",
      "     31        \u001b[36m0.6751\u001b[0m  0.0310\n",
      "     32        \u001b[36m0.6729\u001b[0m  0.0300\n",
      "     33        \u001b[36m0.6711\u001b[0m  0.0290\n",
      "     34        \u001b[36m0.6696\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.6683\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.6672\u001b[0m  0.0290\n",
      "     37        \u001b[36m0.6662\u001b[0m  0.0310\n",
      "     38        \u001b[36m0.6655\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6648\u001b[0m  0.0290\n",
      "     40        \u001b[36m0.6642\u001b[0m  0.0320\n",
      "     41        \u001b[36m0.6637\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6633\u001b[0m  0.0290\n",
      "     43        \u001b[36m0.6630\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6627\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6624\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6622\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6620\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.6619\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6618\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     51        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6615\u001b[0m  0.0290\n",
      "     53        \u001b[36m0.6614\u001b[0m  0.0290\n",
      "     54        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6613\u001b[0m  0.0310\n",
      "     56        \u001b[36m0.6612\u001b[0m  0.0310\n",
      "     57        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.6611\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6611\u001b[0m  0.0310\n",
      "     60        \u001b[36m0.6611\u001b[0m  0.0290\n",
      "     61        \u001b[36m0.6611\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6610\u001b[0m  0.0290\n",
      "     64        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6609\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     82        \u001b[36m0.6609\u001b[0m  0.0290\n",
      "     83        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     85        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     89        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     91        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     92        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     93        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     94        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     95        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     96        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     97        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     98        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "    100        \u001b[36m0.6609\u001b[0m  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.6109\u001b[0m  0.0400\n",
      "      2        \u001b[36m3.4047\u001b[0m  0.0400\n",
      "      3        \u001b[36m3.1830\u001b[0m  0.0400\n",
      "      4        \u001b[36m2.8790\u001b[0m  0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m2.0858\u001b[0m  0.0440\n",
      "      6        \u001b[36m1.0240\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.6973\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.6788\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6763\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6747\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6738\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6731\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6727\u001b[0m  0.0390\n",
      "     14        \u001b[36m0.6723\u001b[0m  0.0400\n",
      "     15        \u001b[36m0.6720\u001b[0m  0.0400\n",
      "     16        \u001b[36m0.6716\u001b[0m  0.0390\n",
      "     17        \u001b[36m0.6713\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6711\u001b[0m  0.0420\n",
      "     19        \u001b[36m0.6709\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6707\u001b[0m  0.0400\n",
      "     21        \u001b[36m0.6705\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6703\u001b[0m  0.0410\n",
      "     23        \u001b[36m0.6700\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.6686\u001b[0m  0.0410\n",
      "     25        0.6862  0.0400\n",
      "     26        0.6768  0.0390\n",
      "     27        \u001b[36m0.6348\u001b[0m  0.0390\n",
      "     28        0.6923  0.0400\n",
      "     29        0.6452  0.0400\n",
      "     30        \u001b[36m0.6291\u001b[0m  0.0390\n",
      "     31        0.6504  0.0400\n",
      "     32        0.6303  0.0400\n",
      "     33        0.6654  0.0410\n",
      "     34        \u001b[36m0.6203\u001b[0m  0.0400\n",
      "     35        0.6445  0.0400\n",
      "     36        \u001b[36m0.6201\u001b[0m  0.0400\n",
      "     37        0.6575  0.0400\n",
      "     38        0.6331  0.0400\n",
      "     39        \u001b[36m0.6107\u001b[0m  0.0400\n",
      "     40        \u001b[36m0.6053\u001b[0m  0.0400\n",
      "     41        \u001b[36m0.6021\u001b[0m  0.0400\n",
      "     42        \u001b[36m0.5950\u001b[0m  0.0390\n",
      "     43        0.5989  0.0400\n",
      "     44        \u001b[36m0.5935\u001b[0m  0.0400\n",
      "     45        0.6058  0.0400\n",
      "     46        \u001b[36m0.5802\u001b[0m  0.0390\n",
      "     47        0.6053  0.0400\n",
      "     48        0.5957  0.0410\n",
      "     49        0.5944  0.0400\n",
      "     50        0.5848  0.0390\n",
      "     51        0.5869  0.0400\n",
      "     52        0.6268  0.0390\n",
      "     53        0.6302  0.0400\n",
      "     54        0.6229  0.0390\n",
      "     55        0.6201  0.0400\n",
      "     56        0.6185  0.0390\n",
      "     57        0.6176  0.0400\n",
      "     58        0.6169  0.0400\n",
      "     59        0.6164  0.0400\n",
      "     60        0.6160  0.0400\n",
      "     61        0.6156  0.0400\n",
      "     62        0.6153  0.0400\n",
      "     63        0.6149  0.0400\n",
      "     64        0.6145  0.0400\n",
      "     65        0.6141  0.0400\n",
      "     66        0.6138  0.0410\n",
      "     67        0.6135  0.0420\n",
      "     68        0.6132  0.0390\n",
      "     69        0.6128  0.0400\n",
      "     70        0.6126  0.0400\n",
      "     71        0.6123  0.0400\n",
      "     72        0.6120  0.0410\n",
      "     73        0.6117  0.0410\n",
      "     74        0.6115  0.0400\n",
      "     75        0.6112  0.0410\n",
      "     76        0.6110  0.0400\n",
      "     77        0.6108  0.0410\n",
      "     78        0.6105  0.0400\n",
      "     79        0.6103  0.0400\n",
      "     80        0.6101  0.0400\n",
      "     81        0.6099  0.0400\n",
      "     82        0.6097  0.0410\n",
      "     83        0.6095  0.0410\n",
      "     84        0.6093  0.0410\n",
      "     85        0.6091  0.0410\n",
      "     86        0.6089  0.0400\n",
      "     87        0.6087  0.0410\n",
      "     88        0.6083  0.0400\n",
      "     89        0.6028  0.0410\n",
      "     90        0.5994  0.0410\n",
      "     91        \u001b[36m0.5705\u001b[0m  0.0400\n",
      "     92        0.5771  0.0440\n",
      "     93        0.5817  0.0410\n",
      "     94        \u001b[36m0.5702\u001b[0m  0.0410\n",
      "     95        0.6102  0.0410\n",
      "     96        0.6079  0.0410\n",
      "     97        0.6048  0.0420\n",
      "     98        0.6001  0.0400\n",
      "     99        0.5705  0.0410\n",
      "    100        0.5746  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.4272\u001b[0m  0.0390\n",
      "      2        \u001b[36m3.2331\u001b[0m  0.0410\n",
      "      3        \u001b[36m3.0290\u001b[0m  0.0450\n",
      "      4        \u001b[36m2.8082\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m2.3173\u001b[0m  0.0400\n",
      "      6        \u001b[36m1.0293\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.6870\u001b[0m  0.0420\n",
      "      8        \u001b[36m0.6745\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6740\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6732\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6727\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6723\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6720\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6718\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6716\u001b[0m  0.0410\n",
      "     16        \u001b[36m0.6714\u001b[0m  0.0400\n",
      "     17        \u001b[36m0.6712\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6711\u001b[0m  0.0400\n",
      "     19        \u001b[36m0.6709\u001b[0m  0.0390\n",
      "     20        \u001b[36m0.6708\u001b[0m  0.0400\n",
      "     21        \u001b[36m0.6707\u001b[0m  0.0400\n",
      "     22        \u001b[36m0.6705\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6698\u001b[0m  0.0400\n",
      "     24        0.6703  0.0400\n",
      "     25        0.6702  0.0400\n",
      "     26        0.6701  0.0390\n",
      "     27        0.6700  0.0400\n",
      "     28        0.6855  0.0380\n",
      "     29        0.6860  0.0400\n",
      "     30        0.6820  0.0400\n",
      "     31        \u001b[36m0.6635\u001b[0m  0.0400\n",
      "     32        \u001b[36m0.6635\u001b[0m  0.0400\n",
      "     33        \u001b[36m0.6634\u001b[0m  0.0400\n",
      "     34        \u001b[36m0.6582\u001b[0m  0.0390\n",
      "     35        0.7143  0.0410\n",
      "     36        0.6705  0.0410\n",
      "     37        0.6646  0.0400\n",
      "     38        \u001b[36m0.6460\u001b[0m  0.0400\n",
      "     39        0.6841  0.0400\n",
      "     40        0.6670  0.0410\n",
      "     41        0.6642  0.0420\n",
      "     42        0.6568  0.0390\n",
      "     43        0.6536  0.0400\n",
      "     44        0.6516  0.0400\n",
      "     45        0.6516  0.0420\n",
      "     46        0.6506  0.0410\n",
      "     47        \u001b[36m0.6284\u001b[0m  0.0390\n",
      "     48        0.6796  0.0390\n",
      "     49        0.6537  0.0400\n",
      "     50        0.6632  0.0400\n",
      "     51        0.6565  0.0410\n",
      "     52        0.6539  0.0410\n",
      "     53        0.6518  0.0390\n",
      "     54        0.6501  0.0410\n",
      "     55        0.6488  0.0390\n",
      "     56        0.6476  0.0400\n",
      "     57        0.6466  0.0390\n",
      "     58        0.6458  0.0400\n",
      "     59        0.6450  0.0390\n",
      "     60        0.6434  0.0400\n",
      "     61        \u001b[36m0.6217\u001b[0m  0.0400\n",
      "     62        0.6789  0.0390\n",
      "     63        0.6522  0.0410\n",
      "     64        0.6539  0.0390\n",
      "     65        0.6520  0.0410\n",
      "     66        0.6502  0.0410\n",
      "     67        0.6500  0.0390\n",
      "     68        0.6491  0.0400\n",
      "     69        0.6484  0.0400\n",
      "     70        0.6477  0.0400\n",
      "     71        0.6470  0.0410\n",
      "     72        0.6465  0.0390\n",
      "     73        0.6460  0.0400\n",
      "     74        0.6455  0.0400\n",
      "     75        0.6451  0.0400\n",
      "     76        0.6448  0.0400\n",
      "     77        0.6444  0.0400\n",
      "     78        0.6441  0.0400\n",
      "     79        0.6438  0.0400\n",
      "     80        0.6436  0.0410\n",
      "     81        0.6433  0.0400\n",
      "     82        0.6431  0.0400\n",
      "     83        0.6429  0.0410\n",
      "     84        0.6427  0.0400\n",
      "     85        0.6425  0.0400\n",
      "     86        0.6423  0.0410\n",
      "     87        0.6421  0.0410\n",
      "     88        0.6419  0.0400\n",
      "     89        0.6418  0.0410\n",
      "     90        0.6416  0.0420\n",
      "     91        0.6415  0.0410\n",
      "     92        0.6413  0.0400\n",
      "     93        0.6412  0.0410\n",
      "     94        0.6411  0.0410\n",
      "     95        0.6409  0.0410\n",
      "     96        0.6408  0.0420\n",
      "     97        0.6407  0.0410\n",
      "     98        0.6406  0.0400\n",
      "     99        0.6405  0.0400\n",
      "    100        0.6404  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7348\u001b[0m  0.0400\n",
      "      2        \u001b[36m2.5209\u001b[0m  0.0440\n",
      "      3        \u001b[36m2.2988\u001b[0m  0.0380\n",
      "      4        \u001b[36m2.0651\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m1.6531\u001b[0m  0.0410\n",
      "      6        \u001b[36m0.9609\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.6997\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.6710\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6690\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6685\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6682\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6680\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6679\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6679\u001b[0m  0.0410\n",
      "     15        \u001b[36m0.6678\u001b[0m  0.0400\n",
      "     16        \u001b[36m0.6677\u001b[0m  0.0400\n",
      "     17        \u001b[36m0.6676\u001b[0m  0.0390\n",
      "     18        \u001b[36m0.6675\u001b[0m  0.0410\n",
      "     19        \u001b[36m0.6675\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6675\u001b[0m  0.0400\n",
      "     21        \u001b[36m0.6667\u001b[0m  0.0400\n",
      "     22        \u001b[36m0.6632\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6560\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.6557\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.6427\u001b[0m  0.0390\n",
      "     26        \u001b[36m0.6392\u001b[0m  0.0400\n",
      "     27        0.6404  0.0390\n",
      "     28        \u001b[36m0.6352\u001b[0m  0.0400\n",
      "     29        \u001b[36m0.6296\u001b[0m  0.0440\n",
      "     30        0.6320  0.0400\n",
      "     31        0.6314  0.0410\n",
      "     32        0.6326  0.0400\n",
      "     33        0.6312  0.0400\n",
      "     34        0.6366  0.0400\n",
      "     35        0.6884  0.0410\n",
      "     36        0.6470  0.0420\n",
      "     37        0.6497  0.0400\n",
      "     38        0.6501  0.0400\n",
      "     39        0.6426  0.0400\n",
      "     40        0.6409  0.0400\n",
      "     41        0.6327  0.0400\n",
      "     42        0.6331  0.0400\n",
      "     43        0.6437  0.0400\n",
      "     44        0.6810  0.0410\n",
      "     45        0.6656  0.0390\n",
      "     46        \u001b[36m0.6196\u001b[0m  0.0400\n",
      "     47        0.6251  0.0400\n",
      "     48        0.6310  0.0400\n",
      "     49        \u001b[36m0.6169\u001b[0m  0.0400\n",
      "     50        \u001b[36m0.6023\u001b[0m  0.0390\n",
      "     51        0.6049  0.0400\n",
      "     52        0.6140  0.0440\n",
      "     53        0.6195  0.0400\n",
      "     54        0.6144  0.0410\n",
      "     55        \u001b[36m0.6010\u001b[0m  0.0400\n",
      "     56        \u001b[36m0.6004\u001b[0m  0.0400\n",
      "     57        0.6170  0.0400\n",
      "     58        0.6085  0.0400\n",
      "     59        \u001b[36m0.5980\u001b[0m  0.0400\n",
      "     60        \u001b[36m0.5909\u001b[0m  0.0400\n",
      "     61        0.5973  0.0410\n",
      "     62        0.5988  0.0400\n",
      "     63        \u001b[36m0.5900\u001b[0m  0.0410\n",
      "     64        \u001b[36m0.5798\u001b[0m  0.0390\n",
      "     65        0.5883  0.0390\n",
      "     66        0.5855  0.0400\n",
      "     67        0.5855  0.0410\n",
      "     68        \u001b[36m0.5758\u001b[0m  0.0400\n",
      "     69        0.6015  0.0410\n",
      "     70        0.5850  0.0400\n",
      "     71        \u001b[36m0.5757\u001b[0m  0.0400\n",
      "     72        0.5769  0.0390\n",
      "     73        0.5955  0.0400\n",
      "     74        0.5836  0.0400\n",
      "     75        0.5814  0.0400\n",
      "     76        0.5801  0.0410\n",
      "     77        0.5794  0.0400\n",
      "     78        0.5787  0.0410\n",
      "     79        0.5782  0.0400\n",
      "     80        0.5777  0.0410\n",
      "     81        0.5773  0.0400\n",
      "     82        0.5769  0.0400\n",
      "     83        0.5765  0.0410\n",
      "     84        0.5762  0.0440\n",
      "     85        0.5759  0.0410\n",
      "     86        \u001b[36m0.5756\u001b[0m  0.0410\n",
      "     87        \u001b[36m0.5753\u001b[0m  0.0420\n",
      "     88        \u001b[36m0.5751\u001b[0m  0.0400\n",
      "     89        \u001b[36m0.5748\u001b[0m  0.0400\n",
      "     90        \u001b[36m0.5746\u001b[0m  0.0400\n",
      "     91        \u001b[36m0.5744\u001b[0m  0.0420\n",
      "     92        \u001b[36m0.5741\u001b[0m  0.0400\n",
      "     93        \u001b[36m0.5739\u001b[0m  0.0410\n",
      "     94        \u001b[36m0.5737\u001b[0m  0.0410\n",
      "     95        \u001b[36m0.5735\u001b[0m  0.0410\n",
      "     96        \u001b[36m0.5729\u001b[0m  0.0400\n",
      "     97        \u001b[36m0.5723\u001b[0m  0.0410\n",
      "     98        \u001b[36m0.5721\u001b[0m  0.0410\n",
      "     99        0.5725  0.0400\n",
      "    100        0.5878  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.1384\u001b[0m  0.0390\n",
      "      2        \u001b[36m2.9255\u001b[0m  0.0410\n",
      "      3        \u001b[36m2.7049\u001b[0m  0.0400\n",
      "      4        \u001b[36m2.4812\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m2.2422\u001b[0m  0.0400\n",
      "      6        \u001b[36m1.7119\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.9103\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.6977\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6739\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6704\u001b[0m  0.0410\n",
      "     11        \u001b[36m0.6692\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6686\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6683\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6674\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6606\u001b[0m  0.0400\n",
      "     16        0.6982  0.0400\n",
      "     17        0.6750  0.0400\n",
      "     18        0.6741  0.0410\n",
      "     19        0.6711  0.0400\n",
      "     20        0.6915  0.0400\n",
      "     21        0.6802  0.0400\n",
      "     22        0.6771  0.0400\n",
      "     23        0.6747  0.0390\n",
      "     24        0.6731  0.0400\n",
      "     25        0.6714  0.0390\n",
      "     26        0.6700  0.0400\n",
      "     27        0.6690  0.0410\n",
      "     28        0.6684  0.0400\n",
      "     29        0.6679  0.0400\n",
      "     30        0.6669  0.0400\n",
      "     31        0.6665  0.0400\n",
      "     32        \u001b[36m0.6444\u001b[0m  0.0400\n",
      "     33        0.6922  0.0390\n",
      "     34        0.6727  0.0420\n",
      "     35        0.6671  0.0400\n",
      "     36        0.6928  0.0400\n",
      "     37        0.6636  0.0400\n",
      "     38        0.6492  0.0400\n",
      "     39        0.6465  0.0410\n",
      "     40        0.6459  0.0390\n",
      "     41        \u001b[36m0.6425\u001b[0m  0.0410\n",
      "     42        \u001b[36m0.6406\u001b[0m  0.0420\n",
      "     43        \u001b[36m0.6397\u001b[0m  0.0400\n",
      "     44        0.7010  0.0390\n",
      "     45        0.6899  0.0400\n",
      "     46        0.6667  0.0390\n",
      "     47        0.6420  0.0390\n",
      "     48        0.6570  0.0390\n",
      "     49        \u001b[36m0.6348\u001b[0m  0.0390\n",
      "     50        \u001b[36m0.6214\u001b[0m  0.0390\n",
      "     51        \u001b[36m0.6168\u001b[0m  0.0410\n",
      "     52        \u001b[36m0.6123\u001b[0m  0.0400\n",
      "     53        0.6365  0.0390\n",
      "     54        \u001b[36m0.5988\u001b[0m  0.0390\n",
      "     55        0.6069  0.0400\n",
      "     56        0.6069  0.0400\n",
      "     57        \u001b[36m0.5974\u001b[0m  0.0410\n",
      "     58        0.6015  0.0410\n",
      "     59        0.6126  0.0400\n",
      "     60        0.5997  0.0400\n",
      "     61        \u001b[36m0.5941\u001b[0m  0.0400\n",
      "     62        0.5963  0.0390\n",
      "     63        0.5986  0.0400\n",
      "     64        0.5951  0.0390\n",
      "     65        \u001b[36m0.5916\u001b[0m  0.0410\n",
      "     66        \u001b[36m0.5887\u001b[0m  0.0400\n",
      "     67        0.5914  0.0420\n",
      "     68        0.5888  0.0410\n",
      "     69        \u001b[36m0.5807\u001b[0m  0.0410\n",
      "     70        0.5907  0.0390\n",
      "     71        0.5886  0.0440\n",
      "     72        0.5913  0.0400\n",
      "     73        0.5891  0.0400\n",
      "     74        0.5846  0.0400\n",
      "     75        0.5915  0.0400\n",
      "     76        0.6020  0.0400\n",
      "     77        0.6161  0.0410\n",
      "     78        0.6120  0.0400\n",
      "     79        0.5902  0.0400\n",
      "     80        0.5883  0.0410\n",
      "     81        0.5852  0.0410\n",
      "     82        0.5836  0.0400\n",
      "     83        0.5857  0.0410\n",
      "     84        0.5840  0.0400\n",
      "     85        0.5842  0.0410\n",
      "     86        0.5839  0.0420\n",
      "     87        0.5836  0.0390\n",
      "     88        0.5834  0.0410\n",
      "     89        0.5831  0.0400\n",
      "     90        0.5829  0.0400\n",
      "     91        0.5826  0.0410\n",
      "     92        0.5824  0.0420\n",
      "     93        0.5821  0.0410\n",
      "     94        0.5819  0.0400\n",
      "     95        0.5817  0.0400\n",
      "     96        0.5815  0.0410\n",
      "     97        0.5813  0.0400\n",
      "     98        0.5811  0.0400\n",
      "     99        0.5809  0.0410\n",
      "    100        0.5808  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.6484\u001b[0m  0.0400\n",
      "      2        \u001b[36m2.4362\u001b[0m  0.0390\n",
      "      3        \u001b[36m2.2176\u001b[0m  0.0400\n",
      "      4        \u001b[36m1.9960\u001b[0m  0.0400\n",
      "      5        \u001b[36m1.7732\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m1.5011\u001b[0m  0.0400\n",
      "      7        \u001b[36m1.1083\u001b[0m  0.0410\n",
      "      8        \u001b[36m0.8921\u001b[0m  0.0390\n",
      "      9        \u001b[36m0.7184\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6722\u001b[0m  0.0410\n",
      "     11        \u001b[36m0.6686\u001b[0m  0.0390\n",
      "     12        \u001b[36m0.6674\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6669\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6665\u001b[0m  0.0400\n",
      "     15        \u001b[36m0.6663\u001b[0m  0.0400\n",
      "     16        \u001b[36m0.6662\u001b[0m  0.0410\n",
      "     17        \u001b[36m0.6661\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6660\u001b[0m  0.0390\n",
      "     19        \u001b[36m0.6660\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6659\u001b[0m  0.0400\n",
      "     21        \u001b[36m0.6659\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6659\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6658\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.6658\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.6658\u001b[0m  0.0400\n",
      "     26        \u001b[36m0.6658\u001b[0m  0.0400\n",
      "     27        \u001b[36m0.6658\u001b[0m  0.0400\n",
      "     28        \u001b[36m0.6657\u001b[0m  0.0390\n",
      "     29        \u001b[36m0.6657\u001b[0m  0.0400\n",
      "     30        \u001b[36m0.6657\u001b[0m  0.0410\n",
      "     31        \u001b[36m0.6657\u001b[0m  0.0400\n",
      "     32        \u001b[36m0.6518\u001b[0m  0.0410\n",
      "     33        0.6872  0.0410\n",
      "     34        0.6918  0.0420\n",
      "     35        0.6736  0.0400\n",
      "     36        0.6874  0.0400\n",
      "     37        0.6717  0.0410\n",
      "     38        0.6667  0.0400\n",
      "     39        0.6750  0.0400\n",
      "     40        \u001b[36m0.6322\u001b[0m  0.0400\n",
      "     41        0.6734  0.0410\n",
      "     42        0.6421  0.0400\n",
      "     43        0.6658  0.0390\n",
      "     44        0.6456  0.0400\n",
      "     45        0.6832  0.0400\n",
      "     46        0.6771  0.0400\n",
      "     47        0.6709  0.0400\n",
      "     48        0.6342  0.0410\n",
      "     49        \u001b[36m0.6148\u001b[0m  0.0400\n",
      "     50        \u001b[36m0.6122\u001b[0m  0.0400\n",
      "     51        \u001b[36m0.6033\u001b[0m  0.0400\n",
      "     52        0.6052  0.0400\n",
      "     53        \u001b[36m0.6019\u001b[0m  0.0410\n",
      "     54        \u001b[36m0.6006\u001b[0m  0.0400\n",
      "     55        \u001b[36m0.6004\u001b[0m  0.0400\n",
      "     56        \u001b[36m0.5856\u001b[0m  0.0400\n",
      "     57        0.5941  0.0430\n",
      "     58        0.5910  0.0410\n",
      "     59        0.5968  0.0410\n",
      "     60        0.5878  0.0390\n",
      "     61        \u001b[36m0.5768\u001b[0m  0.0400\n",
      "     62        0.5803  0.0400\n",
      "     63        0.5939  0.0410\n",
      "     64        0.6042  0.0400\n",
      "     65        0.5799  0.0410\n",
      "     66        0.5914  0.0390\n",
      "     67        0.6016  0.0400\n",
      "     68        0.6024  0.0400\n",
      "     69        0.5857  0.0400\n",
      "     70        0.5866  0.0390\n",
      "     71        0.5796  0.0410\n",
      "     72        0.5786  0.0400\n",
      "     73        0.5835  0.0400\n",
      "     74        0.5785  0.0410\n",
      "     75        0.5782  0.0410\n",
      "     76        0.5782  0.0410\n",
      "     77        0.5805  0.0410\n",
      "     78        0.5815  0.0400\n",
      "     79        0.5862  0.0410\n",
      "     80        0.5818  0.0400\n",
      "     81        0.5826  0.0410\n",
      "     82        0.5891  0.0410\n",
      "     83        0.5997  0.0410\n",
      "     84        0.5873  0.0410\n",
      "     85        0.5909  0.0410\n",
      "     86        0.5776  0.0400\n",
      "     87        0.5801  0.0410\n",
      "     88        0.5956  0.0400\n",
      "     89        0.5784  0.0410\n",
      "     90        0.5801  0.0410\n",
      "     91        0.5843  0.0410\n",
      "     92        0.5781  0.0400\n",
      "     93        0.5796  0.0410\n",
      "     94        0.5785  0.0400\n",
      "     95        0.5812  0.0410\n",
      "     96        \u001b[36m0.5763\u001b[0m  0.0410\n",
      "     97        \u001b[36m0.5721\u001b[0m  0.0410\n",
      "     98        0.5853  0.0400\n",
      "     99        \u001b[36m0.5663\u001b[0m  0.0420\n",
      "    100        0.5880  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.7067\u001b[0m  0.0300\n",
      "      2        \u001b[36m2.5979\u001b[0m  0.0300\n",
      "      3        \u001b[36m2.4893\u001b[0m  0.0300\n",
      "      4        \u001b[36m2.3809\u001b[0m  0.0300\n",
      "      5        \u001b[36m2.2728\u001b[0m  0.0300\n",
      "      6        \u001b[36m2.1651\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m2.0578\u001b[0m  0.0300\n",
      "      8        \u001b[36m1.9513\u001b[0m  0.0300\n",
      "      9        \u001b[36m1.8457\u001b[0m  0.0300\n",
      "     10        \u001b[36m1.7413\u001b[0m  0.0310\n",
      "     11        \u001b[36m1.6384\u001b[0m  0.0300\n",
      "     12        \u001b[36m1.5375\u001b[0m  0.0300\n",
      "     13        \u001b[36m1.4392\u001b[0m  0.0300\n",
      "     14        \u001b[36m1.3442\u001b[0m  0.0290\n",
      "     15        \u001b[36m1.2533\u001b[0m  0.0310\n",
      "     16        \u001b[36m1.1674\u001b[0m  0.0310\n",
      "     17        \u001b[36m1.0874\u001b[0m  0.0300\n",
      "     18        \u001b[36m1.0142\u001b[0m  0.0300\n",
      "     19        \u001b[36m0.9487\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.8912\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.8421\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.8011\u001b[0m  0.0290\n",
      "     23        \u001b[36m0.7677\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.7412\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.7204\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.7046\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.6926\u001b[0m  0.0290\n",
      "     28        \u001b[36m0.6838\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.6773\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.6725\u001b[0m  0.0330\n",
      "     31        \u001b[36m0.6691\u001b[0m  0.0310\n",
      "     32        \u001b[36m0.6666\u001b[0m  0.0310\n",
      "     33        \u001b[36m0.6648\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.6635\u001b[0m  0.0310\n",
      "     35        \u001b[36m0.6626\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.6620\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6611\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     40        \u001b[36m0.6607\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6605\u001b[0m  0.0310\n",
      "     43        \u001b[36m0.6604\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6604\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6603\u001b[0m  0.0290\n",
      "     50        \u001b[36m0.6603\u001b[0m  0.0310\n",
      "     51        \u001b[36m0.6603\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6602\u001b[0m  0.0290\n",
      "     55        \u001b[36m0.6602\u001b[0m  0.0290\n",
      "     56        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.6602\u001b[0m  0.0290\n",
      "     58        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     64        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     67        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     68        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     71        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     72        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     73        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     75        \u001b[36m0.6602\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6602\u001b[0m  0.0290\n",
      "     82        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     83        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     84        0.6602  0.0300\n",
      "     85        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.6602\u001b[0m  0.0310\n",
      "     89        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     90        0.6602  0.0310\n",
      "     91        0.6602  0.0290\n",
      "     92        0.6602  0.0300\n",
      "     93        0.6602  0.0300\n",
      "     94        0.6602  0.0300\n",
      "     95        0.6602  0.0320\n",
      "     96        0.6602  0.0300\n",
      "     97        \u001b[36m0.6602\u001b[0m  0.0300\n",
      "     98        0.6602  0.0300\n",
      "     99        0.6602  0.0300\n",
      "    100        0.6602  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m3.3148\u001b[0m  0.0300\n",
      "      2        \u001b[36m3.2057\u001b[0m  0.0320\n",
      "      3        \u001b[36m3.0965\u001b[0m  0.0300\n",
      "      4        \u001b[36m2.9875\u001b[0m  0.0310\n",
      "      5        \u001b[36m2.8784\u001b[0m  0.0340\n",
      "      6        \u001b[36m2.7695\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m2.6607\u001b[0m  0.0310\n",
      "      8        \u001b[36m2.5519\u001b[0m  0.0300\n",
      "      9        \u001b[36m2.4434\u001b[0m  0.0300\n",
      "     10        \u001b[36m2.3351\u001b[0m  0.0300\n",
      "     11        \u001b[36m2.2272\u001b[0m  0.0300\n",
      "     12        \u001b[36m2.1197\u001b[0m  0.0310\n",
      "     13        \u001b[36m2.0127\u001b[0m  0.0300\n",
      "     14        \u001b[36m1.9066\u001b[0m  0.0300\n",
      "     15        \u001b[36m1.8014\u001b[0m  0.0320\n",
      "     16        \u001b[36m1.6975\u001b[0m  0.0300\n",
      "     17        \u001b[36m1.5954\u001b[0m  0.0300\n",
      "     18        \u001b[36m1.4956\u001b[0m  0.0290\n",
      "     19        \u001b[36m1.3986\u001b[0m  0.0300\n",
      "     20        \u001b[36m1.3051\u001b[0m  0.0300\n",
      "     21        \u001b[36m1.2162\u001b[0m  0.0300\n",
      "     22        \u001b[36m1.1326\u001b[0m  0.0300\n",
      "     23        \u001b[36m1.0554\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.9853\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.9231\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.8691\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.8234\u001b[0m  0.0310\n",
      "     28        \u001b[36m0.7857\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.7553\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.7314\u001b[0m  0.0310\n",
      "     31        \u001b[36m0.7129\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.6988\u001b[0m  0.0290\n",
      "     33        \u001b[36m0.6883\u001b[0m  0.0290\n",
      "     34        \u001b[36m0.6806\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.6750\u001b[0m  0.0330\n",
      "     36        \u001b[36m0.6709\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6679\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6658\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6643\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.6632\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6625\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6619\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6613\u001b[0m  0.0310\n",
      "     45        \u001b[36m0.6611\u001b[0m  0.0310\n",
      "     46        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     48        \u001b[36m0.6608\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6607\u001b[0m  0.0290\n",
      "     50        \u001b[36m0.6607\u001b[0m  0.0300\n",
      "     51        \u001b[36m0.6607\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6606\u001b[0m  0.0290\n",
      "     55        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6606\u001b[0m  0.0310\n",
      "     61        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6606\u001b[0m  0.0310\n",
      "     65        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6606\u001b[0m  0.0290\n",
      "     70        \u001b[36m0.6606\u001b[0m  0.0330\n",
      "     71        \u001b[36m0.6606\u001b[0m  0.0310\n",
      "     72        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.6606\u001b[0m  0.0310\n",
      "     76        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6606\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.6606\u001b[0m  0.0330\n",
      "     81        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     83        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6606\u001b[0m  0.0290\n",
      "     85        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     87        0.6606  0.0310\n",
      "     88        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     89        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.6606\u001b[0m  0.0310\n",
      "     91        0.6606  0.0300\n",
      "     92        0.6606  0.0300\n",
      "     93        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     94        0.6606  0.0300\n",
      "     95        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     96        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     97        0.6606  0.0310\n",
      "     98        \u001b[36m0.6606\u001b[0m  0.0300\n",
      "     99        0.6606  0.0310\n",
      "    100        0.6606  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.5912\u001b[0m  0.0300\n",
      "      2        \u001b[36m2.4813\u001b[0m  0.0310\n",
      "      3        \u001b[36m2.3717\u001b[0m  0.0300\n",
      "      4        \u001b[36m2.2624\u001b[0m  0.0300\n",
      "      5        \u001b[36m2.1535\u001b[0m  0.0300\n",
      "      6        \u001b[36m2.0452\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.9376\u001b[0m  0.0300\n",
      "      8        \u001b[36m1.8309\u001b[0m  0.0300\n",
      "      9        \u001b[36m1.7255\u001b[0m  0.0310\n",
      "     10        \u001b[36m1.6219\u001b[0m  0.0300\n",
      "     11        \u001b[36m1.5203\u001b[0m  0.0310\n",
      "     12        \u001b[36m1.4216\u001b[0m  0.0300\n",
      "     13        \u001b[36m1.3264\u001b[0m  0.0290\n",
      "     14        \u001b[36m1.2355\u001b[0m  0.0290\n",
      "     15        \u001b[36m1.1500\u001b[0m  0.0300\n",
      "     16        \u001b[36m1.0707\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.9986\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.9345\u001b[0m  0.0300\n",
      "     19        \u001b[36m0.8786\u001b[0m  0.0290\n",
      "     20        \u001b[36m0.8313\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.7920\u001b[0m  0.0290\n",
      "     22        \u001b[36m0.7604\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.7354\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.7160\u001b[0m  0.0310\n",
      "     25        \u001b[36m0.7014\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.6904\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.6823\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.6764\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.6721\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.6691\u001b[0m  0.0300\n",
      "     31        \u001b[36m0.6669\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.6653\u001b[0m  0.0340\n",
      "     33        \u001b[36m0.6642\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.6634\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.6628\u001b[0m  0.0310\n",
      "     36        \u001b[36m0.6624\u001b[0m  0.0340\n",
      "     37        \u001b[36m0.6622\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6620\u001b[0m  0.0310\n",
      "     39        \u001b[36m0.6618\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.6617\u001b[0m  0.0310\n",
      "     41        \u001b[36m0.6616\u001b[0m  0.0290\n",
      "     42        \u001b[36m0.6616\u001b[0m  0.0310\n",
      "     43        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     45        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     51        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     52        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6614\u001b[0m  0.0290\n",
      "     54        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6614\u001b[0m  0.0310\n",
      "     57        \u001b[36m0.6614\u001b[0m  0.0310\n",
      "     58        \u001b[36m0.6614\u001b[0m  0.0310\n",
      "     59        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.6614\u001b[0m  0.0310\n",
      "     77        \u001b[36m0.6614\u001b[0m  0.0320\n",
      "     78        \u001b[36m0.6614\u001b[0m  0.0290\n",
      "     79        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     81        0.6614  0.0300\n",
      "     82        0.6614  0.0290\n",
      "     83        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     84        0.6614  0.0330\n",
      "     85        0.6614  0.0300\n",
      "     86        0.6614  0.0300\n",
      "     87        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     88        0.6614  0.0300\n",
      "     89        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     90        0.6614  0.0300\n",
      "     91        0.6614  0.0300\n",
      "     92        0.6614  0.0300\n",
      "     93        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     94        0.6614  0.0300\n",
      "     95        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     96        0.6614  0.0310\n",
      "     97        0.6614  0.0300\n",
      "     98        0.6614  0.0300\n",
      "     99        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "    100        \u001b[36m0.6614\u001b[0m  0.0310\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m4.3018\u001b[0m  0.0300\n",
      "      2        \u001b[36m4.1907\u001b[0m  0.0300\n",
      "      3        \u001b[36m4.0804\u001b[0m  0.0300\n",
      "      4        \u001b[36m3.9699\u001b[0m  0.0290\n",
      "      5        \u001b[36m3.8594\u001b[0m  0.0300\n",
      "      6        \u001b[36m3.7487\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m3.6382\u001b[0m  0.0300\n",
      "      8        \u001b[36m3.5277\u001b[0m  0.0300\n",
      "      9        \u001b[36m3.4172\u001b[0m  0.0310\n",
      "     10        \u001b[36m3.3068\u001b[0m  0.0290\n",
      "     11        \u001b[36m3.1964\u001b[0m  0.0300\n",
      "     12        \u001b[36m3.0859\u001b[0m  0.0290\n",
      "     13        \u001b[36m2.9756\u001b[0m  0.0300\n",
      "     14        \u001b[36m2.8653\u001b[0m  0.0290\n",
      "     15        \u001b[36m2.7551\u001b[0m  0.0300\n",
      "     16        \u001b[36m2.6450\u001b[0m  0.0310\n",
      "     17        \u001b[36m2.5351\u001b[0m  0.0300\n",
      "     18        \u001b[36m2.4253\u001b[0m  0.0300\n",
      "     19        \u001b[36m2.3159\u001b[0m  0.0300\n",
      "     20        \u001b[36m2.2067\u001b[0m  0.0320\n",
      "     21        \u001b[36m2.0981\u001b[0m  0.0310\n",
      "     22        \u001b[36m1.9901\u001b[0m  0.0300\n",
      "     23        \u001b[36m1.8830\u001b[0m  0.0300\n",
      "     24        \u001b[36m1.7769\u001b[0m  0.0300\n",
      "     25        \u001b[36m1.6724\u001b[0m  0.0310\n",
      "     26        \u001b[36m1.5697\u001b[0m  0.0300\n",
      "     27        \u001b[36m1.4696\u001b[0m  0.0290\n",
      "     28        \u001b[36m1.3726\u001b[0m  0.0300\n",
      "     29        \u001b[36m1.2795\u001b[0m  0.0300\n",
      "     30        \u001b[36m1.1912\u001b[0m  0.0300\n",
      "     31        \u001b[36m1.1088\u001b[0m  0.0300\n",
      "     32        \u001b[36m1.0332\u001b[0m  0.0300\n",
      "     33        \u001b[36m0.9651\u001b[0m  0.0310\n",
      "     34        \u001b[36m0.9051\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.8537\u001b[0m  0.0310\n",
      "     36        \u001b[36m0.8105\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.7753\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.7472\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.7252\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.7083\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6956\u001b[0m  0.0310\n",
      "     42        \u001b[36m0.6862\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.6793\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6742\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6706\u001b[0m  0.0290\n",
      "     46        \u001b[36m0.6680\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6661\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.6648\u001b[0m  0.0290\n",
      "     49        \u001b[36m0.6638\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.6632\u001b[0m  0.0290\n",
      "     51        \u001b[36m0.6627\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6623\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6621\u001b[0m  0.0310\n",
      "     54        \u001b[36m0.6619\u001b[0m  0.0310\n",
      "     55        \u001b[36m0.6618\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6617\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     61        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     65        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     70        \u001b[36m0.6615\u001b[0m  0.0290\n",
      "     71        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6615\u001b[0m  0.0320\n",
      "     75        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6615\u001b[0m  0.0290\n",
      "     82        \u001b[36m0.6615\u001b[0m  0.0290\n",
      "     83        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     87        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.6615\u001b[0m  0.0290\n",
      "     89        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     90        \u001b[36m0.6615\u001b[0m  0.0330\n",
      "     91        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     92        \u001b[36m0.6615\u001b[0m  0.0290\n",
      "     93        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     94        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "     95        0.6615  0.0300\n",
      "     96        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     97        0.6615  0.0310\n",
      "     98        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     99        0.6615  0.0310\n",
      "    100        \u001b[36m0.6615\u001b[0m  0.0310\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m2.2572\u001b[0m  0.0300\n",
      "      2        \u001b[36m2.1511\u001b[0m  0.0300\n",
      "      3        \u001b[36m2.0455\u001b[0m  0.0290\n",
      "      4        \u001b[36m1.9406\u001b[0m  0.0300\n",
      "      5        \u001b[36m1.8366\u001b[0m  0.0300\n",
      "      6        \u001b[36m1.7338\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.6327\u001b[0m  0.0310\n",
      "      8        \u001b[36m1.5335\u001b[0m  0.0290\n",
      "      9        \u001b[36m1.4369\u001b[0m  0.0300\n",
      "     10        \u001b[36m1.3436\u001b[0m  0.0310\n",
      "     11        \u001b[36m1.2543\u001b[0m  0.0300\n",
      "     12        \u001b[36m1.1699\u001b[0m  0.0300\n",
      "     13        \u001b[36m1.0914\u001b[0m  0.0300\n",
      "     14        \u001b[36m1.0194\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.9548\u001b[0m  0.0290\n",
      "     16        \u001b[36m0.8981\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.8494\u001b[0m  0.0310\n",
      "     18        \u001b[36m0.8085\u001b[0m  0.0300\n",
      "     19        \u001b[36m0.7750\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.7481\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.7269\u001b[0m  0.0290\n",
      "     22        \u001b[36m0.7106\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.6981\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.6886\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.6816\u001b[0m  0.0290\n",
      "     26        \u001b[36m0.6764\u001b[0m  0.0310\n",
      "     27        \u001b[36m0.6725\u001b[0m  0.0290\n",
      "     28        \u001b[36m0.6696\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.6675\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.6660\u001b[0m  0.0290\n",
      "     31        \u001b[36m0.6648\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.6640\u001b[0m  0.0310\n",
      "     33        \u001b[36m0.6633\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.6629\u001b[0m  0.0290\n",
      "     35        \u001b[36m0.6625\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.6622\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6620\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6618\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6617\u001b[0m  0.0320\n",
      "     40        \u001b[36m0.6616\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6613\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6613\u001b[0m  0.0290\n",
      "     48        \u001b[36m0.6613\u001b[0m  0.0290\n",
      "     49        \u001b[36m0.6613\u001b[0m  0.0310\n",
      "     50        \u001b[36m0.6613\u001b[0m  0.0310\n",
      "     51        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     52        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.6612\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     67        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.6612\u001b[0m  0.0310\n",
      "     72        \u001b[36m0.6612\u001b[0m  0.0320\n",
      "     73        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6612\u001b[0m  0.0310\n",
      "     79        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     80        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.6612\u001b[0m  0.0320\n",
      "     83        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     87        0.6612  0.0300\n",
      "     88        0.6612  0.0300\n",
      "     89        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     91        0.6612  0.0310\n",
      "     92        0.6612  0.0300\n",
      "     93        0.6612  0.0300\n",
      "     94        0.6612  0.0300\n",
      "     95        \u001b[36m0.6612\u001b[0m  0.0350\n",
      "     96        \u001b[36m0.6612\u001b[0m  0.0290\n",
      "     97        \u001b[36m0.6612\u001b[0m  0.0300\n",
      "     98        0.6612  0.0300\n",
      "     99        0.6612  0.0300\n",
      "    100        0.6612  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2991\u001b[0m  0.0400\n",
      "      2        \u001b[36m1.1483\u001b[0m  0.0410\n",
      "      3        \u001b[36m1.0369\u001b[0m  0.0390\n",
      "      4        \u001b[36m0.9525\u001b[0m  0.0390\n",
      "      5        \u001b[36m0.8896\u001b[0m  0.0390"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      6        \u001b[36m0.8393\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.7983\u001b[0m  0.0410\n",
      "      8        \u001b[36m0.7666\u001b[0m  0.0390\n",
      "      9        \u001b[36m0.7416\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.7221\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.7077\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6962\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6870\u001b[0m  0.0390\n",
      "     14        \u001b[36m0.6803\u001b[0m  0.0400\n",
      "     15        \u001b[36m0.6751\u001b[0m  0.0400\n",
      "     16        \u001b[36m0.6734\u001b[0m  0.0390\n",
      "     17        \u001b[36m0.6706\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6673\u001b[0m  0.0400\n",
      "     19        \u001b[36m0.6644\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6618\u001b[0m  0.0390\n",
      "     21        \u001b[36m0.6601\u001b[0m  0.0400\n",
      "     22        \u001b[36m0.6575\u001b[0m  0.0390\n",
      "     23        \u001b[36m0.6560\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.6542\u001b[0m  0.0390\n",
      "     25        0.6595  0.0400\n",
      "     26        \u001b[36m0.6518\u001b[0m  0.0400\n",
      "     27        0.6518  0.0410\n",
      "     28        0.6525  0.0390\n",
      "     29        \u001b[36m0.6503\u001b[0m  0.0390\n",
      "     30        \u001b[36m0.6476\u001b[0m  0.0400\n",
      "     31        \u001b[36m0.6451\u001b[0m  0.0390\n",
      "     32        \u001b[36m0.6427\u001b[0m  0.0390\n",
      "     33        \u001b[36m0.6401\u001b[0m  0.0390\n",
      "     34        \u001b[36m0.6379\u001b[0m  0.0400\n",
      "     35        \u001b[36m0.6356\u001b[0m  0.0400\n",
      "     36        \u001b[36m0.6329\u001b[0m  0.0400\n",
      "     37        \u001b[36m0.6307\u001b[0m  0.0400\n",
      "     38        \u001b[36m0.6287\u001b[0m  0.0400\n",
      "     39        \u001b[36m0.6266\u001b[0m  0.0400\n",
      "     40        \u001b[36m0.6247\u001b[0m  0.0400\n",
      "     41        0.6250  0.0390\n",
      "     42        \u001b[36m0.6210\u001b[0m  0.0390\n",
      "     43        \u001b[36m0.6193\u001b[0m  0.0400\n",
      "     44        \u001b[36m0.6177\u001b[0m  0.0400\n",
      "     45        \u001b[36m0.6162\u001b[0m  0.0390\n",
      "     46        \u001b[36m0.6148\u001b[0m  0.0400\n",
      "     47        \u001b[36m0.6134\u001b[0m  0.0390\n",
      "     48        \u001b[36m0.6121\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.6109\u001b[0m  0.0390\n",
      "     50        \u001b[36m0.6097\u001b[0m  0.0400\n",
      "     51        \u001b[36m0.6089\u001b[0m  0.0390\n",
      "     52        \u001b[36m0.6079\u001b[0m  0.0400\n",
      "     53        \u001b[36m0.6069\u001b[0m  0.0420\n",
      "     54        \u001b[36m0.6060\u001b[0m  0.0400\n",
      "     55        \u001b[36m0.6051\u001b[0m  0.0390\n",
      "     56        \u001b[36m0.6043\u001b[0m  0.0400\n",
      "     57        \u001b[36m0.6035\u001b[0m  0.0400\n",
      "     58        \u001b[36m0.6027\u001b[0m  0.0390\n",
      "     59        \u001b[36m0.6020\u001b[0m  0.0400\n",
      "     60        \u001b[36m0.5988\u001b[0m  0.0390\n",
      "     61        \u001b[36m0.5981\u001b[0m  0.0400\n",
      "     62        \u001b[36m0.5974\u001b[0m  0.0400\n",
      "     63        \u001b[36m0.5968\u001b[0m  0.0400\n",
      "     64        \u001b[36m0.5963\u001b[0m  0.0390\n",
      "     65        \u001b[36m0.5957\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.5952\u001b[0m  0.0390\n",
      "     67        \u001b[36m0.5946\u001b[0m  0.0410\n",
      "     68        \u001b[36m0.5941\u001b[0m  0.0390\n",
      "     69        \u001b[36m0.5936\u001b[0m  0.0400\n",
      "     70        \u001b[36m0.5932\u001b[0m  0.0390\n",
      "     71        0.5936  0.0400\n",
      "     72        \u001b[36m0.5905\u001b[0m  0.0390\n",
      "     73        0.5913  0.0390\n",
      "     74        0.5909  0.0390\n",
      "     75        0.5906  0.0390\n",
      "     76        \u001b[36m0.5901\u001b[0m  0.0390\n",
      "     77        \u001b[36m0.5897\u001b[0m  0.0410\n",
      "     78        \u001b[36m0.5893\u001b[0m  0.0400\n",
      "     79        \u001b[36m0.5889\u001b[0m  0.0390\n",
      "     80        \u001b[36m0.5885\u001b[0m  0.0390\n",
      "     81        \u001b[36m0.5882\u001b[0m  0.0400\n",
      "     82        \u001b[36m0.5878\u001b[0m  0.0400\n",
      "     83        \u001b[36m0.5875\u001b[0m  0.0390\n",
      "     84        \u001b[36m0.5844\u001b[0m  0.0400\n",
      "     85        0.5876  0.0400\n",
      "     86        \u001b[36m0.5832\u001b[0m  0.0420\n",
      "     87        \u001b[36m0.5830\u001b[0m  0.0410\n",
      "     88        \u001b[36m0.5827\u001b[0m  0.0400\n",
      "     89        \u001b[36m0.5820\u001b[0m  0.0400\n",
      "     90        0.5821  0.0430\n",
      "     91        \u001b[36m0.5818\u001b[0m  0.0400\n",
      "     92        \u001b[36m0.5815\u001b[0m  0.0400\n",
      "     93        \u001b[36m0.5812\u001b[0m  0.0400\n",
      "     94        \u001b[36m0.5809\u001b[0m  0.0400\n",
      "     95        \u001b[36m0.5807\u001b[0m  0.0400\n",
      "     96        \u001b[36m0.5804\u001b[0m  0.0400\n",
      "     97        \u001b[36m0.5802\u001b[0m  0.0400\n",
      "     98        \u001b[36m0.5799\u001b[0m  0.0400\n",
      "     99        \u001b[36m0.5778\u001b[0m  0.0390\n",
      "    100        \u001b[36m0.5775\u001b[0m  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.1947\u001b[0m  0.0400\n",
      "      2        \u001b[36m1.0454\u001b[0m  0.0400\n",
      "      3        \u001b[36m0.9236\u001b[0m  0.0390\n",
      "      4        \u001b[36m0.8337\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.7717\u001b[0m  0.0400\n",
      "      6        \u001b[36m0.7302\u001b[0m  0.0390\n",
      "      7        \u001b[36m0.7058\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.6917\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6798\u001b[0m  0.0390\n",
      "     10        \u001b[36m0.6704\u001b[0m  0.0410\n",
      "     11        \u001b[36m0.6649\u001b[0m  0.0390\n",
      "     12        \u001b[36m0.6581\u001b[0m  0.0390\n",
      "     13        \u001b[36m0.6514\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6482\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6451\u001b[0m  0.0390\n",
      "     16        \u001b[36m0.6420\u001b[0m  0.0390\n",
      "     17        \u001b[36m0.6392\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6365\u001b[0m  0.0400\n",
      "     19        \u001b[36m0.6341\u001b[0m  0.0390\n",
      "     20        \u001b[36m0.6318\u001b[0m  0.0410\n",
      "     21        \u001b[36m0.6297\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6279\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6260\u001b[0m  0.0410\n",
      "     24        0.6263  0.0400\n",
      "     25        \u001b[36m0.6247\u001b[0m  0.0390\n",
      "     26        \u001b[36m0.6232\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.6214\u001b[0m  0.0400\n",
      "     28        \u001b[36m0.6176\u001b[0m  0.0410\n",
      "     29        \u001b[36m0.6163\u001b[0m  0.0400\n",
      "     30        \u001b[36m0.6143\u001b[0m  0.0390\n",
      "     31        \u001b[36m0.6126\u001b[0m  0.0400\n",
      "     32        \u001b[36m0.6110\u001b[0m  0.0390\n",
      "     33        \u001b[36m0.6092\u001b[0m  0.0400\n",
      "     34        \u001b[36m0.6076\u001b[0m  0.0400\n",
      "     35        0.6095  0.0400\n",
      "     36        \u001b[36m0.6055\u001b[0m  0.0390\n",
      "     37        \u001b[36m0.6043\u001b[0m  0.0400\n",
      "     38        \u001b[36m0.6028\u001b[0m  0.0390\n",
      "     39        \u001b[36m0.6026\u001b[0m  0.0400\n",
      "     40        \u001b[36m0.6015\u001b[0m  0.0390\n",
      "     41        \u001b[36m0.5998\u001b[0m  0.0400\n",
      "     42        \u001b[36m0.5992\u001b[0m  0.0400\n",
      "     43        \u001b[36m0.5986\u001b[0m  0.0400\n",
      "     44        \u001b[36m0.5980\u001b[0m  0.0400\n",
      "     45        \u001b[36m0.5975\u001b[0m  0.0390\n",
      "     46        \u001b[36m0.5971\u001b[0m  0.0390\n",
      "     47        \u001b[36m0.5966\u001b[0m  0.0400\n",
      "     48        \u001b[36m0.5962\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.5959\u001b[0m  0.0390\n",
      "     50        \u001b[36m0.5955\u001b[0m  0.0400\n",
      "     51        \u001b[36m0.5952\u001b[0m  0.0420\n",
      "     52        0.5952  0.0400\n",
      "     53        \u001b[36m0.5948\u001b[0m  0.0400\n",
      "     54        \u001b[36m0.5946\u001b[0m  0.0400\n",
      "     55        \u001b[36m0.5943\u001b[0m  0.0400\n",
      "     56        \u001b[36m0.5940\u001b[0m  0.0390\n",
      "     57        \u001b[36m0.5937\u001b[0m  0.0410\n",
      "     58        \u001b[36m0.5935\u001b[0m  0.0410\n",
      "     59        \u001b[36m0.5933\u001b[0m  0.0410\n",
      "     60        \u001b[36m0.5930\u001b[0m  0.0390\n",
      "     61        \u001b[36m0.5928\u001b[0m  0.0400\n",
      "     62        \u001b[36m0.5926\u001b[0m  0.0390\n",
      "     63        \u001b[36m0.5924\u001b[0m  0.0400\n",
      "     64        \u001b[36m0.5922\u001b[0m  0.0400\n",
      "     65        \u001b[36m0.5920\u001b[0m  0.0390\n",
      "     66        0.5929  0.0400\n",
      "     67        \u001b[36m0.5917\u001b[0m  0.0390\n",
      "     68        \u001b[36m0.5915\u001b[0m  0.0390\n",
      "     69        \u001b[36m0.5913\u001b[0m  0.0400\n",
      "     70        \u001b[36m0.5912\u001b[0m  0.0400\n",
      "     71        0.5932  0.0430\n",
      "     72        0.5930  0.0400\n",
      "     73        0.5928  0.0390\n",
      "     74        0.5932  0.0400\n",
      "     75        0.5945  0.0390\n",
      "     76        0.5978  0.0400\n",
      "     77        0.5920  0.0390\n",
      "     78        0.5918  0.0400\n",
      "     79        0.5916  0.0410\n",
      "     80        0.5914  0.0400\n",
      "     81        0.5912  0.0400\n",
      "     82        \u001b[36m0.5911\u001b[0m  0.0400\n",
      "     83        \u001b[36m0.5909\u001b[0m  0.0390\n",
      "     84        \u001b[36m0.5907\u001b[0m  0.0400\n",
      "     85        \u001b[36m0.5906\u001b[0m  0.0400\n",
      "     86        \u001b[36m0.5904\u001b[0m  0.0400\n",
      "     87        \u001b[36m0.5902\u001b[0m  0.0400\n",
      "     88        \u001b[36m0.5901\u001b[0m  0.0400\n",
      "     89        \u001b[36m0.5899\u001b[0m  0.0400\n",
      "     90        \u001b[36m0.5898\u001b[0m  0.0390\n",
      "     91        \u001b[36m0.5896\u001b[0m  0.0400\n",
      "     92        \u001b[36m0.5888\u001b[0m  0.0400\n",
      "     93        \u001b[36m0.5887\u001b[0m  0.0400\n",
      "     94        \u001b[36m0.5886\u001b[0m  0.0400\n",
      "     95        \u001b[36m0.5871\u001b[0m  0.0400\n",
      "     96        \u001b[36m0.5870\u001b[0m  0.0390\n",
      "     97        \u001b[36m0.5869\u001b[0m  0.0400\n",
      "     98        \u001b[36m0.5868\u001b[0m  0.0400\n",
      "     99        \u001b[36m0.5866\u001b[0m  0.0390\n",
      "    100        \u001b[36m0.5865\u001b[0m  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7785\u001b[0m  0.0400\n",
      "      2        \u001b[36m0.7494\u001b[0m  0.0390\n",
      "      3        \u001b[36m0.7296\u001b[0m  0.0400\n",
      "      4        \u001b[36m0.7096\u001b[0m  0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.6953\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.6865\u001b[0m  0.0390\n",
      "      7        \u001b[36m0.6775\u001b[0m  0.0390\n",
      "      8        \u001b[36m0.6700\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6621\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.6569\u001b[0m  0.0390\n",
      "     11        \u001b[36m0.6532\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6457\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6407\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6355\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6300\u001b[0m  0.0430\n",
      "     16        0.6316  0.0390\n",
      "     17        \u001b[36m0.6281\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6258\u001b[0m  0.0400\n",
      "     19        \u001b[36m0.6229\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6206\u001b[0m  0.0400\n",
      "     21        \u001b[36m0.6181\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6150\u001b[0m  0.0390\n",
      "     23        \u001b[36m0.6075\u001b[0m  0.0400\n",
      "     24        0.6173  0.0400\n",
      "     25        0.6138  0.0400\n",
      "     26        0.6098  0.0390\n",
      "     27        \u001b[36m0.6062\u001b[0m  0.0390\n",
      "     28        \u001b[36m0.6031\u001b[0m  0.0390\n",
      "     29        \u001b[36m0.6003\u001b[0m  0.0390\n",
      "     30        \u001b[36m0.5979\u001b[0m  0.0410\n",
      "     31        \u001b[36m0.5958\u001b[0m  0.0400\n",
      "     32        \u001b[36m0.5941\u001b[0m  0.0390\n",
      "     33        \u001b[36m0.5926\u001b[0m  0.0390\n",
      "     34        \u001b[36m0.5904\u001b[0m  0.0400\n",
      "     35        \u001b[36m0.5892\u001b[0m  0.0390\n",
      "     36        \u001b[36m0.5882\u001b[0m  0.0390\n",
      "     37        \u001b[36m0.5873\u001b[0m  0.0400\n",
      "     38        \u001b[36m0.5865\u001b[0m  0.0390\n",
      "     39        \u001b[36m0.5858\u001b[0m  0.0390\n",
      "     40        \u001b[36m0.5850\u001b[0m  0.0390\n",
      "     41        \u001b[36m0.5844\u001b[0m  0.0390\n",
      "     42        \u001b[36m0.5836\u001b[0m  0.0400\n",
      "     43        \u001b[36m0.5830\u001b[0m  0.0400\n",
      "     44        \u001b[36m0.5823\u001b[0m  0.0400\n",
      "     45        0.5846  0.0400\n",
      "     46        0.5847  0.0400\n",
      "     47        0.5839  0.0400\n",
      "     48        0.5895  0.0390\n",
      "     49        \u001b[36m0.5747\u001b[0m  0.0400\n",
      "     50        \u001b[36m0.5740\u001b[0m  0.0410\n",
      "     51        \u001b[36m0.5735\u001b[0m  0.0390\n",
      "     52        \u001b[36m0.5729\u001b[0m  0.0400\n",
      "     53        \u001b[36m0.5724\u001b[0m  0.0390\n",
      "     54        \u001b[36m0.5719\u001b[0m  0.0400\n",
      "     55        \u001b[36m0.5714\u001b[0m  0.0400\n",
      "     56        \u001b[36m0.5710\u001b[0m  0.0390\n",
      "     57        \u001b[36m0.5705\u001b[0m  0.0400\n",
      "     58        0.5707  0.0390\n",
      "     59        \u001b[36m0.5703\u001b[0m  0.0390\n",
      "     60        \u001b[36m0.5699\u001b[0m  0.0400\n",
      "     61        \u001b[36m0.5695\u001b[0m  0.0400\n",
      "     62        \u001b[36m0.5691\u001b[0m  0.0390\n",
      "     63        \u001b[36m0.5688\u001b[0m  0.0400\n",
      "     64        \u001b[36m0.5684\u001b[0m  0.0390\n",
      "     65        0.5690  0.0410\n",
      "     66        0.5723  0.0390\n",
      "     67        0.5718  0.0390\n",
      "     68        0.5714  0.0400\n",
      "     69        0.5710  0.0400\n",
      "     70        0.5707  0.0390\n",
      "     71        0.5700  0.0400\n",
      "     72        0.5702  0.0400\n",
      "     73        0.5699  0.0400\n",
      "     74        0.5696  0.0390\n",
      "     75        0.5694  0.0420\n",
      "     76        0.5692  0.0390\n",
      "     77        0.5690  0.0400\n",
      "     78        0.5690  0.0390\n",
      "     79        0.5694  0.0430\n",
      "     80        \u001b[36m0.5682\u001b[0m  0.0400\n",
      "     81        \u001b[36m0.5679\u001b[0m  0.0390\n",
      "     82        \u001b[36m0.5678\u001b[0m  0.0400\n",
      "     83        \u001b[36m0.5677\u001b[0m  0.0400\n",
      "     84        \u001b[36m0.5676\u001b[0m  0.0400\n",
      "     85        \u001b[36m0.5674\u001b[0m  0.0400\n",
      "     86        \u001b[36m0.5672\u001b[0m  0.0400\n",
      "     87        \u001b[36m0.5670\u001b[0m  0.0400\n",
      "     88        \u001b[36m0.5668\u001b[0m  0.0400\n",
      "     89        \u001b[36m0.5667\u001b[0m  0.0400\n",
      "     90        \u001b[36m0.5665\u001b[0m  0.0410\n",
      "     91        \u001b[36m0.5663\u001b[0m  0.0400\n",
      "     92        \u001b[36m0.5662\u001b[0m  0.0400\n",
      "     93        \u001b[36m0.5660\u001b[0m  0.0400\n",
      "     94        \u001b[36m0.5659\u001b[0m  0.0400\n",
      "     95        \u001b[36m0.5657\u001b[0m  0.0400\n",
      "     96        \u001b[36m0.5656\u001b[0m  0.0400\n",
      "     97        \u001b[36m0.5654\u001b[0m  0.0400\n",
      "     98        \u001b[36m0.5653\u001b[0m  0.0390\n",
      "     99        \u001b[36m0.5651\u001b[0m  0.0410\n",
      "    100        \u001b[36m0.5650\u001b[0m  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.1027\u001b[0m  0.0390\n",
      "      2        \u001b[36m0.9697\u001b[0m  0.0400\n",
      "      3        \u001b[36m0.8757\u001b[0m  0.0400\n",
      "      4        \u001b[36m0.8068\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.7651\u001b[0m  0.0400\n",
      "      6        \u001b[36m0.7291\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.7079\u001b[0m  0.0390\n",
      "      8        \u001b[36m0.6961\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6865\u001b[0m  0.0390\n",
      "     10        \u001b[36m0.6726\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6680\u001b[0m  0.0390\n",
      "     12        \u001b[36m0.6637\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6570\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6492\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6426\u001b[0m  0.0400\n",
      "     16        0.6427  0.0400\n",
      "     17        0.6462  0.0400\n",
      "     18        0.6432  0.0390\n",
      "     19        \u001b[36m0.6397\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6370\u001b[0m  0.0390\n",
      "     21        \u001b[36m0.6350\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6331\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6310\u001b[0m  0.0390\n",
      "     24        \u001b[36m0.6286\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.6278\u001b[0m  0.0390\n",
      "     26        \u001b[36m0.6264\u001b[0m  0.0390\n",
      "     27        \u001b[36m0.6251\u001b[0m  0.0390\n",
      "     28        \u001b[36m0.6239\u001b[0m  0.0400\n",
      "     29        \u001b[36m0.6227\u001b[0m  0.0380\n",
      "     30        0.6234  0.0390\n",
      "     31        \u001b[36m0.6217\u001b[0m  0.0400\n",
      "     32        \u001b[36m0.6208\u001b[0m  0.0390\n",
      "     33        \u001b[36m0.6199\u001b[0m  0.0400\n",
      "     34        \u001b[36m0.6192\u001b[0m  0.0400\n",
      "     35        \u001b[36m0.6184\u001b[0m  0.0400\n",
      "     36        \u001b[36m0.6177\u001b[0m  0.0400\n",
      "     37        \u001b[36m0.6170\u001b[0m  0.0390\n",
      "     38        \u001b[36m0.6116\u001b[0m  0.0400\n",
      "     39        \u001b[36m0.6090\u001b[0m  0.0400\n",
      "     40        0.6135  0.0400\n",
      "     41        \u001b[36m0.6089\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.6082\u001b[0m  0.0400\n",
      "     43        0.6098  0.0390\n",
      "     44        \u001b[36m0.6072\u001b[0m  0.0400\n",
      "     45        \u001b[36m0.6052\u001b[0m  0.0390\n",
      "     46        \u001b[36m0.6044\u001b[0m  0.0400\n",
      "     47        \u001b[36m0.6037\u001b[0m  0.0390\n",
      "     48        \u001b[36m0.6030\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.6023\u001b[0m  0.0420\n",
      "     50        \u001b[36m0.6016\u001b[0m  0.0400\n",
      "     51        \u001b[36m0.6010\u001b[0m  0.0400\n",
      "     52        \u001b[36m0.6004\u001b[0m  0.0390\n",
      "     53        \u001b[36m0.5998\u001b[0m  0.0390\n",
      "     54        \u001b[36m0.5924\u001b[0m  0.0390\n",
      "     55        \u001b[36m0.5903\u001b[0m  0.0400\n",
      "     56        \u001b[36m0.5888\u001b[0m  0.0390\n",
      "     57        \u001b[36m0.5877\u001b[0m  0.0400\n",
      "     58        \u001b[36m0.5832\u001b[0m  0.0400\n",
      "     59        0.5906  0.0400\n",
      "     60        0.6022  0.0390\n",
      "     61        0.5994  0.0390\n",
      "     62        0.5996  0.0390\n",
      "     63        0.6028  0.0400\n",
      "     64        0.6045  0.0390\n",
      "     65        0.6072  0.0400\n",
      "     66        0.6121  0.0390\n",
      "     67        0.6063  0.0410\n",
      "     68        0.6068  0.0390\n",
      "     69        0.6068  0.0390\n",
      "     70        0.6076  0.0390\n",
      "     71        0.6104  0.0400\n",
      "     72        0.6126  0.0390\n",
      "     73        0.6177  0.0400\n",
      "     74        0.6316  0.0410\n",
      "     75        0.6368  0.0390\n",
      "     76        0.6162  0.0390\n",
      "     77        0.6149  0.0400\n",
      "     78        0.6136  0.0390\n",
      "     79        0.6125  0.0400\n",
      "     80        0.6269  0.0390\n",
      "     81        0.6188  0.0390\n",
      "     82        0.6178  0.0400\n",
      "     83        0.6182  0.0400\n",
      "     84        0.6173  0.0390\n",
      "     85        0.6163  0.0400\n",
      "     86        0.6151  0.0400\n",
      "     87        0.6119  0.0390\n",
      "     88        0.6126  0.0400\n",
      "     89        0.6118  0.0400\n",
      "     90        0.6111  0.0440\n",
      "     91        0.6105  0.0400\n",
      "     92        0.6100  0.0400\n",
      "     93        0.6094  0.0390\n",
      "     94        0.6089  0.0410\n",
      "     95        0.6084  0.0390\n",
      "     96        0.6080  0.0400\n",
      "     97        0.6090  0.0390\n",
      "     98        0.6049  0.0420\n",
      "     99        0.6085  0.0400\n",
      "    100        0.6079  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7282\u001b[0m  0.0390\n",
      "      2        \u001b[36m0.6441\u001b[0m  0.0400\n",
      "      3        \u001b[36m0.6373\u001b[0m  0.0400\n",
      "      4        \u001b[36m0.6367\u001b[0m  0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.6354\u001b[0m  0.0410\n",
      "      6        \u001b[36m0.6332\u001b[0m  0.0390\n",
      "      7        \u001b[36m0.6307\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.6236\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6206\u001b[0m  0.0400\n",
      "     10        0.6284  0.0390\n",
      "     11        0.6255  0.0390\n",
      "     12        0.6231  0.0410\n",
      "     13        0.6232  0.0400\n",
      "     14        0.6225  0.0390\n",
      "     15        0.6209  0.0400\n",
      "     16        \u001b[36m0.6165\u001b[0m  0.0400\n",
      "     17        \u001b[36m0.6132\u001b[0m  0.0400\n",
      "     18        0.6156  0.0390\n",
      "     19        \u001b[36m0.6106\u001b[0m  0.0400\n",
      "     20        0.6109  0.0400\n",
      "     21        \u001b[36m0.6104\u001b[0m  0.0410\n",
      "     22        \u001b[36m0.6096\u001b[0m  0.0390\n",
      "     23        \u001b[36m0.6087\u001b[0m  0.0420\n",
      "     24        \u001b[36m0.6079\u001b[0m  0.0390\n",
      "     25        0.6108  0.0400\n",
      "     26        0.6114  0.0390\n",
      "     27        0.6119  0.0400\n",
      "     28        0.6082  0.0390\n",
      "     29        \u001b[36m0.6069\u001b[0m  0.0390\n",
      "     30        \u001b[36m0.6060\u001b[0m  0.0400\n",
      "     31        \u001b[36m0.6052\u001b[0m  0.0390\n",
      "     32        \u001b[36m0.6044\u001b[0m  0.0400\n",
      "     33        \u001b[36m0.6041\u001b[0m  0.0400\n",
      "     34        \u001b[36m0.5979\u001b[0m  0.0400\n",
      "     35        0.5992  0.0400\n",
      "     36        0.6006  0.0390\n",
      "     37        0.5992  0.0400\n",
      "     38        0.6053  0.0390\n",
      "     39        0.6011  0.0400\n",
      "     40        0.6029  0.0400\n",
      "     41        0.6034  0.0400\n",
      "     42        0.5994  0.0400\n",
      "     43        0.5992  0.0390\n",
      "     44        0.5990  0.0400\n",
      "     45        0.5988  0.0390\n",
      "     46        0.5981  0.0400\n",
      "     47        \u001b[36m0.5970\u001b[0m  0.0390\n",
      "     48        0.5973  0.0410\n",
      "     49        0.5974  0.0410\n",
      "     50        0.5973  0.0390\n",
      "     51        0.5972  0.0400\n",
      "     52        0.5976  0.0400\n",
      "     53        0.5975  0.0400\n",
      "     54        0.5973  0.0390\n",
      "     55        0.5984  0.0400\n",
      "     56        0.5982  0.0390\n",
      "     57        0.5980  0.0400\n",
      "     58        0.5978  0.0400\n",
      "     59        0.5976  0.0390\n",
      "     60        0.5974  0.0390\n",
      "     61        0.5972  0.0400\n",
      "     62        0.5971  0.0400\n",
      "     63        \u001b[36m0.5969\u001b[0m  0.0390\n",
      "     64        \u001b[36m0.5967\u001b[0m  0.0390\n",
      "     65        \u001b[36m0.5965\u001b[0m  0.0390\n",
      "     66        \u001b[36m0.5964\u001b[0m  0.0400\n",
      "     67        \u001b[36m0.5962\u001b[0m  0.0400\n",
      "     68        \u001b[36m0.5961\u001b[0m  0.0390\n",
      "     69        \u001b[36m0.5959\u001b[0m  0.0400\n",
      "     70        \u001b[36m0.5958\u001b[0m  0.0400\n",
      "     71        0.6026  0.0400\n",
      "     72        0.6083  0.0390\n",
      "     73        0.6083  0.0390\n",
      "     74        0.6081  0.0390\n",
      "     75        0.6079  0.0390\n",
      "     76        0.6078  0.0390\n",
      "     77        0.6076  0.0430\n",
      "     78        0.6074  0.0390\n",
      "     79        0.6073  0.0400\n",
      "     80        0.6071  0.0400\n",
      "     81        0.6058  0.0390\n",
      "     82        0.6018  0.0400\n",
      "     83        0.6085  0.0400\n",
      "     84        0.6074  0.0400\n",
      "     85        0.6067  0.0400\n",
      "     86        0.6061  0.0400\n",
      "     87        0.6056  0.0400\n",
      "     88        0.6050  0.0390\n",
      "     89        0.6089  0.0390\n",
      "     90        0.6083  0.0390\n",
      "     91        0.6076  0.0400\n",
      "     92        0.6082  0.0390\n",
      "     93        0.6076  0.0400\n",
      "     94        0.6059  0.0400\n",
      "     95        0.6094  0.0400\n",
      "     96        0.6145  0.0400\n",
      "     97        0.6079  0.0420\n",
      "     98        0.6148  0.0410\n",
      "     99        0.6181  0.0400\n",
      "    100        0.6167  0.0390\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2898\u001b[0m  0.0290\n",
      "      2        \u001b[36m1.1914\u001b[0m  0.0300\n",
      "      3        \u001b[36m1.1353\u001b[0m  0.0300\n",
      "      4        \u001b[36m1.1031\u001b[0m  0.0290\n",
      "      5        \u001b[36m1.0806\u001b[0m  0.0300\n",
      "      6        \u001b[36m1.0638\u001b[0m  0.0300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        \u001b[36m1.0498\u001b[0m  0.0300\n",
      "      8        \u001b[36m1.0377\u001b[0m  0.0300\n",
      "      9        \u001b[36m1.0271\u001b[0m  0.0300\n",
      "     10        \u001b[36m1.0173\u001b[0m  0.0290\n",
      "     11        \u001b[36m1.0080\u001b[0m  0.0290\n",
      "     12        \u001b[36m0.9992\u001b[0m  0.0300\n",
      "     13        \u001b[36m0.9907\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.9824\u001b[0m  0.0290\n",
      "     15        \u001b[36m0.9743\u001b[0m  0.0300\n",
      "     16        \u001b[36m0.9665\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.9588\u001b[0m  0.0290\n",
      "     18        \u001b[36m0.9513\u001b[0m  0.0300\n",
      "     19        \u001b[36m0.9440\u001b[0m  0.0290\n",
      "     20        \u001b[36m0.9369\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.9299\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.9231\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.9165\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.9100\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.9037\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.8976\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.8917\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.8859\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.8803\u001b[0m  0.0310\n",
      "     30        \u001b[36m0.8748\u001b[0m  0.0290\n",
      "     31        \u001b[36m0.8696\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.8644\u001b[0m  0.0310\n",
      "     33        \u001b[36m0.8595\u001b[0m  0.0290\n",
      "     34        \u001b[36m0.8546\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.8499\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.8453\u001b[0m  0.0310\n",
      "     37        \u001b[36m0.8409\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.8366\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.8324\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.8283\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.8243\u001b[0m  0.0310\n",
      "     42        \u001b[36m0.8204\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.8166\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.8128\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.8092\u001b[0m  0.0290\n",
      "     46        \u001b[36m0.8056\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.8022\u001b[0m  0.0290\n",
      "     48        \u001b[36m0.7988\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.7954\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.7921\u001b[0m  0.0300\n",
      "     51        \u001b[36m0.7889\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.7858\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.7827\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.7797\u001b[0m  0.0290\n",
      "     55        \u001b[36m0.7767\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.7738\u001b[0m  0.0290\n",
      "     57        \u001b[36m0.7709\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.7681\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.7653\u001b[0m  0.0330\n",
      "     60        \u001b[36m0.7626\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.7599\u001b[0m  0.0310\n",
      "     62        \u001b[36m0.7573\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.7547\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.7522\u001b[0m  0.0290\n",
      "     65        \u001b[36m0.7497\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.7472\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.7448\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.7424\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.7400\u001b[0m  0.0290\n",
      "     70        \u001b[36m0.7377\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.7354\u001b[0m  0.0290\n",
      "     72        \u001b[36m0.7332\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.7310\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.7288\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.7267\u001b[0m  0.0310\n",
      "     76        \u001b[36m0.7246\u001b[0m  0.0290\n",
      "     77        \u001b[36m0.7225\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.7204\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.7184\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.7164\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.7145\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.7126\u001b[0m  0.0310\n",
      "     83        \u001b[36m0.7107\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.7088\u001b[0m  0.0310\n",
      "     85        \u001b[36m0.7070\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.7051\u001b[0m  0.0310\n",
      "     87        \u001b[36m0.7033\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.7016\u001b[0m  0.0300\n",
      "     89        \u001b[36m0.6999\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.6981\u001b[0m  0.0300\n",
      "     91        \u001b[36m0.6965\u001b[0m  0.0350\n",
      "     92        \u001b[36m0.6948\u001b[0m  0.0300\n",
      "     93        \u001b[36m0.6932\u001b[0m  0.0300\n",
      "     94        \u001b[36m0.6916\u001b[0m  0.0310\n",
      "     95        \u001b[36m0.6900\u001b[0m  0.0300\n",
      "     96        \u001b[36m0.6884\u001b[0m  0.0310\n",
      "     97        \u001b[36m0.6869\u001b[0m  0.0300\n",
      "     98        \u001b[36m0.6854\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.6839\u001b[0m  0.0300\n",
      "    100        \u001b[36m0.6824\u001b[0m  0.0290\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2015\u001b[0m  0.0290\n",
      "      2        \u001b[36m1.1649\u001b[0m  0.0310\n",
      "      3        \u001b[36m1.1327\u001b[0m  0.0300\n",
      "      4        \u001b[36m1.1034\u001b[0m  0.0300\n",
      "      5        \u001b[36m1.0771\u001b[0m  0.0290\n",
      "      6        \u001b[36m1.0533\u001b[0m  0.0350"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        \u001b[36m1.0320\u001b[0m  0.0300\n",
      "      8        \u001b[36m1.0129\u001b[0m  0.0290\n",
      "      9        \u001b[36m0.9957\u001b[0m  0.0300\n",
      "     10        \u001b[36m0.9760\u001b[0m  0.0290\n",
      "     11        \u001b[36m0.9623\u001b[0m  0.0300\n",
      "     12        \u001b[36m0.9498\u001b[0m  0.0290\n",
      "     13        \u001b[36m0.9385\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.9282\u001b[0m  0.0310\n",
      "     15        \u001b[36m0.9187\u001b[0m  0.0290\n",
      "     16        \u001b[36m0.9101\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.9022\u001b[0m  0.0290\n",
      "     18        \u001b[36m0.8950\u001b[0m  0.0300\n",
      "     19        \u001b[36m0.8883\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.8822\u001b[0m  0.0290\n",
      "     21        \u001b[36m0.8765\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.8711\u001b[0m  0.0290\n",
      "     23        \u001b[36m0.8662\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.8615\u001b[0m  0.0290\n",
      "     25        \u001b[36m0.8572\u001b[0m  0.0290\n",
      "     26        \u001b[36m0.8530\u001b[0m  0.0310\n",
      "     27        \u001b[36m0.8491\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.8453\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.8417\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.8383\u001b[0m  0.0290\n",
      "     31        \u001b[36m0.8349\u001b[0m  0.0290\n",
      "     32        \u001b[36m0.8317\u001b[0m  0.0300\n",
      "     33        \u001b[36m0.8286\u001b[0m  0.0290\n",
      "     34        \u001b[36m0.8255\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.8226\u001b[0m  0.0290\n",
      "     36        \u001b[36m0.8197\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.8169\u001b[0m  0.0290\n",
      "     38        \u001b[36m0.8141\u001b[0m  0.0310\n",
      "     39        \u001b[36m0.8114\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.8088\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.8062\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.8036\u001b[0m  0.0290\n",
      "     43        \u001b[36m0.8011\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.7987\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.7963\u001b[0m  0.0290\n",
      "     46        \u001b[36m0.7939\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.7916\u001b[0m  0.0290\n",
      "     48        \u001b[36m0.7893\u001b[0m  0.0360\n",
      "     49        \u001b[36m0.7870\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.7848\u001b[0m  0.0300\n",
      "     51        \u001b[36m0.7826\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.7805\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.7784\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.7763\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.7742\u001b[0m  0.0290\n",
      "     56        \u001b[36m0.7722\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.7702\u001b[0m  0.0290\n",
      "     58        \u001b[36m0.7682\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.7662\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.7643\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.7624\u001b[0m  0.0290\n",
      "     62        \u001b[36m0.7605\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.7587\u001b[0m  0.0290\n",
      "     64        \u001b[36m0.7568\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.7550\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.7532\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.7515\u001b[0m  0.0290\n",
      "     68        \u001b[36m0.7497\u001b[0m  0.0310\n",
      "     69        \u001b[36m0.7480\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.7463\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.7446\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.7429\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.7413\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.7396\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.7380\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.7364\u001b[0m  0.0290\n",
      "     77        \u001b[36m0.7348\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.7332\u001b[0m  0.0320\n",
      "     79        \u001b[36m0.7317\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.7301\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.7286\u001b[0m  0.0310\n",
      "     82        \u001b[36m0.7271\u001b[0m  0.0300\n",
      "     83        \u001b[36m0.7256\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.7241\u001b[0m  0.0290\n",
      "     85        \u001b[36m0.7227\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.7212\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.7198\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.7184\u001b[0m  0.0300\n",
      "     89        \u001b[36m0.7170\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.7156\u001b[0m  0.0300\n",
      "     91        \u001b[36m0.7142\u001b[0m  0.0300\n",
      "     92        \u001b[36m0.7128\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.7115\u001b[0m  0.0300\n",
      "     94        \u001b[36m0.7101\u001b[0m  0.0300\n",
      "     95        \u001b[36m0.7088\u001b[0m  0.0300\n",
      "     96        \u001b[36m0.7075\u001b[0m  0.0290\n",
      "     97        \u001b[36m0.7062\u001b[0m  0.0300\n",
      "     98        \u001b[36m0.7049\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.7036\u001b[0m  0.0290\n",
      "    100        \u001b[36m0.7023\u001b[0m  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6801\u001b[0m  0.0300\n",
      "      2        \u001b[36m0.6778\u001b[0m  0.0310\n",
      "      3        \u001b[36m0.6762\u001b[0m  0.0300\n",
      "      4        \u001b[36m0.6746\u001b[0m  0.0300\n",
      "      5        \u001b[36m0.6730\u001b[0m  0.0300\n",
      "      6        \u001b[36m0.6716\u001b[0m  0.0300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        \u001b[36m0.6702\u001b[0m  0.0300\n",
      "      8        \u001b[36m0.6688\u001b[0m  0.0300\n",
      "      9        \u001b[36m0.6675\u001b[0m  0.0300\n",
      "     10        \u001b[36m0.6662\u001b[0m  0.0300\n",
      "     11        \u001b[36m0.6650\u001b[0m  0.0310\n",
      "     12        \u001b[36m0.6638\u001b[0m  0.0310\n",
      "     13        \u001b[36m0.6627\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.6615\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.6604\u001b[0m  0.0300\n",
      "     16        \u001b[36m0.6593\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.6583\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.6573\u001b[0m  0.0300\n",
      "     19        \u001b[36m0.6562\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.6552\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.6543\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.6533\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.6523\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.6514\u001b[0m  0.0310\n",
      "     25        \u001b[36m0.6505\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.6496\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.6487\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.6478\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.6469\u001b[0m  0.0290\n",
      "     30        \u001b[36m0.6461\u001b[0m  0.0300\n",
      "     31        \u001b[36m0.6453\u001b[0m  0.0310\n",
      "     32        \u001b[36m0.6444\u001b[0m  0.0300\n",
      "     33        \u001b[36m0.6436\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.6428\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.6420\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.6412\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6404\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6396\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6389\u001b[0m  0.0310\n",
      "     40        \u001b[36m0.6381\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6374\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6366\u001b[0m  0.0310\n",
      "     43        \u001b[36m0.6359\u001b[0m  0.0320\n",
      "     44        \u001b[36m0.6352\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6345\u001b[0m  0.0290\n",
      "     46        \u001b[36m0.6338\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6331\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.6324\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6317\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6310\u001b[0m  0.0310\n",
      "     51        \u001b[36m0.6304\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6297\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6290\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6284\u001b[0m  0.0310\n",
      "     55        \u001b[36m0.6278\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6271\u001b[0m  0.0310\n",
      "     57        \u001b[36m0.6265\u001b[0m  0.0310\n",
      "     58        \u001b[36m0.6259\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6253\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6246\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6240\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6234\u001b[0m  0.0330\n",
      "     63        \u001b[36m0.6229\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6223\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6217\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6211\u001b[0m  0.0310\n",
      "     67        \u001b[36m0.6205\u001b[0m  0.0290\n",
      "     68        \u001b[36m0.6200\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6194\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6189\u001b[0m  0.0310\n",
      "     71        \u001b[36m0.6183\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.6178\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6172\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6167\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.6162\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.6157\u001b[0m  0.0310\n",
      "     77        \u001b[36m0.6151\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6146\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6141\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.6136\u001b[0m  0.0310\n",
      "     81        \u001b[36m0.6131\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.6126\u001b[0m  0.0300\n",
      "     83        \u001b[36m0.6121\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6117\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.6112\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6107\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.6102\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.6098\u001b[0m  0.0310\n",
      "     89        \u001b[36m0.6093\u001b[0m  0.0310\n",
      "     90        \u001b[36m0.6088\u001b[0m  0.0300\n",
      "     91        \u001b[36m0.6084\u001b[0m  0.0290\n",
      "     92        \u001b[36m0.6079\u001b[0m  0.0300\n",
      "     93        \u001b[36m0.6075\u001b[0m  0.0300\n",
      "     94        \u001b[36m0.6070\u001b[0m  0.0330\n",
      "     95        \u001b[36m0.6066\u001b[0m  0.0300\n",
      "     96        \u001b[36m0.6062\u001b[0m  0.0310\n",
      "     97        \u001b[36m0.6057\u001b[0m  0.0310\n",
      "     98        \u001b[36m0.6053\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.6049\u001b[0m  0.0300\n",
      "    100        \u001b[36m0.6045\u001b[0m  0.0310\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7402\u001b[0m  0.0300\n",
      "      2        \u001b[36m0.7325\u001b[0m  0.0290\n",
      "      3        \u001b[36m0.7268\u001b[0m  0.0300\n",
      "      4        \u001b[36m0.7176\u001b[0m  0.0300\n",
      "      5        \u001b[36m0.7086\u001b[0m  0.0300\n",
      "      6        \u001b[36m0.7005\u001b[0m  0.0300"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        \u001b[36m0.6932\u001b[0m  0.0310\n",
      "      8        \u001b[36m0.6865\u001b[0m  0.0330\n",
      "      9        \u001b[36m0.6805\u001b[0m  0.0300\n",
      "     10        \u001b[36m0.6751\u001b[0m  0.0300\n",
      "     11        \u001b[36m0.6702\u001b[0m  0.0300\n",
      "     12        \u001b[36m0.6658\u001b[0m  0.0300\n",
      "     13        \u001b[36m0.6617\u001b[0m  0.0290\n",
      "     14        \u001b[36m0.6581\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.6547\u001b[0m  0.0300\n",
      "     16        \u001b[36m0.6517\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.6479\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.6454\u001b[0m  0.0310\n",
      "     19        \u001b[36m0.6432\u001b[0m  0.0310\n",
      "     20        \u001b[36m0.6411\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.6391\u001b[0m  0.0310\n",
      "     22        \u001b[36m0.6374\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.6357\u001b[0m  0.0290\n",
      "     24        \u001b[36m0.6342\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.6328\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.6315\u001b[0m  0.0290\n",
      "     27        \u001b[36m0.6302\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.6291\u001b[0m  0.0290\n",
      "     29        \u001b[36m0.6280\u001b[0m  0.0310\n",
      "     30        \u001b[36m0.6270\u001b[0m  0.0300\n",
      "     31        \u001b[36m0.6261\u001b[0m  0.0290\n",
      "     32        \u001b[36m0.6252\u001b[0m  0.0300\n",
      "     33        \u001b[36m0.6243\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.6235\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.6228\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.6221\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6214\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6207\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6201\u001b[0m  0.0290\n",
      "     40        \u001b[36m0.6195\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6189\u001b[0m  0.0310\n",
      "     42        \u001b[36m0.6184\u001b[0m  0.0290\n",
      "     43        \u001b[36m0.6178\u001b[0m  0.0310\n",
      "     44        \u001b[36m0.6173\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6139\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6050\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6046\u001b[0m  0.0310\n",
      "     48        \u001b[36m0.6041\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.5958\u001b[0m  0.0300\n",
      "     50        0.6018  0.0300\n",
      "     51        0.6006  0.0300\n",
      "     52        0.6012  0.0300\n",
      "     53        0.6008  0.0310\n",
      "     54        0.6005  0.0310\n",
      "     55        0.6001  0.0300\n",
      "     56        0.5998  0.0300\n",
      "     57        0.5994  0.0300\n",
      "     58        0.5991  0.0300\n",
      "     59        0.5988  0.0300\n",
      "     60        0.5985  0.0300\n",
      "     61        0.5982  0.0300\n",
      "     62        0.5979  0.0310\n",
      "     63        0.5976  0.0300\n",
      "     64        0.5973  0.0300\n",
      "     65        0.5971  0.0300\n",
      "     66        0.5968  0.0300\n",
      "     67        0.5965  0.0300\n",
      "     68        0.5963  0.0300\n",
      "     69        0.5960  0.0390\n",
      "     70        \u001b[36m0.5958\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.5955\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.5953\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.5951\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.5948\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.5946\u001b[0m  0.0310\n",
      "     76        \u001b[36m0.5944\u001b[0m  0.0290\n",
      "     77        \u001b[36m0.5942\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.5939\u001b[0m  0.0310\n",
      "     79        \u001b[36m0.5937\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.5935\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.5933\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.5931\u001b[0m  0.0300\n",
      "     83        \u001b[36m0.5929\u001b[0m  0.0310\n",
      "     84        \u001b[36m0.5927\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.5925\u001b[0m  0.0310\n",
      "     86        \u001b[36m0.5923\u001b[0m  0.0330\n",
      "     87        \u001b[36m0.5921\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.5919\u001b[0m  0.0300\n",
      "     89        \u001b[36m0.5918\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.5916\u001b[0m  0.0310\n",
      "     91        \u001b[36m0.5914\u001b[0m  0.0300\n",
      "     92        \u001b[36m0.5912\u001b[0m  0.0300\n",
      "     93        \u001b[36m0.5910\u001b[0m  0.0310\n",
      "     94        \u001b[36m0.5909\u001b[0m  0.0300\n",
      "     95        \u001b[36m0.5907\u001b[0m  0.0300\n",
      "     96        \u001b[36m0.5905\u001b[0m  0.0310\n",
      "     97        \u001b[36m0.5904\u001b[0m  0.0310\n",
      "     98        \u001b[36m0.5902\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.5900\u001b[0m  0.0300\n",
      "    100        \u001b[36m0.5899\u001b[0m  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0269\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.0004\u001b[0m  0.0300\n",
      "      3        \u001b[36m0.9855\u001b[0m  0.0300\n",
      "      4        \u001b[36m0.9701\u001b[0m  0.0300\n",
      "      5        \u001b[36m0.9535\u001b[0m  0.0320\n",
      "      6        \u001b[36m0.9398\u001b[0m  0.0290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.9257\u001b[0m  0.0310\n",
      "      8        \u001b[36m0.9148\u001b[0m  0.0300\n",
      "      9        \u001b[36m0.9048\u001b[0m  0.0300\n",
      "     10        \u001b[36m0.8955\u001b[0m  0.0300\n",
      "     11        \u001b[36m0.8869\u001b[0m  0.0300\n",
      "     12        \u001b[36m0.8790\u001b[0m  0.0300\n",
      "     13        \u001b[36m0.8716\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.8648\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.8584\u001b[0m  0.0310\n",
      "     16        \u001b[36m0.8524\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.8469\u001b[0m  0.0310\n",
      "     18        \u001b[36m0.8416\u001b[0m  0.0310\n",
      "     19        \u001b[36m0.8367\u001b[0m  0.0290\n",
      "     20        \u001b[36m0.8321\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.8277\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.8235\u001b[0m  0.0350\n",
      "     23        \u001b[36m0.8196\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.8158\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.8122\u001b[0m  0.0310\n",
      "     26        \u001b[36m0.8088\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.8055\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.8023\u001b[0m  0.0290\n",
      "     29        \u001b[36m0.7993\u001b[0m  0.0310\n",
      "     30        \u001b[36m0.7963\u001b[0m  0.0320\n",
      "     31        \u001b[36m0.7935\u001b[0m  0.0310\n",
      "     32        \u001b[36m0.7907\u001b[0m  0.0310\n",
      "     33        \u001b[36m0.7880\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.7854\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.7829\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.7805\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.7781\u001b[0m  0.0320\n",
      "     38        \u001b[36m0.7757\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.7734\u001b[0m  0.0310\n",
      "     40        \u001b[36m0.7712\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.7690\u001b[0m  0.0310\n",
      "     42        \u001b[36m0.7668\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.7647\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.7626\u001b[0m  0.0310\n",
      "     45        \u001b[36m0.7605\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.7585\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.7565\u001b[0m  0.0310\n",
      "     48        \u001b[36m0.7546\u001b[0m  0.0310\n",
      "     49        \u001b[36m0.7526\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.7507\u001b[0m  0.0300\n",
      "     51        \u001b[36m0.7488\u001b[0m  0.0310\n",
      "     52        \u001b[36m0.7470\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.7452\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.7433\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.7416\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.7398\u001b[0m  0.0290\n",
      "     57        \u001b[36m0.7380\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.7364\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.7347\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.7330\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.7314\u001b[0m  0.0290\n",
      "     62        \u001b[36m0.7297\u001b[0m  0.0310\n",
      "     63        \u001b[36m0.7281\u001b[0m  0.0310\n",
      "     64        \u001b[36m0.7265\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.7250\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.7234\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.7219\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.7204\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.7189\u001b[0m  0.0310\n",
      "     70        \u001b[36m0.7174\u001b[0m  0.0290\n",
      "     71        \u001b[36m0.7160\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.7145\u001b[0m  0.0310\n",
      "     73        \u001b[36m0.7131\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.7117\u001b[0m  0.0310\n",
      "     75        \u001b[36m0.7103\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.7090\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.7077\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.7063\u001b[0m  0.0290\n",
      "     79        \u001b[36m0.7050\u001b[0m  0.0310\n",
      "     80        \u001b[36m0.7038\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.7025\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.7012\u001b[0m  0.0310\n",
      "     83        \u001b[36m0.7000\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6988\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.6976\u001b[0m  0.0330\n",
      "     86        \u001b[36m0.6964\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.6953\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.6942\u001b[0m  0.0300\n",
      "     89        \u001b[36m0.6930\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.6919\u001b[0m  0.0300\n",
      "     91        \u001b[36m0.6908\u001b[0m  0.0300\n",
      "     92        \u001b[36m0.6898\u001b[0m  0.0300\n",
      "     93        \u001b[36m0.6887\u001b[0m  0.0310\n",
      "     94        \u001b[36m0.6877\u001b[0m  0.0310\n",
      "     95        \u001b[36m0.6867\u001b[0m  0.0300\n",
      "     96        \u001b[36m0.6856\u001b[0m  0.0300\n",
      "     97        \u001b[36m0.6847\u001b[0m  0.0310\n",
      "     98        \u001b[36m0.6837\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.6827\u001b[0m  0.0300\n",
      "    100        \u001b[36m0.6818\u001b[0m  0.0310\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m5.3694\u001b[0m  0.0410\n",
      "      2        \u001b[36m4.4940\u001b[0m  0.0400\n",
      "      3        \u001b[36m3.4807\u001b[0m  0.0410\n",
      "      4        \u001b[36m2.4862\u001b[0m  0.0400\n",
      "      5        \u001b[36m1.4072\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.8974\u001b[0m  0.0400\n",
      "      7        \u001b[36m0.8085\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.7650\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.7405\u001b[0m  0.0410\n",
      "     10        \u001b[36m0.7288\u001b[0m  0.0390\n",
      "     11        0.7319  0.0410\n",
      "     12        \u001b[36m0.7164\u001b[0m  0.0390\n",
      "     13        \u001b[36m0.7074\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6978\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6929\u001b[0m  0.0390\n",
      "     16        \u001b[36m0.6778\u001b[0m  0.0400\n",
      "     17        \u001b[36m0.6586\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6416\u001b[0m  0.0400\n",
      "     19        \u001b[36m0.6366\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.6280\u001b[0m  0.0400\n",
      "     21        \u001b[36m0.6240\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6229\u001b[0m  0.0390\n",
      "     23        \u001b[36m0.6186\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.6000\u001b[0m  0.0390\n",
      "     25        0.6007  0.0400\n",
      "     26        \u001b[36m0.5885\u001b[0m  0.0400\n",
      "     27        \u001b[36m0.5860\u001b[0m  0.0400\n",
      "     28        0.5909  0.0410\n",
      "     29        \u001b[36m0.5832\u001b[0m  0.0390\n",
      "     30        0.5885  0.0410\n",
      "     31        0.5865  0.0400\n",
      "     32        0.5870  0.0400\n",
      "     33        0.5853  0.0400\n",
      "     34        \u001b[36m0.5795\u001b[0m  0.0390\n",
      "     35        \u001b[36m0.5778\u001b[0m  0.0400\n",
      "     36        \u001b[36m0.5725\u001b[0m  0.0410\n",
      "     37        \u001b[36m0.5701\u001b[0m  0.0390\n",
      "     38        0.5722  0.0390\n",
      "     39        \u001b[36m0.5700\u001b[0m  0.0400\n",
      "     40        \u001b[36m0.5570\u001b[0m  0.0390\n",
      "     41        \u001b[36m0.5565\u001b[0m  0.0400\n",
      "     42        0.5610  0.0400\n",
      "     43        0.5626  0.0390\n",
      "     44        \u001b[36m0.5419\u001b[0m  0.0410\n",
      "     45        0.5492  0.0400\n",
      "     46        0.5507  0.0410\n",
      "     47        0.5478  0.0400\n",
      "     48        0.5471  0.0400\n",
      "     49        0.5457  0.0400\n",
      "     50        0.5442  0.0400\n",
      "     51        \u001b[36m0.5339\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.5267\u001b[0m  0.0400\n",
      "     53        \u001b[36m0.5253\u001b[0m  0.0410\n",
      "     54        \u001b[36m0.5243\u001b[0m  0.0400\n",
      "     55        \u001b[36m0.5228\u001b[0m  0.0400\n",
      "     56        0.5228  0.0390\n",
      "     57        \u001b[36m0.5219\u001b[0m  0.0410\n",
      "     58        \u001b[36m0.5212\u001b[0m  0.0400\n",
      "     59        \u001b[36m0.5206\u001b[0m  0.0400\n",
      "     60        \u001b[36m0.5199\u001b[0m  0.0410\n",
      "     61        0.5286  0.0400\n",
      "     62        0.5237  0.0400\n",
      "     63        \u001b[36m0.5167\u001b[0m  0.0390\n",
      "     64        \u001b[36m0.5160\u001b[0m  0.0400\n",
      "     65        \u001b[36m0.5151\u001b[0m  0.0390\n",
      "     66        \u001b[36m0.5142\u001b[0m  0.0400\n",
      "     67        \u001b[36m0.5097\u001b[0m  0.0400\n",
      "     68        \u001b[36m0.5092\u001b[0m  0.0400\n",
      "     69        \u001b[36m0.5086\u001b[0m  0.0400\n",
      "     70        \u001b[36m0.5082\u001b[0m  0.0390\n",
      "     71        \u001b[36m0.5078\u001b[0m  0.0400\n",
      "     72        0.5318  0.0390\n",
      "     73        0.6778  0.0410\n",
      "     74        0.6183  0.0400\n",
      "     75        0.5926  0.0400\n",
      "     76        0.5739  0.0410\n",
      "     77        0.5652  0.0400\n",
      "     78        0.5535  0.0400\n",
      "     79        0.5534  0.0400\n",
      "     80        0.5430  0.0400\n",
      "     81        0.5406  0.0400\n",
      "     82        0.5378  0.0400\n",
      "     83        0.5354  0.0400\n",
      "     84        0.5335  0.0400\n",
      "     85        0.5318  0.0400\n",
      "     86        0.5303  0.0390\n",
      "     87        0.5291  0.0400\n",
      "     88        0.5280  0.0390\n",
      "     89        0.5270  0.0400\n",
      "     90        0.5261  0.0400\n",
      "     91        0.5253  0.0410\n",
      "     92        0.5246  0.0400\n",
      "     93        0.5240  0.0390\n",
      "     94        0.5234  0.0400\n",
      "     95        0.5224  0.0400\n",
      "     96        0.5232  0.0400\n",
      "     97        0.5225  0.0400\n",
      "     98        0.5218  0.0400\n",
      "     99        0.5212  0.0410\n",
      "    100        0.5208  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0813\u001b[0m  0.0400\n",
      "      2        \u001b[36m0.9345\u001b[0m  0.0400\n",
      "      3        \u001b[36m0.9058\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.8793\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.8438\u001b[0m  0.0410\n",
      "      6        \u001b[36m0.8071\u001b[0m  0.0410\n",
      "      7        \u001b[36m0.7897\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.7636\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.7251\u001b[0m  0.0400\n",
      "     10        \u001b[36m0.7097\u001b[0m  0.0410\n",
      "     11        \u001b[36m0.6986\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.6925\u001b[0m  0.0390\n",
      "     13        \u001b[36m0.6709\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6613\u001b[0m  0.0400\n",
      "     15        \u001b[36m0.6538\u001b[0m  0.0390\n",
      "     16        \u001b[36m0.6459\u001b[0m  0.0400\n",
      "     17        \u001b[36m0.6433\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6231\u001b[0m  0.0400\n",
      "     19        0.6235  0.0390\n",
      "     20        \u001b[36m0.6155\u001b[0m  0.0410\n",
      "     21        \u001b[36m0.6105\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.6058\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.6044\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.6010\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.5954\u001b[0m  0.0390\n",
      "     26        \u001b[36m0.5925\u001b[0m  0.0400\n",
      "     27        \u001b[36m0.5862\u001b[0m  0.0400\n",
      "     28        \u001b[36m0.5820\u001b[0m  0.0400\n",
      "     29        \u001b[36m0.5780\u001b[0m  0.0400\n",
      "     30        \u001b[36m0.5741\u001b[0m  0.0390\n",
      "     31        \u001b[36m0.5704\u001b[0m  0.0400\n",
      "     32        \u001b[36m0.5670\u001b[0m  0.0390\n",
      "     33        \u001b[36m0.5636\u001b[0m  0.0390\n",
      "     34        \u001b[36m0.5604\u001b[0m  0.0410\n",
      "     35        \u001b[36m0.5573\u001b[0m  0.0400\n",
      "     36        \u001b[36m0.5544\u001b[0m  0.0390\n",
      "     37        \u001b[36m0.5516\u001b[0m  0.0400\n",
      "     38        \u001b[36m0.5489\u001b[0m  0.0390\n",
      "     39        \u001b[36m0.5464\u001b[0m  0.0400\n",
      "     40        0.5479  0.0400\n",
      "     41        0.5518  0.0400\n",
      "     42        0.5491  0.0390\n",
      "     43        0.5738  0.0400\n",
      "     44        0.5835  0.0410\n",
      "     45        0.5608  0.0400\n",
      "     46        \u001b[36m0.5431\u001b[0m  0.0400\n",
      "     47        \u001b[36m0.5393\u001b[0m  0.0400\n",
      "     48        \u001b[36m0.5386\u001b[0m  0.0400\n",
      "     49        \u001b[36m0.5327\u001b[0m  0.0410\n",
      "     50        \u001b[36m0.5322\u001b[0m  0.0400\n",
      "     51        \u001b[36m0.5313\u001b[0m  0.0410\n",
      "     52        \u001b[36m0.5261\u001b[0m  0.0400\n",
      "     53        0.5292  0.0400\n",
      "     54        0.5275  0.0390\n",
      "     55        0.5286  0.0400\n",
      "     56        0.5407  0.0400\n",
      "     57        \u001b[36m0.5215\u001b[0m  0.0400\n",
      "     58        \u001b[36m0.5124\u001b[0m  0.0400\n",
      "     59        0.5624  0.0410\n",
      "     60        0.5309  0.0390\n",
      "     61        0.5254  0.0390\n",
      "     62        0.5249  0.0400\n",
      "     63        0.5217  0.0400\n",
      "     64        0.5216  0.0400\n",
      "     65        0.5152  0.0400\n",
      "     66        0.5143  0.0400\n",
      "     67        0.5150  0.0400\n",
      "     68        \u001b[36m0.5103\u001b[0m  0.0400\n",
      "     69        \u001b[36m0.5101\u001b[0m  0.0400\n",
      "     70        \u001b[36m0.5084\u001b[0m  0.0390\n",
      "     71        \u001b[36m0.5079\u001b[0m  0.0400\n",
      "     72        \u001b[36m0.5055\u001b[0m  0.0400\n",
      "     73        \u001b[36m0.5051\u001b[0m  0.0400\n",
      "     74        \u001b[36m0.5038\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.5027\u001b[0m  0.0410\n",
      "     76        \u001b[36m0.5016\u001b[0m  0.0410\n",
      "     77        \u001b[36m0.5008\u001b[0m  0.0400\n",
      "     78        \u001b[36m0.4979\u001b[0m  0.0400\n",
      "     79        \u001b[36m0.4973\u001b[0m  0.0400\n",
      "     80        \u001b[36m0.4964\u001b[0m  0.0400\n",
      "     81        0.4981  0.0400\n",
      "     82        0.4967  0.0400\n",
      "     83        \u001b[36m0.4958\u001b[0m  0.0400\n",
      "     84        \u001b[36m0.4950\u001b[0m  0.0420\n",
      "     85        \u001b[36m0.4943\u001b[0m  0.0390\n",
      "     86        \u001b[36m0.4936\u001b[0m  0.0400\n",
      "     87        \u001b[36m0.4929\u001b[0m  0.0400\n",
      "     88        \u001b[36m0.4923\u001b[0m  0.0400\n",
      "     89        \u001b[36m0.4917\u001b[0m  0.0400\n",
      "     90        \u001b[36m0.4911\u001b[0m  0.0400\n",
      "     91        \u001b[36m0.4905\u001b[0m  0.0400\n",
      "     92        \u001b[36m0.4899\u001b[0m  0.0400\n",
      "     93        \u001b[36m0.4894\u001b[0m  0.0410\n",
      "     94        \u001b[36m0.4889\u001b[0m  0.0410\n",
      "     95        \u001b[36m0.4884\u001b[0m  0.0400\n",
      "     96        \u001b[36m0.4879\u001b[0m  0.0400\n",
      "     97        \u001b[36m0.4875\u001b[0m  0.0400\n",
      "     98        0.4881  0.0440\n",
      "     99        \u001b[36m0.4868\u001b[0m  0.0400\n",
      "    100        0.4909  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.1758\u001b[0m  0.0400\n",
      "      2        \u001b[36m0.8400\u001b[0m  0.0400\n",
      "      3        \u001b[36m0.7398\u001b[0m  0.0400\n",
      "      4        \u001b[36m0.7010\u001b[0m  0.0410\n",
      "      5        \u001b[36m0.6783\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.6522\u001b[0m  0.0410\n",
      "      7        0.6542  0.0410\n",
      "      8        \u001b[36m0.6376\u001b[0m  0.0420\n",
      "      9        \u001b[36m0.6252\u001b[0m  0.0390\n",
      "     10        \u001b[36m0.6085\u001b[0m  0.0390\n",
      "     11        \u001b[36m0.6058\u001b[0m  0.0400\n",
      "     12        \u001b[36m0.5977\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.5918\u001b[0m  0.0390\n",
      "     14        \u001b[36m0.5693\u001b[0m  0.0400\n",
      "     15        \u001b[36m0.5653\u001b[0m  0.0400\n",
      "     16        \u001b[36m0.5481\u001b[0m  0.0400\n",
      "     17        \u001b[36m0.5454\u001b[0m  0.0390\n",
      "     18        \u001b[36m0.5413\u001b[0m  0.0400\n",
      "     19        \u001b[36m0.5357\u001b[0m  0.0390\n",
      "     20        \u001b[36m0.5321\u001b[0m  0.0400\n",
      "     21        \u001b[36m0.5285\u001b[0m  0.0400\n",
      "     22        \u001b[36m0.5245\u001b[0m  0.0400\n",
      "     23        \u001b[36m0.5215\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.5166\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.5145\u001b[0m  0.0400\n",
      "     26        \u001b[36m0.5091\u001b[0m  0.0400\n",
      "     27        \u001b[36m0.5089\u001b[0m  0.0390\n",
      "     28        \u001b[36m0.5079\u001b[0m  0.0400\n",
      "     29        \u001b[36m0.5053\u001b[0m  0.0390\n",
      "     30        \u001b[36m0.5042\u001b[0m  0.0400\n",
      "     31        \u001b[36m0.5030\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.5024\u001b[0m  0.0400\n",
      "     33        \u001b[36m0.4977\u001b[0m  0.0410\n",
      "     34        \u001b[36m0.4974\u001b[0m  0.0400\n",
      "     35        0.6723  0.0400\n",
      "     36        0.7561  0.0390\n",
      "     37        0.6217  0.0400\n",
      "     38        0.5752  0.0400\n",
      "     39        0.5655  0.0400\n",
      "     40        0.5441  0.0410\n",
      "     41        0.5200  0.0390\n",
      "     42        0.5146  0.0400\n",
      "     43        0.5102  0.0400\n",
      "     44        0.5049  0.0520\n",
      "     45        0.5036  0.0400\n",
      "     46        0.4978  0.0400\n",
      "     47        0.5015  0.0400\n",
      "     48        0.5326  0.0400\n",
      "     49        0.5058  0.0400\n",
      "     50        \u001b[36m0.4846\u001b[0m  0.0410\n",
      "     51        0.4960  0.0390\n",
      "     52        0.4961  0.0420\n",
      "     53        0.4965  0.0390\n",
      "     54        0.5020  0.0400\n",
      "     55        0.4963  0.0400\n",
      "     56        0.4969  0.0400\n",
      "     57        0.4959  0.0410\n",
      "     58        0.4955  0.0390\n",
      "     59        0.4951  0.0400\n",
      "     60        0.4947  0.0390\n",
      "     61        0.4992  0.0400\n",
      "     62        0.4994  0.0390\n",
      "     63        0.4986  0.0400\n",
      "     64        0.4986  0.0390\n",
      "     65        0.4993  0.0400\n",
      "     66        0.4988  0.0400\n",
      "     67        0.4985  0.0400\n",
      "     68        0.4994  0.0390\n",
      "     69        0.4991  0.0400\n",
      "     70        0.4987  0.0400\n",
      "     71        0.4983  0.0410\n",
      "     72        0.5059  0.0390\n",
      "     73        0.5058  0.0410\n",
      "     74        0.5080  0.0400\n",
      "     75        0.5066  0.0400\n",
      "     76        0.5054  0.0400\n",
      "     77        0.5051  0.0400\n",
      "     78        0.5023  0.0390\n",
      "     79        0.5037  0.0420\n",
      "     80        0.5040  0.0400\n",
      "     81        0.5112  0.0400\n",
      "     82        0.5143  0.0400\n",
      "     83        0.5167  0.0390\n",
      "     84        0.5145  0.0400\n",
      "     85        0.5125  0.0400\n",
      "     86        0.5131  0.0390\n",
      "     87        0.5148  0.0400\n",
      "     88        0.5107  0.0400\n",
      "     89        0.5083  0.0400\n",
      "     90        0.5154  0.0390\n",
      "     91        0.5104  0.0400\n",
      "     92        0.5079  0.0400\n",
      "     93        0.5060  0.0390\n",
      "     94        0.5048  0.0400\n",
      "     95        0.5040  0.0400\n",
      "     96        0.5033  0.0410\n",
      "     97        0.5028  0.0390\n",
      "     98        0.5024  0.0410\n",
      "     99        0.5022  0.0410\n",
      "    100        0.5019  0.0390\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0301\u001b[0m  0.0440\n",
      "      2        \u001b[36m0.9348\u001b[0m  0.0400\n",
      "      3        \u001b[36m0.8922\u001b[0m  0.0400\n",
      "      4        \u001b[36m0.8071\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.7700\u001b[0m  0.0410\n",
      "      6        \u001b[36m0.7434\u001b[0m  0.0410\n",
      "      7        \u001b[36m0.6966\u001b[0m  0.0400\n",
      "      8        \u001b[36m0.6631\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6481\u001b[0m  0.0390\n",
      "     10        \u001b[36m0.6381\u001b[0m  0.0400\n",
      "     11        \u001b[36m0.6166\u001b[0m  0.0390\n",
      "     12        \u001b[36m0.5996\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.5847\u001b[0m  0.0390\n",
      "     14        \u001b[36m0.5552\u001b[0m  0.0390\n",
      "     15        0.5569  0.0400\n",
      "     16        \u001b[36m0.5379\u001b[0m  0.0390\n",
      "     17        \u001b[36m0.5165\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.5152\u001b[0m  0.0390\n",
      "     19        \u001b[36m0.5088\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.4940\u001b[0m  0.0400\n",
      "     21        0.5035  0.0400\n",
      "     22        0.5042  0.0400\n",
      "     23        \u001b[36m0.4897\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.4849\u001b[0m  0.0450\n",
      "     25        \u001b[36m0.4801\u001b[0m  0.1100\n",
      "     26        0.4870  0.0740\n",
      "     27        0.4840  0.0840\n",
      "     28        \u001b[36m0.4780\u001b[0m  0.0410\n",
      "     29        \u001b[36m0.4742\u001b[0m  0.0400\n",
      "     30        \u001b[36m0.4683\u001b[0m  0.0410\n",
      "     31        \u001b[36m0.4665\u001b[0m  0.0400\n",
      "     32        \u001b[36m0.4637\u001b[0m  0.0410\n",
      "     33        0.4745  0.0410\n",
      "     34        \u001b[36m0.4594\u001b[0m  0.0400\n",
      "     35        0.4721  0.0410\n",
      "     36        0.4727  0.0430\n",
      "     37        0.4753  0.0410\n",
      "     38        0.4856  0.0400\n",
      "     39        0.4635  0.0410\n",
      "     40        0.4789  0.0400\n",
      "     41        0.4727  0.0420\n",
      "     42        0.4643  0.0410\n",
      "     43        \u001b[36m0.4526\u001b[0m  0.0420\n",
      "     44        0.4534  0.0410\n",
      "     45        \u001b[36m0.4509\u001b[0m  0.0440\n",
      "     46        0.4705  0.0450\n",
      "     47        0.4627  0.0420\n",
      "     48        0.4534  0.0420\n",
      "     49        0.4697  0.0400\n",
      "     50        0.4718  0.0390\n",
      "     51        0.4747  0.0400\n",
      "     52        0.4652  0.0400\n",
      "     53        0.4730  0.0400\n",
      "     54        0.4598  0.0400\n",
      "     55        0.5414  0.0400\n",
      "     56        0.6549  0.0390\n",
      "     57        0.5508  0.0400\n",
      "     58        0.5327  0.0400\n",
      "     59        0.5156  0.0390\n",
      "     60        0.5071  0.0400\n",
      "     61        0.5091  0.0390\n",
      "     62        0.4801  0.0400\n",
      "     63        0.4901  0.0400\n",
      "     64        0.5001  0.0400\n",
      "     65        0.4831  0.0390\n",
      "     66        0.4773  0.0420\n",
      "     67        0.4738  0.0400\n",
      "     68        0.4707  0.0420\n",
      "     69        0.4686  0.0410\n",
      "     70        0.4669  0.0410\n",
      "     71        0.4655  0.0400\n",
      "     72        0.4639  0.0400\n",
      "     73        0.4628  0.0390\n",
      "     74        0.4618  0.0400\n",
      "     75        0.4609  0.0400\n",
      "     76        0.4600  0.0410\n",
      "     77        0.4592  0.0400\n",
      "     78        0.4585  0.0390\n",
      "     79        0.4577  0.0400\n",
      "     80        0.4567  0.0400\n",
      "     81        0.4561  0.0390\n",
      "     82        0.4554  0.0400\n",
      "     83        0.4548  0.0390\n",
      "     84        0.4542  0.0390\n",
      "     85        0.4632  0.0400\n",
      "     86        0.4547  0.0400\n",
      "     87        0.4589  0.0400\n",
      "     88        0.5295  0.0400\n",
      "     89        0.5160  0.0400\n",
      "     90        0.4612  0.0390\n",
      "     91        0.4547  0.0390\n",
      "     92        0.4679  0.0410\n",
      "     93        0.4544  0.0400\n",
      "     94        0.4564  0.0400\n",
      "     95        0.4576  0.0410\n",
      "     96        0.4645  0.0440\n",
      "     97        0.4549  0.0390\n",
      "     98        \u001b[36m0.4349\u001b[0m  0.0410\n",
      "     99        0.4493  0.0390\n",
      "    100        0.4385  0.0400\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2091\u001b[0m  0.0420\n",
      "      2        \u001b[36m0.8740\u001b[0m  0.0390\n",
      "      3        \u001b[36m0.8018\u001b[0m  0.0400\n",
      "      4        \u001b[36m0.7682\u001b[0m  0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.7212\u001b[0m  0.0400\n",
      "      6        \u001b[36m0.7077\u001b[0m  0.0390\n",
      "      7        \u001b[36m0.6922\u001b[0m  0.0390\n",
      "      8        \u001b[36m0.6795\u001b[0m  0.0400\n",
      "      9        \u001b[36m0.6638\u001b[0m  0.0390\n",
      "     10        0.6652  0.0410\n",
      "     11        \u001b[36m0.6537\u001b[0m  0.0390\n",
      "     12        \u001b[36m0.6439\u001b[0m  0.0400\n",
      "     13        \u001b[36m0.6382\u001b[0m  0.0400\n",
      "     14        \u001b[36m0.6293\u001b[0m  0.0390\n",
      "     15        \u001b[36m0.6224\u001b[0m  0.0400\n",
      "     16        \u001b[36m0.6198\u001b[0m  0.0400\n",
      "     17        \u001b[36m0.6029\u001b[0m  0.0400\n",
      "     18        \u001b[36m0.6002\u001b[0m  0.0400\n",
      "     19        \u001b[36m0.5869\u001b[0m  0.0400\n",
      "     20        \u001b[36m0.5777\u001b[0m  0.0390\n",
      "     21        \u001b[36m0.5705\u001b[0m  0.0390\n",
      "     22        \u001b[36m0.5680\u001b[0m  0.0390\n",
      "     23        \u001b[36m0.5660\u001b[0m  0.0400\n",
      "     24        \u001b[36m0.5643\u001b[0m  0.0400\n",
      "     25        \u001b[36m0.5641\u001b[0m  0.0410\n",
      "     26        \u001b[36m0.5635\u001b[0m  0.0400\n",
      "     27        \u001b[36m0.5619\u001b[0m  0.0410\n",
      "     28        \u001b[36m0.5607\u001b[0m  0.0390\n",
      "     29        \u001b[36m0.5572\u001b[0m  0.0390\n",
      "     30        \u001b[36m0.5561\u001b[0m  0.0400\n",
      "     31        \u001b[36m0.5526\u001b[0m  0.0390\n",
      "     32        \u001b[36m0.5437\u001b[0m  0.0390\n",
      "     33        \u001b[36m0.5432\u001b[0m  0.0390\n",
      "     34        \u001b[36m0.5425\u001b[0m  0.0400\n",
      "     35        \u001b[36m0.5425\u001b[0m  0.0390\n",
      "     36        0.5441  0.0400\n",
      "     37        0.5451  0.0390\n",
      "     38        0.5536  0.0390\n",
      "     39        0.5473  0.0390\n",
      "     40        \u001b[36m0.5350\u001b[0m  0.0400\n",
      "     41        0.5385  0.0420\n",
      "     42        0.5765  0.0400\n",
      "     43        0.5583  0.0400\n",
      "     44        0.5695  0.0400\n",
      "     45        \u001b[36m0.5275\u001b[0m  0.0390\n",
      "     46        \u001b[36m0.5264\u001b[0m  0.0390\n",
      "     47        \u001b[36m0.5222\u001b[0m  0.0400\n",
      "     48        \u001b[36m0.5168\u001b[0m  0.0390\n",
      "     49        0.5237  0.0400\n",
      "     50        0.5211  0.0400\n",
      "     51        0.5192  0.0400\n",
      "     52        0.5188  0.0390\n",
      "     53        0.5208  0.0400\n",
      "     54        0.5200  0.0390\n",
      "     55        0.5195  0.0400\n",
      "     56        0.5170  0.0400\n",
      "     57        0.5244  0.0400\n",
      "     58        0.5235  0.0400\n",
      "     59        0.5229  0.0400\n",
      "     60        0.5228  0.0410\n",
      "     61        0.5231  0.0390\n",
      "     62        0.5225  0.0400\n",
      "     63        0.5224  0.0400\n",
      "     64        0.5218  0.0400\n",
      "     65        0.5212  0.0400\n",
      "     66        0.5223  0.0390\n",
      "     67        0.5282  0.0410\n",
      "     68        0.5262  0.0400\n",
      "     69        0.5247  0.0390\n",
      "     70        0.5222  0.0410\n",
      "     71        0.5209  0.0390\n",
      "     72        0.5198  0.0400\n",
      "     73        \u001b[36m0.5162\u001b[0m  0.0400\n",
      "     74        0.5191  0.0400\n",
      "     75        0.5183  0.0400\n",
      "     76        0.5176  0.0400\n",
      "     77        \u001b[36m0.5161\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.5136\u001b[0m  0.0390\n",
      "     79        0.5186  0.0390\n",
      "     80        0.5172  0.0400\n",
      "     81        0.5161  0.0400\n",
      "     82        0.5152  0.0400\n",
      "     83        \u001b[36m0.5133\u001b[0m  0.0390\n",
      "     84        \u001b[36m0.4976\u001b[0m  0.0400\n",
      "     85        0.5149  0.0410\n",
      "     86        0.5124  0.0400\n",
      "     87        \u001b[36m0.4958\u001b[0m  0.0400\n",
      "     88        0.5217  0.0390\n",
      "     89        0.5208  0.0400\n",
      "     90        0.5164  0.0400\n",
      "     91        0.5077  0.0400\n",
      "     92        0.5047  0.0400\n",
      "     93        0.5090  0.0400\n",
      "     94        0.5069  0.0400\n",
      "     95        0.4995  0.0400\n",
      "     96        0.5051  0.0400\n",
      "     97        0.5018  0.0400\n",
      "     98        0.5015  0.0400\n",
      "     99        0.5041  0.0410\n",
      "    100        0.5008  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2340\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.0040\u001b[0m  0.0300\n",
      "      3        \u001b[36m0.9612\u001b[0m  0.0300\n",
      "      4        \u001b[36m0.9276\u001b[0m  0.0300\n",
      "      5        \u001b[36m0.8987\u001b[0m  0.0300\n",
      "      6        \u001b[36m0.8723\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.8503\u001b[0m  0.0300\n",
      "      8        \u001b[36m0.8318\u001b[0m  0.0300\n",
      "      9        \u001b[36m0.8163\u001b[0m  0.0300\n",
      "     10        \u001b[36m0.8032\u001b[0m  0.0300\n",
      "     11        \u001b[36m0.7921\u001b[0m  0.0300\n",
      "     12        \u001b[36m0.7825\u001b[0m  0.0300\n",
      "     13        \u001b[36m0.7743\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.7671\u001b[0m  0.0310\n",
      "     15        \u001b[36m0.7608\u001b[0m  0.0300\n",
      "     16        \u001b[36m0.7553\u001b[0m  0.0310\n",
      "     17        \u001b[36m0.7503\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.7459\u001b[0m  0.0310\n",
      "     19        \u001b[36m0.7412\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.7375\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.7341\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.7310\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.7279\u001b[0m  0.0310\n",
      "     24        \u001b[36m0.7245\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.7216\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.7193\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.7171\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.7148\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.7129\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.7104\u001b[0m  0.0310\n",
      "     31        \u001b[36m0.7087\u001b[0m  0.0310\n",
      "     32        \u001b[36m0.7070\u001b[0m  0.0300\n",
      "     33        0.7083  0.0300\n",
      "     34        0.7111  0.0300\n",
      "     35        \u001b[36m0.6975\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.6953\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.6939\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.6926\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6914\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.6901\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6889\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6877\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.6866\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.6854\u001b[0m  0.0310\n",
      "     45        \u001b[36m0.6843\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6832\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6821\u001b[0m  0.0330\n",
      "     48        \u001b[36m0.6810\u001b[0m  0.0310\n",
      "     49        \u001b[36m0.6799\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6788\u001b[0m  0.0310\n",
      "     51        \u001b[36m0.6778\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6767\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6757\u001b[0m  0.0310\n",
      "     54        \u001b[36m0.6747\u001b[0m  0.0310\n",
      "     55        \u001b[36m0.6736\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.6726\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.6716\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.6706\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6696\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6686\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6676\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6667\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6657\u001b[0m  0.0310\n",
      "     64        \u001b[36m0.6647\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6638\u001b[0m  0.0310\n",
      "     66        \u001b[36m0.6628\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6619\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     69        \u001b[36m0.6600\u001b[0m  0.0310\n",
      "     70        \u001b[36m0.6591\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.6581\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.6572\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6563\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6554\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.6545\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.6536\u001b[0m  0.0310\n",
      "     77        \u001b[36m0.6527\u001b[0m  0.0310\n",
      "     78        \u001b[36m0.6518\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6509\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.6500\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6492\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.6483\u001b[0m  0.0320\n",
      "     83        \u001b[36m0.6474\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6466\u001b[0m  0.0320\n",
      "     85        \u001b[36m0.6457\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6449\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.6440\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.6432\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.6424\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.6415\u001b[0m  0.0310\n",
      "     91        \u001b[36m0.6407\u001b[0m  0.0300\n",
      "     92        \u001b[36m0.6399\u001b[0m  0.0310\n",
      "     93        \u001b[36m0.6391\u001b[0m  0.0300\n",
      "     94        \u001b[36m0.6382\u001b[0m  0.0300\n",
      "     95        \u001b[36m0.6374\u001b[0m  0.0320\n",
      "     96        \u001b[36m0.6366\u001b[0m  0.0310\n",
      "     97        \u001b[36m0.6358\u001b[0m  0.0300\n",
      "     98        \u001b[36m0.6350\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.6343\u001b[0m  0.0300\n",
      "    100        \u001b[36m0.6335\u001b[0m  0.0310\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.8450\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.6945\u001b[0m  0.0300\n",
      "      3        \u001b[36m1.5564\u001b[0m  0.0310\n",
      "      4        \u001b[36m1.4919\u001b[0m  0.0300\n",
      "      5        \u001b[36m1.4493\u001b[0m  0.0300\n",
      "      6        \u001b[36m1.4131\u001b[0m  0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.3828\u001b[0m  0.0300\n",
      "      8        \u001b[36m1.3010\u001b[0m  0.0300\n",
      "      9        \u001b[36m1.2742\u001b[0m  0.0300\n",
      "     10        \u001b[36m1.2561\u001b[0m  0.0300\n",
      "     11        \u001b[36m1.2402\u001b[0m  0.0310\n",
      "     12        \u001b[36m1.2260\u001b[0m  0.0300\n",
      "     13        \u001b[36m1.2130\u001b[0m  0.0300\n",
      "     14        \u001b[36m1.2008\u001b[0m  0.0300\n",
      "     15        \u001b[36m1.1893\u001b[0m  0.0310\n",
      "     16        \u001b[36m1.1783\u001b[0m  0.0310\n",
      "     17        \u001b[36m1.1678\u001b[0m  0.0310\n",
      "     18        \u001b[36m1.1575\u001b[0m  0.0310\n",
      "     19        \u001b[36m1.1476\u001b[0m  0.0300\n",
      "     20        \u001b[36m1.1380\u001b[0m  0.0300\n",
      "     21        \u001b[36m1.1285\u001b[0m  0.0310\n",
      "     22        \u001b[36m1.0647\u001b[0m  0.0300\n",
      "     23        \u001b[36m1.0546\u001b[0m  0.0300\n",
      "     24        \u001b[36m1.0451\u001b[0m  0.0310\n",
      "     25        \u001b[36m1.0359\u001b[0m  0.0300\n",
      "     26        \u001b[36m1.0271\u001b[0m  0.0300\n",
      "     27        \u001b[36m1.0186\u001b[0m  0.0320\n",
      "     28        \u001b[36m1.0103\u001b[0m  0.0300\n",
      "     29        \u001b[36m1.0024\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.9947\u001b[0m  0.0300\n",
      "     31        \u001b[36m0.9873\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.9802\u001b[0m  0.0310\n",
      "     33        \u001b[36m0.9732\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.9656\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.9592\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.9530\u001b[0m  0.0310\n",
      "     37        \u001b[36m0.9464\u001b[0m  0.0300\n",
      "     38        \u001b[36m0.9345\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.9289\u001b[0m  0.0310\n",
      "     40        \u001b[36m0.9234\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.9181\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.9130\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.9080\u001b[0m  0.0300\n",
      "     44        \u001b[36m0.9031\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.8983\u001b[0m  0.0290\n",
      "     46        \u001b[36m0.8936\u001b[0m  0.0320\n",
      "     47        \u001b[36m0.8891\u001b[0m  0.0300\n",
      "     48        \u001b[36m0.8846\u001b[0m  0.0310\n",
      "     49        \u001b[36m0.8803\u001b[0m  0.0310\n",
      "     50        \u001b[36m0.8760\u001b[0m  0.0310\n",
      "     51        \u001b[36m0.8719\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.8678\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.8638\u001b[0m  0.0310\n",
      "     54        \u001b[36m0.8599\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.8561\u001b[0m  0.0310\n",
      "     56        \u001b[36m0.8523\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.8487\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.8450\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.8415\u001b[0m  0.0310\n",
      "     60        \u001b[36m0.8380\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.8346\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.8312\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.8279\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.8247\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.8215\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.8184\u001b[0m  0.0310\n",
      "     67        \u001b[36m0.8153\u001b[0m  0.0300\n",
      "     68        \u001b[36m0.8122\u001b[0m  0.0310\n",
      "     69        \u001b[36m0.8093\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.8063\u001b[0m  0.0300\n",
      "     71        \u001b[36m0.8034\u001b[0m  0.0310\n",
      "     72        \u001b[36m0.8006\u001b[0m  0.0310\n",
      "     73        \u001b[36m0.7978\u001b[0m  0.0310\n",
      "     74        \u001b[36m0.7950\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.7923\u001b[0m  0.0320\n",
      "     76        \u001b[36m0.7896\u001b[0m  0.0310\n",
      "     77        \u001b[36m0.7870\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.7844\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.7819\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.7793\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.7769\u001b[0m  0.0310\n",
      "     82        \u001b[36m0.7744\u001b[0m  0.0350\n",
      "     83        \u001b[36m0.7720\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.7696\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.7673\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.7650\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.7627\u001b[0m  0.0300\n",
      "     88        \u001b[36m0.7605\u001b[0m  0.0300\n",
      "     89        \u001b[36m0.7583\u001b[0m  0.0310\n",
      "     90        \u001b[36m0.7561\u001b[0m  0.0300\n",
      "     91        \u001b[36m0.7540\u001b[0m  0.0310\n",
      "     92        \u001b[36m0.7518\u001b[0m  0.0300\n",
      "     93        \u001b[36m0.7497\u001b[0m  0.0300\n",
      "     94        \u001b[36m0.7477\u001b[0m  0.0310\n",
      "     95        \u001b[36m0.7457\u001b[0m  0.0300\n",
      "     96        \u001b[36m0.7437\u001b[0m  0.0300\n",
      "     97        \u001b[36m0.7417\u001b[0m  0.0300\n",
      "     98        \u001b[36m0.7397\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.7378\u001b[0m  0.0310\n",
      "    100        \u001b[36m0.7359\u001b[0m  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.5942\u001b[0m  0.0300\n",
      "      2        1.6291  0.0310\n",
      "      3        \u001b[36m1.5029\u001b[0m  0.0310\n",
      "      4        \u001b[36m1.4087\u001b[0m  0.0300\n",
      "      5        \u001b[36m1.3397\u001b[0m  0.0300\n",
      "      6        \u001b[36m1.2885\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m1.2496\u001b[0m  0.0300\n",
      "      8        \u001b[36m1.2186\u001b[0m  0.0310\n",
      "      9        \u001b[36m1.1926\u001b[0m  0.0290\n",
      "     10        \u001b[36m1.1696\u001b[0m  0.0300\n",
      "     11        \u001b[36m1.1484\u001b[0m  0.0310\n",
      "     12        \u001b[36m1.1282\u001b[0m  0.0300\n",
      "     13        \u001b[36m1.1085\u001b[0m  0.0310\n",
      "     14        \u001b[36m1.0891\u001b[0m  0.0310\n",
      "     15        \u001b[36m1.0698\u001b[0m  0.0300\n",
      "     16        \u001b[36m1.0509\u001b[0m  0.0300\n",
      "     17        \u001b[36m1.0322\u001b[0m  0.0300\n",
      "     18        \u001b[36m1.0141\u001b[0m  0.0300\n",
      "     19        \u001b[36m0.9963\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.9791\u001b[0m  0.0310\n",
      "     21        \u001b[36m0.9623\u001b[0m  0.0290\n",
      "     22        \u001b[36m0.9458\u001b[0m  0.0310\n",
      "     23        \u001b[36m0.9294\u001b[0m  0.0310\n",
      "     24        \u001b[36m0.9132\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.8970\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.8806\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.8643\u001b[0m  0.0300\n",
      "     28        \u001b[36m0.8480\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.8322\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.8172\u001b[0m  0.0310\n",
      "     31        \u001b[36m0.8032\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.7906\u001b[0m  0.0310\n",
      "     33        \u001b[36m0.7793\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.7694\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.7607\u001b[0m  0.0310\n",
      "     36        \u001b[36m0.7530\u001b[0m  0.0300\n",
      "     37        \u001b[36m0.7463\u001b[0m  0.0320\n",
      "     38        \u001b[36m0.7403\u001b[0m  0.0310\n",
      "     39        \u001b[36m0.7351\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.7303\u001b[0m  0.0310\n",
      "     41        \u001b[36m0.7261\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.7222\u001b[0m  0.0310\n",
      "     43        \u001b[36m0.7187\u001b[0m  0.0310\n",
      "     44        \u001b[36m0.7154\u001b[0m  0.0310\n",
      "     45        \u001b[36m0.7124\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.7097\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6917\u001b[0m  0.0350\n",
      "     48        \u001b[36m0.6889\u001b[0m  0.0300\n",
      "     49        \u001b[36m0.6865\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6843\u001b[0m  0.0310\n",
      "     51        \u001b[36m0.6822\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6803\u001b[0m  0.0310\n",
      "     53        \u001b[36m0.6784\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6766\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6749\u001b[0m  0.0310\n",
      "     56        \u001b[36m0.6732\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.6716\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.6701\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6686\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6671\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6657\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6643\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6630\u001b[0m  0.0310\n",
      "     64        \u001b[36m0.6617\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6604\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6591\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6579\u001b[0m  0.0310\n",
      "     68        \u001b[36m0.6567\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.6555\u001b[0m  0.0300\n",
      "     70        \u001b[36m0.6544\u001b[0m  0.0310\n",
      "     71        \u001b[36m0.6533\u001b[0m  0.0300\n",
      "     72        \u001b[36m0.6522\u001b[0m  0.0310\n",
      "     73        \u001b[36m0.6511\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.6500\u001b[0m  0.0310\n",
      "     75        \u001b[36m0.6490\u001b[0m  0.0310\n",
      "     76        \u001b[36m0.6480\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6470\u001b[0m  0.0320\n",
      "     78        \u001b[36m0.6460\u001b[0m  0.0310\n",
      "     79        \u001b[36m0.6450\u001b[0m  0.0310\n",
      "     80        \u001b[36m0.6441\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6431\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.6422\u001b[0m  0.0300\n",
      "     83        \u001b[36m0.6413\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6405\u001b[0m  0.0310\n",
      "     85        \u001b[36m0.6396\u001b[0m  0.0310\n",
      "     86        \u001b[36m0.6388\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.6379\u001b[0m  0.0310\n",
      "     88        \u001b[36m0.6371\u001b[0m  0.0310\n",
      "     89        \u001b[36m0.6363\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.6355\u001b[0m  0.0300\n",
      "     91        \u001b[36m0.6347\u001b[0m  0.0310\n",
      "     92        \u001b[36m0.6340\u001b[0m  0.0300\n",
      "     93        \u001b[36m0.6332\u001b[0m  0.0300\n",
      "     94        \u001b[36m0.6325\u001b[0m  0.0310\n",
      "     95        \u001b[36m0.6318\u001b[0m  0.0410\n",
      "     96        \u001b[36m0.6311\u001b[0m  0.0300\n",
      "     97        \u001b[36m0.6304\u001b[0m  0.0310\n",
      "     98        \u001b[36m0.6297\u001b[0m  0.0300\n",
      "     99        \u001b[36m0.6290\u001b[0m  0.0300\n",
      "    100        \u001b[36m0.6284\u001b[0m  0.0300\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.2185\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.1054\u001b[0m  0.0300\n",
      "      3        \u001b[36m1.0109\u001b[0m  0.0300\n",
      "      4        \u001b[36m0.9514\u001b[0m  0.0310\n",
      "      5        \u001b[36m0.9131\u001b[0m  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.8886\u001b[0m  0.0310\n",
      "      7        \u001b[36m0.8721\u001b[0m  0.0310\n",
      "      8        \u001b[36m0.8600\u001b[0m  0.0300\n",
      "      9        \u001b[36m0.8504\u001b[0m  0.0310\n",
      "     10        \u001b[36m0.8423\u001b[0m  0.0300\n",
      "     11        \u001b[36m0.8350\u001b[0m  0.0300\n",
      "     12        \u001b[36m0.8283\u001b[0m  0.0300\n",
      "     13        \u001b[36m0.8219\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.8159\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.8100\u001b[0m  0.0310\n",
      "     16        \u001b[36m0.8043\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.7988\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.7934\u001b[0m  0.0310\n",
      "     19        \u001b[36m0.7881\u001b[0m  0.0310\n",
      "     20        \u001b[36m0.7818\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.7624\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.7264\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.7245\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.7227\u001b[0m  0.0300\n",
      "     25        \u001b[36m0.7210\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.7193\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.7177\u001b[0m  0.0310\n",
      "     28        \u001b[36m0.7162\u001b[0m  0.0310\n",
      "     29        \u001b[36m0.7147\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.7132\u001b[0m  0.0310\n",
      "     31        \u001b[36m0.7118\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.7104\u001b[0m  0.0300\n",
      "     33        \u001b[36m0.7090\u001b[0m  0.0300\n",
      "     34        \u001b[36m0.7077\u001b[0m  0.0300\n",
      "     35        \u001b[36m0.7064\u001b[0m  0.0300\n",
      "     36        \u001b[36m0.7051\u001b[0m  0.0310\n",
      "     37        \u001b[36m0.7039\u001b[0m  0.0310\n",
      "     38        \u001b[36m0.7027\u001b[0m  0.0310\n",
      "     39        \u001b[36m0.7015\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.7004\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6992\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6982\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.6971\u001b[0m  0.0310\n",
      "     44        \u001b[36m0.6960\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6950\u001b[0m  0.0300\n",
      "     46        \u001b[36m0.6940\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6930\u001b[0m  0.0310\n",
      "     48        \u001b[36m0.6921\u001b[0m  0.0310\n",
      "     49        \u001b[36m0.6912\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6902\u001b[0m  0.0310\n",
      "     51        \u001b[36m0.6893\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6885\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6876\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.6868\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.6859\u001b[0m  0.0290\n",
      "     56        \u001b[36m0.6851\u001b[0m  0.0310\n",
      "     57        \u001b[36m0.6843\u001b[0m  0.0310\n",
      "     58        \u001b[36m0.6835\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.6828\u001b[0m  0.0300\n",
      "     60        \u001b[36m0.6820\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.6813\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.6806\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.6798\u001b[0m  0.0300\n",
      "     64        \u001b[36m0.6791\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.6785\u001b[0m  0.0300\n",
      "     66        \u001b[36m0.6778\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.6771\u001b[0m  0.0320\n",
      "     68        \u001b[36m0.6765\u001b[0m  0.0310\n",
      "     69        \u001b[36m0.6758\u001b[0m  0.0330\n",
      "     70        \u001b[36m0.6752\u001b[0m  0.0310\n",
      "     71        \u001b[36m0.6745\u001b[0m  0.0310\n",
      "     72        \u001b[36m0.6739\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.6733\u001b[0m  0.0320\n",
      "     74        \u001b[36m0.6727\u001b[0m  0.0300\n",
      "     75        \u001b[36m0.6721\u001b[0m  0.0310\n",
      "     76        \u001b[36m0.6716\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.6710\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.6704\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.6699\u001b[0m  0.0300\n",
      "     80        \u001b[36m0.6693\u001b[0m  0.0300\n",
      "     81        \u001b[36m0.6688\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.6682\u001b[0m  0.0310\n",
      "     83        \u001b[36m0.6677\u001b[0m  0.0300\n",
      "     84        \u001b[36m0.6672\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.6667\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.6662\u001b[0m  0.0310\n",
      "     87        \u001b[36m0.6657\u001b[0m  0.0310\n",
      "     88        \u001b[36m0.6652\u001b[0m  0.0310\n",
      "     89        \u001b[36m0.6647\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.6642\u001b[0m  0.0300\n",
      "     91        \u001b[36m0.6637\u001b[0m  0.0300\n",
      "     92        \u001b[36m0.6633\u001b[0m  0.0310\n",
      "     93        \u001b[36m0.6628\u001b[0m  0.0310\n",
      "     94        \u001b[36m0.6624\u001b[0m  0.0300\n",
      "     95        \u001b[36m0.6619\u001b[0m  0.0310\n",
      "     96        \u001b[36m0.6614\u001b[0m  0.0300\n",
      "     97        \u001b[36m0.6610\u001b[0m  0.0300\n",
      "     98        \u001b[36m0.6606\u001b[0m  0.0310\n",
      "     99        \u001b[36m0.6601\u001b[0m  0.0310\n",
      "    100        \u001b[36m0.6597\u001b[0m  0.0310\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.3158\u001b[0m  0.0300\n",
      "      2        \u001b[36m1.1627\u001b[0m  0.0300\n",
      "      3        \u001b[36m1.0207\u001b[0m  0.0320\n",
      "      4        \u001b[36m0.9443\u001b[0m  0.0310\n",
      "      5        \u001b[36m0.8961\u001b[0m  0.0310\n",
      "      6        0.9346  0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.8715\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.8397\u001b[0m  0.0310\n",
      "      9        \u001b[36m0.8114\u001b[0m  0.0300\n",
      "     10        \u001b[36m0.7870\u001b[0m  0.0300\n",
      "     11        \u001b[36m0.7663\u001b[0m  0.0310\n",
      "     12        \u001b[36m0.7491\u001b[0m  0.0300\n",
      "     13        \u001b[36m0.7347\u001b[0m  0.0300\n",
      "     14        \u001b[36m0.7226\u001b[0m  0.0300\n",
      "     15        \u001b[36m0.7124\u001b[0m  0.0310\n",
      "     16        \u001b[36m0.7036\u001b[0m  0.0300\n",
      "     17        \u001b[36m0.6960\u001b[0m  0.0300\n",
      "     18        \u001b[36m0.6893\u001b[0m  0.0310\n",
      "     19        \u001b[36m0.6834\u001b[0m  0.0300\n",
      "     20        \u001b[36m0.6780\u001b[0m  0.0300\n",
      "     21        \u001b[36m0.6732\u001b[0m  0.0300\n",
      "     22        \u001b[36m0.6687\u001b[0m  0.0300\n",
      "     23        \u001b[36m0.6646\u001b[0m  0.0300\n",
      "     24        \u001b[36m0.6609\u001b[0m  0.0310\n",
      "     25        \u001b[36m0.6573\u001b[0m  0.0300\n",
      "     26        \u001b[36m0.6540\u001b[0m  0.0300\n",
      "     27        \u001b[36m0.6509\u001b[0m  0.0310\n",
      "     28        \u001b[36m0.6479\u001b[0m  0.0300\n",
      "     29        \u001b[36m0.6451\u001b[0m  0.0300\n",
      "     30        \u001b[36m0.6425\u001b[0m  0.0310\n",
      "     31        \u001b[36m0.6399\u001b[0m  0.0300\n",
      "     32        \u001b[36m0.6353\u001b[0m  0.0300\n",
      "     33        \u001b[36m0.6330\u001b[0m  0.0310\n",
      "     34        \u001b[36m0.6308\u001b[0m  0.0310\n",
      "     35        \u001b[36m0.6286\u001b[0m  0.0310\n",
      "     36        \u001b[36m0.6266\u001b[0m  0.0310\n",
      "     37        \u001b[36m0.6246\u001b[0m  0.0310\n",
      "     38        \u001b[36m0.6226\u001b[0m  0.0300\n",
      "     39        \u001b[36m0.6208\u001b[0m  0.0300\n",
      "     40        \u001b[36m0.6190\u001b[0m  0.0300\n",
      "     41        \u001b[36m0.6173\u001b[0m  0.0300\n",
      "     42        \u001b[36m0.6156\u001b[0m  0.0300\n",
      "     43        \u001b[36m0.6139\u001b[0m  0.0310\n",
      "     44        \u001b[36m0.6124\u001b[0m  0.0300\n",
      "     45        \u001b[36m0.6108\u001b[0m  0.0290\n",
      "     46        \u001b[36m0.6093\u001b[0m  0.0300\n",
      "     47        \u001b[36m0.6079\u001b[0m  0.0310\n",
      "     48        \u001b[36m0.6065\u001b[0m  0.0290\n",
      "     49        \u001b[36m0.6051\u001b[0m  0.0300\n",
      "     50        \u001b[36m0.6038\u001b[0m  0.0300\n",
      "     51        \u001b[36m0.6025\u001b[0m  0.0300\n",
      "     52        \u001b[36m0.6012\u001b[0m  0.0300\n",
      "     53        \u001b[36m0.6000\u001b[0m  0.0300\n",
      "     54        \u001b[36m0.5988\u001b[0m  0.0300\n",
      "     55        \u001b[36m0.5976\u001b[0m  0.0300\n",
      "     56        \u001b[36m0.5965\u001b[0m  0.0300\n",
      "     57        \u001b[36m0.5953\u001b[0m  0.0300\n",
      "     58        \u001b[36m0.5943\u001b[0m  0.0300\n",
      "     59        \u001b[36m0.5932\u001b[0m  0.0310\n",
      "     60        \u001b[36m0.5922\u001b[0m  0.0300\n",
      "     61        \u001b[36m0.5911\u001b[0m  0.0300\n",
      "     62        \u001b[36m0.5902\u001b[0m  0.0300\n",
      "     63        \u001b[36m0.5892\u001b[0m  0.0310\n",
      "     64        \u001b[36m0.5883\u001b[0m  0.0300\n",
      "     65        \u001b[36m0.5873\u001b[0m  0.0310\n",
      "     66        \u001b[36m0.5864\u001b[0m  0.0300\n",
      "     67        \u001b[36m0.5855\u001b[0m  0.0320\n",
      "     68        \u001b[36m0.5847\u001b[0m  0.0300\n",
      "     69        \u001b[36m0.5838\u001b[0m  0.0310\n",
      "     70        \u001b[36m0.5830\u001b[0m  0.0310\n",
      "     71        \u001b[36m0.5822\u001b[0m  0.0310\n",
      "     72        \u001b[36m0.5814\u001b[0m  0.0300\n",
      "     73        \u001b[36m0.5806\u001b[0m  0.0300\n",
      "     74        \u001b[36m0.5799\u001b[0m  0.0310\n",
      "     75        \u001b[36m0.5791\u001b[0m  0.0300\n",
      "     76        \u001b[36m0.5784\u001b[0m  0.0300\n",
      "     77        \u001b[36m0.5777\u001b[0m  0.0300\n",
      "     78        \u001b[36m0.5770\u001b[0m  0.0300\n",
      "     79        \u001b[36m0.5763\u001b[0m  0.0310\n",
      "     80        \u001b[36m0.5756\u001b[0m  0.0310\n",
      "     81        \u001b[36m0.5749\u001b[0m  0.0300\n",
      "     82        \u001b[36m0.5743\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.5736\u001b[0m  0.0290\n",
      "     84        \u001b[36m0.5730\u001b[0m  0.0300\n",
      "     85        \u001b[36m0.5724\u001b[0m  0.0300\n",
      "     86        \u001b[36m0.5718\u001b[0m  0.0300\n",
      "     87        \u001b[36m0.5712\u001b[0m  0.0310\n",
      "     88        \u001b[36m0.5706\u001b[0m  0.0310\n",
      "     89        \u001b[36m0.5700\u001b[0m  0.0300\n",
      "     90        \u001b[36m0.5695\u001b[0m  0.0310\n",
      "     91        \u001b[36m0.5689\u001b[0m  0.0300\n",
      "     92        \u001b[36m0.5684\u001b[0m  0.0300\n",
      "     93        \u001b[36m0.5678\u001b[0m  0.0300\n",
      "     94        \u001b[36m0.5673\u001b[0m  0.0300\n",
      "     95        \u001b[36m0.5668\u001b[0m  0.0300\n",
      "     96        \u001b[36m0.5663\u001b[0m  0.0300\n",
      "     97        \u001b[36m0.5658\u001b[0m  0.0310\n",
      "     98        \u001b[36m0.5653\u001b[0m  0.0310\n",
      "     99        \u001b[36m0.5648\u001b[0m  0.0310\n",
      "    100        \u001b[36m0.5643\u001b[0m  0.0310\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0570\n",
      "      2        1.0000  0.0430\n",
      "      3        1.0000  0.0440\n",
      "      4        1.0000  0.0430\n",
      "      5        1.0000  0.0440\n",
      "      6        1.0000  0.0430\n",
      "      7        1.0000  0.0440\n",
      "      8        1.0000  0.0450\n",
      "      9        1.0000  0.0430\n",
      "     10        1.0000  0.0430\n",
      "     11        1.0000  0.0430\n",
      "     12        1.0000  0.0430\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0430\n",
      "     15        1.0000  0.0440\n",
      "     16        1.0000  0.0430\n",
      "     17        1.0000  0.0430\n",
      "     18        1.0000  0.0420\n",
      "     19        1.0000  0.0470\n",
      "     20        1.0000  0.0440\n",
      "     21        1.0000  0.0430\n",
      "     22        1.0000  0.0440\n",
      "     23        1.0000  0.0450\n",
      "     24        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.4631\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3714\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3714\u001b[0m  0.0430\n",
      "     28        0.3714  0.0440\n",
      "     29        0.3714  0.0440\n",
      "     30        0.3714  0.0430\n",
      "     31        0.3714  0.0440\n",
      "     32        0.3714  0.0430\n",
      "     33        0.3714  0.0440\n",
      "     34        0.3714  0.0430\n",
      "     35        0.3714  0.0430\n",
      "     36        0.3714  0.0430\n",
      "     37        0.3714  0.0440\n",
      "     38        0.3714  0.0440\n",
      "     39        0.3714  0.0430\n",
      "     40        0.3714  0.0430\n",
      "     41        0.3714  0.0430\n",
      "     42        0.3714  0.0440\n",
      "     43        0.3714  0.0440\n",
      "     44        0.3714  0.0440\n",
      "     45        0.3714  0.0440\n",
      "     46        0.3714  0.0440\n",
      "     47        0.3714  0.0430\n",
      "     48        0.3714  0.0430\n",
      "     49        0.3714  0.0440\n",
      "     50        0.3714  0.0450\n",
      "     51        0.3714  0.0450\n",
      "     52        0.3714  0.0420\n",
      "     53        0.3714  0.0440\n",
      "     54        0.3714  0.0430\n",
      "     55        0.3714  0.0440\n",
      "     56        0.3714  0.0480\n",
      "     57        0.3714  0.0430\n",
      "     58        0.3714  0.0430\n",
      "     59        0.3714  0.0430\n",
      "     60        0.3714  0.0430\n",
      "     61        0.3714  0.0430\n",
      "     62        0.3714  0.0440\n",
      "     63        0.3714  0.0430\n",
      "     64        0.3714  0.0430\n",
      "     65        0.3714  0.0450\n",
      "     66        0.3714  0.0430\n",
      "     67        0.3714  0.0440\n",
      "     68        0.3714  0.0430\n",
      "     69        0.3714  0.0440\n",
      "     70        0.3714  0.0440\n",
      "     71        0.3714  0.0430\n",
      "     72        0.3714  0.0430\n",
      "     73        0.3714  0.0430\n",
      "     74        0.3714  0.0440\n",
      "     75        0.3714  0.0440\n",
      "     76        0.3714  0.0440\n",
      "     77        0.3714  0.0430\n",
      "     78        0.3714  0.0440\n",
      "     79        0.3714  0.0450\n",
      "     80        0.3714  0.0440\n",
      "     81        0.3714  0.0440\n",
      "     82        0.3714  0.0430\n",
      "     83        0.3714  0.0440\n",
      "     84        0.3714  0.0430\n",
      "     85        0.3714  0.0430\n",
      "     86        0.3714  0.0430\n",
      "     87        0.3714  0.0450\n",
      "     88        0.3714  0.0440\n",
      "     89        0.3714  0.0430\n",
      "     90        0.3714  0.0440\n",
      "     91        0.3714  0.0470\n",
      "     92        0.3714  0.0480\n",
      "     93        0.3714  0.0430\n",
      "     94        0.3714  0.0420\n",
      "     95        0.3714  0.0440\n",
      "     96        0.3714  0.0430\n",
      "     97        0.3714  0.0420\n",
      "     98        0.3714  0.0440\n",
      "     99        0.3714  0.0430\n",
      "    100        0.3714  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "      2        1.0000  0.0430\n",
      "      3        1.0000  0.0430\n",
      "      4        1.0000  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        1.0000  0.0440\n",
      "      6        1.0000  0.0430\n",
      "      7        1.0000  0.0430\n",
      "      8        1.0000  0.0430\n",
      "      9        1.0000  0.0450\n",
      "     10        1.0000  0.0430\n",
      "     11        1.0000  0.0440\n",
      "     12        1.0000  0.0440\n",
      "     13        1.0000  0.0440\n",
      "     14        1.0000  0.0440\n",
      "     15        1.0000  0.0430\n",
      "     16        1.0000  0.0430\n",
      "     17        1.0000  0.0430\n",
      "     18        1.0000  0.0430\n",
      "     19        1.0000  0.0430\n",
      "     20        1.0000  0.0430\n",
      "     21        1.0000  0.0440\n",
      "     22        1.0000  0.0430\n",
      "     23        \u001b[36m0.9366\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3714\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3714\u001b[0m  0.0420\n",
      "     26        0.3714  0.0440\n",
      "     27        0.3714  0.0430\n",
      "     28        0.3714  0.0440\n",
      "     29        0.3714  0.0430\n",
      "     30        0.3714  0.0440\n",
      "     31        0.3714  0.0440\n",
      "     32        0.3714  0.0430\n",
      "     33        0.3714  0.0430\n",
      "     34        0.3714  0.0430\n",
      "     35        0.3714  0.0440\n",
      "     36        0.3714  0.0440\n",
      "     37        0.3714  0.0440\n",
      "     38        0.3714  0.0430\n",
      "     39        0.3714  0.0430\n",
      "     40        0.3714  0.0430\n",
      "     41        0.3714  0.0430\n",
      "     42        0.3714  0.0430\n",
      "     43        0.3714  0.0430\n",
      "     44        0.3714  0.0440\n",
      "     45        0.3714  0.0420\n",
      "     46        0.3714  0.0430\n",
      "     47        0.3714  0.0430\n",
      "     48        0.3714  0.0430\n",
      "     49        0.3714  0.0430\n",
      "     50        0.3714  0.0440\n",
      "     51        0.3714  0.0430\n",
      "     52        0.3714  0.0430\n",
      "     53        0.3714  0.0430\n",
      "     54        0.3714  0.0430\n",
      "     55        0.3714  0.0430\n",
      "     56        0.3714  0.0430\n",
      "     57        0.3714  0.0430\n",
      "     58        0.3714  0.0430\n",
      "     59        0.3714  0.0430\n",
      "     60        0.3714  0.0440\n",
      "     61        0.3714  0.0430\n",
      "     62        0.3714  0.0440\n",
      "     63        0.3714  0.0420\n",
      "     64        0.3714  0.0440\n",
      "     65        0.3714  0.0430\n",
      "     66        0.3714  0.0440\n",
      "     67        0.3714  0.0440\n",
      "     68        0.3714  0.0440\n",
      "     69        0.3714  0.0430\n",
      "     70        0.3714  0.0430\n",
      "     71        0.3714  0.0430\n",
      "     72        0.3714  0.0430\n",
      "     73        0.3714  0.0430\n",
      "     74        0.3714  0.0430\n",
      "     75        0.3714  0.0430\n",
      "     76        0.3714  0.0430\n",
      "     77        0.3714  0.0440\n",
      "     78        0.3714  0.0440\n",
      "     79        0.3714  0.0440\n",
      "     80        0.3714  0.0440\n",
      "     81        0.3714  0.0440\n",
      "     82        0.3714  0.0430\n",
      "     83        0.3714  0.0430\n",
      "     84        0.3714  0.0430\n",
      "     85        0.3714  0.0440\n",
      "     86        0.3714  0.0440\n",
      "     87        0.3714  0.0440\n",
      "     88        0.3714  0.0430\n",
      "     89        0.3714  0.0430\n",
      "     90        0.3714  0.0440\n",
      "     91        0.3714  0.0440\n",
      "     92        0.3714  0.0430\n",
      "     93        0.3714  0.0430\n",
      "     94        0.3714  0.0430\n",
      "     95        0.3714  0.0440\n",
      "     96        0.3714  0.0440\n",
      "     97        0.3714  0.0430\n",
      "     98        0.3714  0.0440\n",
      "     99        0.3714  0.0430\n",
      "    100        0.3714  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "      2        1.0000  0.0430\n",
      "      3        1.0000  0.0440\n",
      "      4        1.0000  0.0430"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      5        1.0000  0.0430\n",
      "      6        1.0000  0.0430\n",
      "      7        1.0000  0.0440\n",
      "      8        1.0000  0.0430\n",
      "      9        1.0000  0.0430\n",
      "     10        1.0000  0.0430\n",
      "     11        1.0000  0.0430\n",
      "     12        1.0000  0.0430\n",
      "     13        1.0000  0.0440\n",
      "     14        1.0000  0.0430\n",
      "     15        1.0000  0.0430\n",
      "     16        1.0000  0.0420\n",
      "     17        1.0000  0.0430\n",
      "     18        \u001b[36m1.0000\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.5000\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     21        0.3736  0.0440\n",
      "     22        0.3736  0.0430\n",
      "     23        0.3736  0.0440\n",
      "     24        0.3736  0.0420\n",
      "     25        0.3736  0.0440\n",
      "     26        0.3736  0.0440\n",
      "     27        0.3736  0.0430\n",
      "     28        0.3736  0.0430\n",
      "     29        0.3736  0.0440\n",
      "     30        0.3736  0.0440\n",
      "     31        0.3736  0.0430\n",
      "     32        0.3736  0.0430\n",
      "     33        0.3736  0.0440\n",
      "     34        0.3736  0.0430\n",
      "     35        0.3736  0.0440\n",
      "     36        0.3736  0.0430\n",
      "     37        0.3736  0.0430\n",
      "     38        0.3736  0.0430\n",
      "     39        0.3736  0.0430\n",
      "     40        0.3736  0.0440\n",
      "     41        0.3736  0.0420\n",
      "     42        0.3736  0.0430\n",
      "     43        0.3736  0.0440\n",
      "     44        0.3736  0.0440\n",
      "     45        0.3736  0.0430\n",
      "     46        0.3736  0.0430\n",
      "     47        0.3736  0.0430\n",
      "     48        0.3736  0.0430\n",
      "     49        0.3736  0.0440\n",
      "     50        0.3736  0.0430\n",
      "     51        0.3736  0.0440\n",
      "     52        0.3736  0.0440\n",
      "     53        0.3736  0.0430\n",
      "     54        0.3736  0.0430\n",
      "     55        0.3736  0.0430\n",
      "     56        0.3736  0.0440\n",
      "     57        0.3736  0.0430\n",
      "     58        0.3736  0.0440\n",
      "     59        0.3736  0.0490\n",
      "     60        0.3736  0.0440\n",
      "     61        0.3736  0.0430\n",
      "     62        0.3736  0.0430\n",
      "     63        0.3736  0.0430\n",
      "     64        0.3736  0.0440\n",
      "     65        0.3736  0.0440\n",
      "     66        0.3736  0.0430\n",
      "     67        0.3736  0.0450\n",
      "     68        0.3736  0.0430\n",
      "     69        0.3736  0.0440\n",
      "     70        0.3736  0.0440\n",
      "     71        0.3736  0.0430\n",
      "     72        0.3736  0.0450\n",
      "     73        0.3736  0.0460\n",
      "     74        0.3736  0.0430\n",
      "     75        0.3736  0.0440\n",
      "     76        0.3736  0.0440\n",
      "     77        0.3736  0.0430\n",
      "     78        0.3736  0.0440\n",
      "     79        0.3736  0.0430\n",
      "     80        0.3736  0.0440\n",
      "     81        0.3736  0.0440\n",
      "     82        0.3736  0.0430\n",
      "     83        0.3736  0.0430\n",
      "     84        0.3736  0.0440\n",
      "     85        0.3736  0.0440\n",
      "     86        0.3736  0.0440\n",
      "     87        0.3736  0.0440\n",
      "     88        0.3736  0.0440\n",
      "     89        0.3736  0.0440\n",
      "     90        0.3736  0.0430\n",
      "     91        0.3736  0.0440\n",
      "     92        0.3736  0.0440\n",
      "     93        0.3736  0.0440\n",
      "     94        0.3736  0.0440\n",
      "     95        0.3736  0.0440\n",
      "     96        0.3736  0.0440\n",
      "     97        0.3736  0.0440\n",
      "     98        0.3736  0.0440\n",
      "     99        0.3736  0.0440\n",
      "    100        0.3736  0.0450\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "      2        1.0000  0.0430\n",
      "      3        1.0000  0.0450\n",
      "      4        1.0000  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        1.0000  0.0430\n",
      "      6        1.0000  0.0440\n",
      "      7        1.0000  0.0430\n",
      "      8        1.0000  0.0430\n",
      "      9        1.0000  0.0430\n",
      "     10        1.0000  0.0430\n",
      "     11        1.0000  0.0430\n",
      "     12        1.0000  0.0450\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0440\n",
      "     15        1.0000  0.0440\n",
      "     16        1.0000  0.0430\n",
      "     17        1.0000  0.0440\n",
      "     18        1.0000  0.0430\n",
      "     19        1.0000  0.0440\n",
      "     20        1.0000  0.0430\n",
      "     21        1.0000  0.0440\n",
      "     22        1.0000  0.0430\n",
      "     23        1.0000  0.0430\n",
      "     24        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.8554\u001b[0m  0.0440\n",
      "     26        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     28        0.3736  0.0430\n",
      "     29        0.3736  0.0440\n",
      "     30        0.3736  0.0440\n",
      "     31        0.3736  0.0430\n",
      "     32        0.3736  0.0430\n",
      "     33        0.3736  0.0430\n",
      "     34        0.3736  0.0440\n",
      "     35        0.3736  0.0430\n",
      "     36        0.3736  0.0430\n",
      "     37        0.3736  0.0440\n",
      "     38        0.3736  0.0440\n",
      "     39        0.3736  0.0430\n",
      "     40        0.3736  0.0430\n",
      "     41        0.3736  0.0450\n",
      "     42        0.3736  0.0430\n",
      "     43        0.3736  0.0430\n",
      "     44        0.3736  0.0430\n",
      "     45        0.3736  0.0430\n",
      "     46        0.3736  0.0430\n",
      "     47        0.3736  0.0430\n",
      "     48        0.3736  0.0440\n",
      "     49        0.3736  0.0430\n",
      "     50        0.3736  0.0430\n",
      "     51        0.3736  0.0430\n",
      "     52        0.3736  0.0430\n",
      "     53        0.3736  0.0430\n",
      "     54        0.3736  0.0440\n",
      "     55        0.3736  0.0430\n",
      "     56        0.3736  0.0440\n",
      "     57        0.3736  0.0450\n",
      "     58        0.3736  0.0430\n",
      "     59        0.3736  0.0440\n",
      "     60        0.3736  0.0430\n",
      "     61        0.3736  0.0440\n",
      "     62        0.3736  0.0430\n",
      "     63        0.3736  0.0440\n",
      "     64        0.3736  0.0430\n",
      "     65        0.3736  0.0440\n",
      "     66        0.3736  0.0440\n",
      "     67        0.3736  0.0430\n",
      "     68        0.3736  0.0430\n",
      "     69        0.3736  0.0440\n",
      "     70        0.3736  0.0430\n",
      "     71        0.3736  0.0440\n",
      "     72        0.3736  0.0440\n",
      "     73        0.3736  0.0430\n",
      "     74        0.3736  0.0430\n",
      "     75        0.3736  0.0430\n",
      "     76        0.3736  0.0430\n",
      "     77        0.3736  0.0430\n",
      "     78        0.3736  0.0430\n",
      "     79        0.3736  0.0440\n",
      "     80        0.3736  0.0440\n",
      "     81        0.3736  0.0440\n",
      "     82        0.3736  0.0440\n",
      "     83        0.3736  0.0430\n",
      "     84        0.3736  0.0430\n",
      "     85        0.3736  0.0450\n",
      "     86        0.3736  0.0430\n",
      "     87        0.3736  0.0440\n",
      "     88        0.3736  0.0430\n",
      "     89        0.3736  0.0440\n",
      "     90        0.3736  0.0430\n",
      "     91        0.3736  0.0430\n",
      "     92        0.3736  0.0440\n",
      "     93        0.3736  0.0430\n",
      "     94        0.3736  0.0440\n",
      "     95        0.3736  0.0440\n",
      "     96        0.3736  0.0440\n",
      "     97        0.3736  0.0430\n",
      "     98        0.3736  0.0430\n",
      "     99        0.3736  0.0440\n",
      "    100        0.3736  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "      2        1.0000  0.0430\n",
      "      3        1.0000  0.0430\n",
      "      4        1.0000  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        1.0000  0.0430\n",
      "      6        1.0000  0.0440\n",
      "      7        1.0000  0.0440\n",
      "      8        1.0000  0.0430\n",
      "      9        1.0000  0.0460\n",
      "     10        1.0000  0.0440\n",
      "     11        1.0000  0.0430\n",
      "     12        1.0000  0.0430\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0440\n",
      "     15        1.0000  0.0430\n",
      "     16        1.0000  0.0430\n",
      "     17        1.0000  0.0440\n",
      "     18        1.0000  0.0420\n",
      "     19        1.0000  0.0430\n",
      "     20        1.0000  0.0430\n",
      "     21        1.0000  0.0430\n",
      "     22        \u001b[36m0.9954\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3728\u001b[0m  0.0430\n",
      "     25        0.3728  0.0440\n",
      "     26        0.3728  0.0430\n",
      "     27        0.3728  0.0430\n",
      "     28        0.3728  0.0470\n",
      "     29        0.3728  0.0450\n",
      "     30        0.3728  0.0430\n",
      "     31        0.3728  0.0430\n",
      "     32        0.3728  0.0440\n",
      "     33        0.3728  0.0440\n",
      "     34        0.3728  0.0440\n",
      "     35        0.3728  0.0430\n",
      "     36        0.3728  0.0430\n",
      "     37        0.3728  0.0430\n",
      "     38        0.3728  0.0430\n",
      "     39        0.3728  0.0430\n",
      "     40        0.3728  0.0440\n",
      "     41        0.3728  0.0430\n",
      "     42        0.3728  0.0430\n",
      "     43        0.3728  0.0440\n",
      "     44        0.3728  0.0440\n",
      "     45        0.3728  0.0430\n",
      "     46        0.3728  0.0440\n",
      "     47        0.3728  0.0430\n",
      "     48        0.3728  0.0430\n",
      "     49        0.3728  0.0430\n",
      "     50        0.3728  0.0440\n",
      "     51        0.3728  0.0440\n",
      "     52        0.3728  0.0430\n",
      "     53        0.3728  0.0430\n",
      "     54        0.3728  0.0440\n",
      "     55        0.3728  0.0440\n",
      "     56        0.3728  0.0430\n",
      "     57        0.3728  0.0440\n",
      "     58        0.3728  0.0430\n",
      "     59        0.3728  0.0430\n",
      "     60        0.3728  0.0430\n",
      "     61        0.3728  0.0430\n",
      "     62        0.3728  0.0430\n",
      "     63        0.3728  0.0440\n",
      "     64        0.3728  0.0440\n",
      "     65        0.3728  0.0440\n",
      "     66        0.3728  0.0440\n",
      "     67        0.3728  0.0440\n",
      "     68        0.3728  0.0430\n",
      "     69        0.3728  0.0440\n",
      "     70        0.3728  0.0430\n",
      "     71        0.3728  0.0430\n",
      "     72        0.3728  0.0430\n",
      "     73        0.3728  0.0440\n",
      "     74        0.3728  0.0430\n",
      "     75        0.3728  0.0450\n",
      "     76        0.3728  0.0430\n",
      "     77        0.3728  0.0430\n",
      "     78        0.3728  0.0440\n",
      "     79        0.3728  0.0430\n",
      "     80        0.3728  0.0440\n",
      "     81        0.3728  0.0430\n",
      "     82        0.3728  0.0430\n",
      "     83        0.3728  0.0430\n",
      "     84        0.3728  0.0440\n",
      "     85        0.3728  0.0440\n",
      "     86        0.3728  0.0430\n",
      "     87        0.3728  0.0440\n",
      "     88        0.3728  0.0430\n",
      "     89        0.3728  0.0430\n",
      "     90        0.3728  0.0430\n",
      "     91        0.3728  0.0440\n",
      "     92        0.3728  0.0430\n",
      "     93        0.3728  0.0430\n",
      "     94        0.3728  0.0440\n",
      "     95        0.3728  0.0430\n",
      "     96        0.3728  0.0480\n",
      "     97        0.3728  0.0450\n",
      "     98        0.3728  0.0430\n",
      "     99        0.3728  0.0440\n",
      "    100        0.3728  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0330\n",
      "      4        1.0000  0.0330\n",
      "      5        1.0000  0.0340\n",
      "      6        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        1.0000  0.0330\n",
      "      8        1.0000  0.0330\n",
      "      9        1.0000  0.0340\n",
      "     10        1.0000  0.0330\n",
      "     11        1.0000  0.0340\n",
      "     12        1.0000  0.0390\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0330\n",
      "     15        1.0000  0.0340\n",
      "     16        1.0000  0.0330\n",
      "     17        1.0000  0.0330\n",
      "     18        1.0000  0.0340\n",
      "     19        1.0000  0.0330\n",
      "     20        1.0000  0.0340\n",
      "     21        1.0000  0.0330\n",
      "     22        1.0000  0.0340\n",
      "     23        1.0000  0.0330\n",
      "     24        1.0000  0.0330\n",
      "     25        1.0000  0.0340\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0340\n",
      "     28        1.0000  0.0340\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0330\n",
      "     31        1.0000  0.0330\n",
      "     32        1.0000  0.0350\n",
      "     33        1.0000  0.0340\n",
      "     34        1.0000  0.0330\n",
      "     35        1.0000  0.0340\n",
      "     36        1.0000  0.0330\n",
      "     37        1.0000  0.0330\n",
      "     38        1.0000  0.0340\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0340\n",
      "     41        1.0000  0.0340\n",
      "     42        1.0000  0.0340\n",
      "     43        1.0000  0.0330\n",
      "     44        1.0000  0.0330\n",
      "     45        1.0000  0.0330\n",
      "     46        1.0000  0.0340\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0330\n",
      "     49        1.0000  0.0340\n",
      "     50        1.0000  0.0340\n",
      "     51        1.0000  0.0340\n",
      "     52        1.0000  0.0340\n",
      "     53        1.0000  0.0350\n",
      "     54        1.0000  0.0340\n",
      "     55        1.0000  0.0340\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0340\n",
      "     58        1.0000  0.0340\n",
      "     59        1.0000  0.0350\n",
      "     60        1.0000  0.0340\n",
      "     61        1.0000  0.0340\n",
      "     62        1.0000  0.0330\n",
      "     63        1.0000  0.0340\n",
      "     64        1.0000  0.0340\n",
      "     65        1.0000  0.0340\n",
      "     66        1.0000  0.0340\n",
      "     67        1.0000  0.0330\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0330\n",
      "     71        1.0000  0.0340\n",
      "     72        1.0000  0.0330\n",
      "     73        1.0000  0.0330\n",
      "     74        1.0000  0.0340\n",
      "     75        1.0000  0.0340\n",
      "     76        1.0000  0.0340\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0340\n",
      "     79        1.0000  0.0340\n",
      "     80        1.0000  0.0330\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0340\n",
      "     83        1.0000  0.0340\n",
      "     84        1.0000  0.0350\n",
      "     85        1.0000  0.0330\n",
      "     86        1.0000  0.0340\n",
      "     87        1.0000  0.0340\n",
      "     88        1.0000  0.0330\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0330\n",
      "     93        1.0000  0.0340\n",
      "     94        1.0000  0.0350\n",
      "     95        1.0000  0.0330\n",
      "     96        1.0000  0.0340\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0330\n",
      "     99        1.0000  0.0340\n",
      "    100        1.0000  0.0330\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0370\n",
      "      3        1.0000  0.0340\n",
      "      4        1.0000  0.0330\n",
      "      5        1.0000  0.0340\n",
      "      6        1.0000  0.0330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        1.0000  0.0340\n",
      "      8        1.0000  0.0340\n",
      "      9        1.0000  0.0330\n",
      "     10        1.0000  0.0340\n",
      "     11        1.0000  0.0340\n",
      "     12        1.0000  0.0340\n",
      "     13        1.0000  0.0330\n",
      "     14        1.0000  0.0340\n",
      "     15        1.0000  0.0330\n",
      "     16        1.0000  0.0340\n",
      "     17        1.0000  0.0340\n",
      "     18        1.0000  0.0340\n",
      "     19        1.0000  0.0340\n",
      "     20        1.0000  0.0330\n",
      "     21        1.0000  0.0330\n",
      "     22        1.0000  0.0330\n",
      "     23        1.0000  0.0330\n",
      "     24        1.0000  0.0340\n",
      "     25        1.0000  0.0340\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0330\n",
      "     28        1.0000  0.0330\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0330\n",
      "     31        1.0000  0.0340\n",
      "     32        1.0000  0.0330\n",
      "     33        1.0000  0.0340\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0340\n",
      "     36        1.0000  0.0330\n",
      "     37        1.0000  0.0330\n",
      "     38        1.0000  0.0340\n",
      "     39        1.0000  0.0330\n",
      "     40        1.0000  0.0330\n",
      "     41        1.0000  0.0330\n",
      "     42        1.0000  0.0340\n",
      "     43        1.0000  0.0340\n",
      "     44        1.0000  0.0340\n",
      "     45        1.0000  0.0330\n",
      "     46        1.0000  0.0340\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0330\n",
      "     49        1.0000  0.0340\n",
      "     50        1.0000  0.0330\n",
      "     51        1.0000  0.0340\n",
      "     52        1.0000  0.0330\n",
      "     53        1.0000  0.0340\n",
      "     54        1.0000  0.0330\n",
      "     55        1.0000  0.0340\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0330\n",
      "     58        1.0000  0.0330\n",
      "     59        1.0000  0.0340\n",
      "     60        1.0000  0.0340\n",
      "     61        1.0000  0.0330\n",
      "     62        1.0000  0.0340\n",
      "     63        1.0000  0.0330\n",
      "     64        1.0000  0.0340\n",
      "     65        1.0000  0.0330\n",
      "     66        1.0000  0.0340\n",
      "     67        1.0000  0.0340\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0330\n",
      "     71        1.0000  0.0340\n",
      "     72        1.0000  0.0340\n",
      "     73        1.0000  0.0330\n",
      "     74        1.0000  0.0340\n",
      "     75        1.0000  0.0340\n",
      "     76        1.0000  0.0340\n",
      "     77        1.0000  0.0330\n",
      "     78        1.0000  0.0340\n",
      "     79        1.0000  0.0330\n",
      "     80        1.0000  0.0340\n",
      "     81        1.0000  0.0330\n",
      "     82        1.0000  0.0350\n",
      "     83        1.0000  0.0330\n",
      "     84        1.0000  0.0340\n",
      "     85        1.0000  0.0340\n",
      "     86        1.0000  0.0330\n",
      "     87        1.0000  0.0330\n",
      "     88        1.0000  0.0330\n",
      "     89        1.0000  0.0350\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0330\n",
      "     93        1.0000  0.0340\n",
      "     94        1.0000  0.0330\n",
      "     95        1.0000  0.0330\n",
      "     96        1.0000  0.0340\n",
      "     97        1.0000  0.0330\n",
      "     98        1.0000  0.0330\n",
      "     99        1.0000  0.0340\n",
      "    100        1.0000  0.0330\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0330\n",
      "      4        1.0000  0.0340\n",
      "      5        1.0000  0.0330\n",
      "      6        1.0000  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        1.0000  0.0340\n",
      "      8        1.0000  0.0330\n",
      "      9        1.0000  0.0330\n",
      "     10        1.0000  0.0330\n",
      "     11        1.0000  0.0340\n",
      "     12        1.0000  0.0330\n",
      "     13        1.0000  0.0340\n",
      "     14        1.0000  0.0340\n",
      "     15        1.0000  0.0340\n",
      "     16        1.0000  0.0330\n",
      "     17        1.0000  0.0340\n",
      "     18        1.0000  0.0330\n",
      "     19        1.0000  0.0340\n",
      "     20        1.0000  0.0340\n",
      "     21        1.0000  0.0340\n",
      "     22        1.0000  0.0330\n",
      "     23        1.0000  0.0330\n",
      "     24        1.0000  0.0340\n",
      "     25        1.0000  0.0340\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0340\n",
      "     28        1.0000  0.0340\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0330\n",
      "     31        1.0000  0.0340\n",
      "     32        1.0000  0.0330\n",
      "     33        1.0000  0.0340\n",
      "     34        1.0000  0.0330\n",
      "     35        1.0000  0.0350\n",
      "     36        1.0000  0.0340\n",
      "     37        1.0000  0.0330\n",
      "     38        1.0000  0.0330\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0330\n",
      "     41        1.0000  0.0330\n",
      "     42        1.0000  0.0330\n",
      "     43        1.0000  0.0340\n",
      "     44        1.0000  0.0330\n",
      "     45        1.0000  0.0330\n",
      "     46        1.0000  0.0340\n",
      "     47        1.0000  0.0330\n",
      "     48        1.0000  0.0330\n",
      "     49        1.0000  0.0340\n",
      "     50        1.0000  0.0330\n",
      "     51        1.0000  0.0340\n",
      "     52        1.0000  0.0330\n",
      "     53        1.0000  0.0330\n",
      "     54        1.0000  0.0340\n",
      "     55        1.0000  0.0340\n",
      "     56        1.0000  0.0330\n",
      "     57        1.0000  0.0340\n",
      "     58        1.0000  0.0330\n",
      "     59        1.0000  0.0340\n",
      "     60        1.0000  0.0330\n",
      "     61        1.0000  0.0350\n",
      "     62        1.0000  0.0340\n",
      "     63        1.0000  0.0330\n",
      "     64        1.0000  0.0340\n",
      "     65        1.0000  0.0330\n",
      "     66        1.0000  0.0330\n",
      "     67        1.0000  0.0330\n",
      "     68        1.0000  0.0330\n",
      "     69        1.0000  0.0330\n",
      "     70        1.0000  0.0350\n",
      "     71        1.0000  0.0330\n",
      "     72        1.0000  0.0340\n",
      "     73        1.0000  0.0350\n",
      "     74        1.0000  0.0340\n",
      "     75        1.0000  0.0330\n",
      "     76        1.0000  0.0330\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0340\n",
      "     79        1.0000  0.0340\n",
      "     80        1.0000  0.0340\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0340\n",
      "     83        1.0000  0.0340\n",
      "     84        1.0000  0.0340\n",
      "     85        1.0000  0.0330\n",
      "     86        1.0000  0.0330\n",
      "     87        1.0000  0.0420\n",
      "     88        1.0000  0.0330\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0350\n",
      "     92        1.0000  0.0340\n",
      "     93        1.0000  0.0340\n",
      "     94        1.0000  0.0330\n",
      "     95        1.0000  0.0340\n",
      "     96        1.0000  0.0340\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0330\n",
      "     99        1.0000  0.0340\n",
      "    100        1.0000  0.0330\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0340\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0340\n",
      "      4        1.0000  0.0330\n",
      "      5        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        1.0000  0.0340\n",
      "      7        1.0000  0.0330\n",
      "      8        1.0000  0.0340\n",
      "      9        1.0000  0.0330\n",
      "     10        1.0000  0.0340\n",
      "     11        1.0000  0.0340\n",
      "     12        1.0000  0.0340\n",
      "     13        1.0000  0.0330\n",
      "     14        1.0000  0.0340\n",
      "     15        1.0000  0.0330\n",
      "     16        1.0000  0.0330\n",
      "     17        1.0000  0.0340\n",
      "     18        1.0000  0.0330\n",
      "     19        1.0000  0.0330\n",
      "     20        1.0000  0.0340\n",
      "     21        1.0000  0.0340\n",
      "     22        1.0000  0.0340\n",
      "     23        1.0000  0.0330\n",
      "     24        1.0000  0.0330\n",
      "     25        1.0000  0.0330\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0330\n",
      "     28        1.0000  0.0330\n",
      "     29        1.0000  0.0330\n",
      "     30        1.0000  0.0340\n",
      "     31        1.0000  0.0340\n",
      "     32        1.0000  0.0330\n",
      "     33        1.0000  0.0330\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0340\n",
      "     36        1.0000  0.0340\n",
      "     37        1.0000  0.0330\n",
      "     38        1.0000  0.0330\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0340\n",
      "     41        1.0000  0.0330\n",
      "     42        1.0000  0.0350\n",
      "     43        1.0000  0.0330\n",
      "     44        1.0000  0.0330\n",
      "     45        1.0000  0.0340\n",
      "     46        1.0000  0.0330\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0330\n",
      "     49        1.0000  0.0340\n",
      "     50        1.0000  0.0330\n",
      "     51        1.0000  0.0340\n",
      "     52        1.0000  0.0330\n",
      "     53        1.0000  0.0330\n",
      "     54        1.0000  0.0340\n",
      "     55        1.0000  0.0330\n",
      "     56        1.0000  0.0330\n",
      "     57        1.0000  0.0330\n",
      "     58        1.0000  0.0340\n",
      "     59        1.0000  0.0330\n",
      "     60        1.0000  0.0340\n",
      "     61        1.0000  0.0340\n",
      "     62        1.0000  0.0330\n",
      "     63        1.0000  0.0330\n",
      "     64        1.0000  0.0340\n",
      "     65        1.0000  0.0330\n",
      "     66        1.0000  0.0340\n",
      "     67        1.0000  0.0330\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0330\n",
      "     71        1.0000  0.0340\n",
      "     72        1.0000  0.0340\n",
      "     73        1.0000  0.0330\n",
      "     74        1.0000  0.0340\n",
      "     75        1.0000  0.0340\n",
      "     76        1.0000  0.0340\n",
      "     77        1.0000  0.0330\n",
      "     78        1.0000  0.0340\n",
      "     79        1.0000  0.0340\n",
      "     80        1.0000  0.0340\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0330\n",
      "     83        1.0000  0.0340\n",
      "     84        1.0000  0.0330\n",
      "     85        1.0000  0.0340\n",
      "     86        1.0000  0.0330\n",
      "     87        1.0000  0.0330\n",
      "     88        1.0000  0.0340\n",
      "     89        1.0000  0.0330\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0330\n",
      "     92        1.0000  0.0330\n",
      "     93        1.0000  0.0350\n",
      "     94        1.0000  0.0350\n",
      "     95        1.0000  0.0340\n",
      "     96        1.0000  0.0330\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0340\n",
      "     99        1.0000  0.0330\n",
      "    100        1.0000  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0330\n",
      "      4        1.0000  0.0330\n",
      "      5        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        1.0000  0.0350\n",
      "      7        1.0000  0.0330\n",
      "      8        1.0000  0.0330\n",
      "      9        1.0000  0.0340\n",
      "     10        1.0000  0.0350\n",
      "     11        1.0000  0.0330\n",
      "     12        1.0000  0.0340\n",
      "     13        1.0000  0.0340\n",
      "     14        1.0000  0.0330\n",
      "     15        1.0000  0.0330\n",
      "     16        1.0000  0.0330\n",
      "     17        1.0000  0.0340\n",
      "     18        1.0000  0.0340\n",
      "     19        1.0000  0.0340\n",
      "     20        1.0000  0.0330\n",
      "     21        1.0000  0.0340\n",
      "     22        1.0000  0.0330\n",
      "     23        1.0000  0.0330\n",
      "     24        1.0000  0.0330\n",
      "     25        1.0000  0.0330\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0330\n",
      "     28        1.0000  0.0330\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0340\n",
      "     31        1.0000  0.0330\n",
      "     32        1.0000  0.0330\n",
      "     33        1.0000  0.0340\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0330\n",
      "     36        1.0000  0.0340\n",
      "     37        1.0000  0.0340\n",
      "     38        1.0000  0.0340\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0340\n",
      "     41        1.0000  0.0330\n",
      "     42        1.0000  0.0340\n",
      "     43        1.0000  0.0330\n",
      "     44        1.0000  0.0340\n",
      "     45        1.0000  0.0340\n",
      "     46        1.0000  0.0340\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0340\n",
      "     49        1.0000  0.0330\n",
      "     50        1.0000  0.0340\n",
      "     51        1.0000  0.0340\n",
      "     52        1.0000  0.0330\n",
      "     53        1.0000  0.0340\n",
      "     54        1.0000  0.0330\n",
      "     55        1.0000  0.0340\n",
      "     56        1.0000  0.0330\n",
      "     57        1.0000  0.0330\n",
      "     58        1.0000  0.0340\n",
      "     59        1.0000  0.0340\n",
      "     60        1.0000  0.0340\n",
      "     61        1.0000  0.0330\n",
      "     62        1.0000  0.0330\n",
      "     63        1.0000  0.0340\n",
      "     64        1.0000  0.0330\n",
      "     65        1.0000  0.0340\n",
      "     66        1.0000  0.0340\n",
      "     67        1.0000  0.0340\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0340\n",
      "     71        1.0000  0.0330\n",
      "     72        1.0000  0.0330\n",
      "     73        1.0000  0.0340\n",
      "     74        1.0000  0.0330\n",
      "     75        1.0000  0.0350\n",
      "     76        1.0000  0.0380\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0330\n",
      "     79        1.0000  0.0340\n",
      "     80        1.0000  0.0330\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0340\n",
      "     83        1.0000  0.0330\n",
      "     84        1.0000  0.0340\n",
      "     85        1.0000  0.0330\n",
      "     86        1.0000  0.0340\n",
      "     87        1.0000  0.0350\n",
      "     88        1.0000  0.0330\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0330\n",
      "     92        1.0000  0.0340\n",
      "     93        1.0000  0.0340\n",
      "     94        1.0000  0.0330\n",
      "     95        1.0000  0.0350\n",
      "     96        1.0000  0.0330\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0340\n",
      "     99        1.0000  0.0330\n",
      "    100        1.0000  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "      2        1.0000  0.0430\n",
      "      3        1.0000  0.0430\n",
      "      4        1.0000  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        1.0000  0.0450\n",
      "      6        1.0000  0.0440\n",
      "      7        1.0000  0.0430\n",
      "      8        1.0000  0.0430\n",
      "      9        1.0000  0.0440\n",
      "     10        1.0000  0.0430\n",
      "     11        1.0000  0.0430\n",
      "     12        1.0000  0.0440\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0430\n",
      "     15        1.0000  0.0430\n",
      "     16        1.0000  0.0430\n",
      "     17        1.0000  0.0440\n",
      "     18        1.0000  0.0430\n",
      "     19        1.0000  0.0430\n",
      "     20        1.0000  0.0440\n",
      "     21        1.0000  0.0430\n",
      "     22        1.0000  0.0430\n",
      "     23        1.0000  0.0440\n",
      "     24        1.0000  0.0430\n",
      "     25        1.0000  0.0430\n",
      "     26        \u001b[36m1.0000\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.6760\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.3714\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3714\u001b[0m  0.0440\n",
      "     30        0.3714  0.0440\n",
      "     31        0.3714  0.0430\n",
      "     32        0.3714  0.0440\n",
      "     33        0.3714  0.0440\n",
      "     34        0.3714  0.0430\n",
      "     35        0.3714  0.0440\n",
      "     36        0.3714  0.0430\n",
      "     37        0.3714  0.0440\n",
      "     38        0.3714  0.0440\n",
      "     39        0.3714  0.0440\n",
      "     40        0.3714  0.0440\n",
      "     41        0.3714  0.0430\n",
      "     42        0.3714  0.0430\n",
      "     43        0.3714  0.0440\n",
      "     44        0.3714  0.0430\n",
      "     45        0.3714  0.0440\n",
      "     46        0.3714  0.0430\n",
      "     47        0.3714  0.0440\n",
      "     48        0.3714  0.0440\n",
      "     49        0.3714  0.0430\n",
      "     50        0.3714  0.0480\n",
      "     51        0.3714  0.0430\n",
      "     52        0.3714  0.0430\n",
      "     53        0.3714  0.0440\n",
      "     54        0.3714  0.0430\n",
      "     55        0.3714  0.0430\n",
      "     56        0.3714  0.0430\n",
      "     57        0.3714  0.0440\n",
      "     58        0.3714  0.0430\n",
      "     59        0.3714  0.0440\n",
      "     60        0.3714  0.0430\n",
      "     61        0.3714  0.0430\n",
      "     62        0.3714  0.0450\n",
      "     63        0.3714  0.0450\n",
      "     64        0.3714  0.0430\n",
      "     65        0.3714  0.0440\n",
      "     66        0.3714  0.0430\n",
      "     67        0.3714  0.0440\n",
      "     68        0.3714  0.0440\n",
      "     69        0.3714  0.0430\n",
      "     70        0.3714  0.0440\n",
      "     71        0.3714  0.0440\n",
      "     72        0.3714  0.0440\n",
      "     73        0.3714  0.0440\n",
      "     74        0.3714  0.0440\n",
      "     75        0.3714  0.0430\n",
      "     76        0.3714  0.0430\n",
      "     77        0.3714  0.0440\n",
      "     78        0.3714  0.0430\n",
      "     79        0.3714  0.0440\n",
      "     80        0.3714  0.0450\n",
      "     81        0.3714  0.0440\n",
      "     82        0.3714  0.0440\n",
      "     83        0.3714  0.0440\n",
      "     84        0.3714  0.0440\n",
      "     85        0.3714  0.0430\n",
      "     86        0.3714  0.0440\n",
      "     87        0.3714  0.0440\n",
      "     88        0.3714  0.0440\n",
      "     89        0.3714  0.0440\n",
      "     90        0.3714  0.0440\n",
      "     91        0.3714  0.0430\n",
      "     92        0.3714  0.0430\n",
      "     93        0.3714  0.0440\n",
      "     94        0.3714  0.0440\n",
      "     95        0.3714  0.0450\n",
      "     96        0.3714  0.0440\n",
      "     97        0.3714  0.0440\n",
      "     98        0.3714  0.0440\n",
      "     99        0.3714  0.0430\n",
      "    100        0.3714  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "      2        1.0000  0.0440\n",
      "      3        1.0000  0.0430\n",
      "      4        1.0000  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        1.0000  0.0440\n",
      "      6        1.0000  0.0430\n",
      "      7        1.0000  0.0440\n",
      "      8        1.0000  0.0440\n",
      "      9        1.0000  0.0440\n",
      "     10        1.0000  0.0450\n",
      "     11        1.0000  0.0440\n",
      "     12        1.0000  0.0440\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0430\n",
      "     15        1.0000  0.0440\n",
      "     16        1.0000  0.0430\n",
      "     17        1.0000  0.0430\n",
      "     18        1.0000  0.0440\n",
      "     19        1.0000  0.0430\n",
      "     20        1.0000  0.0420\n",
      "     21        1.0000  0.0430\n",
      "     22        1.0000  0.0440\n",
      "     23        1.0000  0.0430\n",
      "     24        1.0000  0.0430\n",
      "     25        1.0000  0.0430\n",
      "     26        1.0000  0.0430\n",
      "     27        \u001b[36m1.0000\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.7035\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3714\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3714\u001b[0m  0.0440\n",
      "     31        0.3714  0.0430\n",
      "     32        0.3714  0.0440\n",
      "     33        0.3714  0.0430\n",
      "     34        0.3714  0.0430\n",
      "     35        0.3714  0.0440\n",
      "     36        0.3714  0.0440\n",
      "     37        0.3714  0.0430\n",
      "     38        0.3714  0.0440\n",
      "     39        0.3714  0.0440\n",
      "     40        0.3714  0.0440\n",
      "     41        0.3714  0.0440\n",
      "     42        0.3714  0.0430\n",
      "     43        0.3714  0.0430\n",
      "     44        0.3714  0.0430\n",
      "     45        0.3714  0.0440\n",
      "     46        0.3714  0.0430\n",
      "     47        0.3714  0.0440\n",
      "     48        0.3714  0.0440\n",
      "     49        0.3714  0.0440\n",
      "     50        0.3714  0.0440\n",
      "     51        0.3714  0.0430\n",
      "     52        0.3714  0.0440\n",
      "     53        0.3714  0.0430\n",
      "     54        0.3714  0.0440\n",
      "     55        0.3714  0.0470\n",
      "     56        0.3714  0.0430\n",
      "     57        0.3714  0.0440\n",
      "     58        0.3714  0.0430\n",
      "     59        0.3714  0.0430\n",
      "     60        0.3714  0.0440\n",
      "     61        0.3714  0.0440\n",
      "     62        0.3714  0.0440\n",
      "     63        0.3714  0.0450\n",
      "     64        0.3714  0.0430\n",
      "     65        0.3714  0.0440\n",
      "     66        0.3714  0.0440\n",
      "     67        0.3714  0.0430\n",
      "     68        0.3714  0.0440\n",
      "     69        0.3714  0.0430\n",
      "     70        0.3714  0.0440\n",
      "     71        0.3714  0.0430\n",
      "     72        0.3714  0.0440\n",
      "     73        0.3714  0.0430\n",
      "     74        0.3714  0.0440\n",
      "     75        0.3714  0.0430\n",
      "     76        0.3714  0.0460\n",
      "     77        0.3714  0.0430\n",
      "     78        0.3714  0.0450\n",
      "     79        0.3714  0.0440\n",
      "     80        0.3714  0.0440\n",
      "     81        0.3714  0.0440\n",
      "     82        0.3714  0.0430\n",
      "     83        0.3714  0.0440\n",
      "     84        0.3714  0.0440\n",
      "     85        0.3714  0.0450\n",
      "     86        0.3714  0.0440\n",
      "     87        0.3714  0.0440\n",
      "     88        0.3714  0.0440\n",
      "     89        0.3714  0.0430\n",
      "     90        0.3714  0.0440\n",
      "     91        0.3714  0.0440\n",
      "     92        0.3714  0.0450\n",
      "     93        0.3714  0.0440\n",
      "     94        0.3714  0.0440\n",
      "     95        0.3714  0.0440\n",
      "     96        0.3714  0.0450\n",
      "     97        0.3714  0.0440\n",
      "     98        0.3714  0.0440\n",
      "     99        0.3714  0.0430\n",
      "    100        0.3714  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0440\n",
      "      2        1.0000  0.0430\n",
      "      3        1.0000  0.0440\n",
      "      4        1.0000  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        1.0000  0.0450\n",
      "      6        1.0000  0.0440\n",
      "      7        1.0000  0.0440\n",
      "      8        1.0000  0.0430\n",
      "      9        1.0000  0.0440\n",
      "     10        1.0000  0.0440\n",
      "     11        1.0000  0.0440\n",
      "     12        1.0000  0.0430\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0430\n",
      "     15        1.0000  0.0430\n",
      "     16        1.0000  0.0440\n",
      "     17        1.0000  0.0440\n",
      "     18        1.0000  0.0430\n",
      "     19        1.0000  0.0430\n",
      "     20        1.0000  0.0430\n",
      "     21        1.0000  0.0430\n",
      "     22        1.0000  0.0430\n",
      "     23        1.0000  0.0440\n",
      "     24        1.0000  0.0440\n",
      "     25        1.0000  0.0430\n",
      "     26        1.0000  0.0430\n",
      "     27        1.0000  0.0430\n",
      "     28        \u001b[36m0.7459\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     30        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     31        0.3736  0.0440\n",
      "     32        0.3736  0.0440\n",
      "     33        0.3736  0.0430\n",
      "     34        0.3736  0.0430\n",
      "     35        0.3736  0.0430\n",
      "     36        0.3736  0.0430\n",
      "     37        0.3736  0.0440\n",
      "     38        0.3736  0.0430\n",
      "     39        0.3736  0.0430\n",
      "     40        0.3736  0.0430\n",
      "     41        0.3736  0.0440\n",
      "     42        0.3736  0.0430\n",
      "     43        0.3736  0.0430\n",
      "     44        0.3736  0.0450\n",
      "     45        0.3736  0.0440\n",
      "     46        0.3736  0.0440\n",
      "     47        0.3736  0.0430\n",
      "     48        0.3736  0.0430\n",
      "     49        0.3736  0.0440\n",
      "     50        0.3736  0.0430\n",
      "     51        0.3736  0.0430\n",
      "     52        0.3736  0.0430\n",
      "     53        0.3736  0.0440\n",
      "     54        0.3736  0.0440\n",
      "     55        0.3736  0.0430\n",
      "     56        0.3736  0.0430\n",
      "     57        0.3736  0.0430\n",
      "     58        0.3736  0.0440\n",
      "     59        0.3736  0.0430\n",
      "     60        0.3736  0.0430\n",
      "     61        0.3736  0.0440\n",
      "     62        0.3736  0.0430\n",
      "     63        0.3736  0.0430\n",
      "     64        0.3736  0.0430\n",
      "     65        0.3736  0.0430\n",
      "     66        0.3736  0.0440\n",
      "     67        0.3736  0.0430\n",
      "     68        0.3736  0.0430\n",
      "     69        0.3736  0.0440\n",
      "     70        0.3736  0.0430\n",
      "     71        0.3736  0.0450\n",
      "     72        0.3736  0.0440\n",
      "     73        0.3736  0.0440\n",
      "     74        0.3736  0.0430\n",
      "     75        0.3736  0.0440\n",
      "     76        0.3736  0.0430\n",
      "     77        0.3736  0.0430\n",
      "     78        0.3736  0.0440\n",
      "     79        0.3736  0.0430\n",
      "     80        0.3736  0.0440\n",
      "     81        0.3736  0.0430\n",
      "     82        0.3736  0.0440\n",
      "     83        0.3736  0.0430\n",
      "     84        0.3736  0.0430\n",
      "     85        0.3736  0.0440\n",
      "     86        0.3736  0.0440\n",
      "     87        0.3736  0.0460\n",
      "     88        0.3736  0.0450\n",
      "     89        0.3736  0.0440\n",
      "     90        0.3736  0.0440\n",
      "     91        0.3736  0.0460\n",
      "     92        0.3736  0.0440\n",
      "     93        0.3736  0.0450\n",
      "     94        0.3736  0.0460\n",
      "     95        0.3736  0.0450\n",
      "     96        0.3736  0.0480\n",
      "     97        0.3736  0.0450\n",
      "     98        0.3736  0.0440\n",
      "     99        0.3736  0.0440\n",
      "    100        0.3736  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "      2        1.0000  0.0430\n",
      "      3        1.0000  0.0430\n",
      "      4        1.0000  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        1.0000  0.0440\n",
      "      6        1.0000  0.0440\n",
      "      7        1.0000  0.0440\n",
      "      8        1.0000  0.0430\n",
      "      9        1.0000  0.0430\n",
      "     10        1.0000  0.0430\n",
      "     11        1.0000  0.0430\n",
      "     12        1.0000  0.0440\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0430\n",
      "     15        1.0000  0.0440\n",
      "     16        1.0000  0.0440\n",
      "     17        1.0000  0.0440\n",
      "     18        1.0000  0.0430\n",
      "     19        1.0000  0.0450\n",
      "     20        1.0000  0.0430\n",
      "     21        1.0000  0.0440\n",
      "     22        1.0000  0.0440\n",
      "     23        1.0000  0.0430\n",
      "     24        1.0000  0.0430\n",
      "     25        1.0000  0.0440\n",
      "     26        1.0000  0.0440\n",
      "     27        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3926\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     30        0.3736  0.0430\n",
      "     31        0.3736  0.0450\n",
      "     32        0.3736  0.0450\n",
      "     33        0.3736  0.0440\n",
      "     34        0.3736  0.0470\n",
      "     35        0.3736  0.0430\n",
      "     36        0.3736  0.0430\n",
      "     37        0.3736  0.0430\n",
      "     38        0.3736  0.0430\n",
      "     39        0.3736  0.0440\n",
      "     40        0.3736  0.0440\n",
      "     41        0.3736  0.0430\n",
      "     42        0.3736  0.0440\n",
      "     43        0.3736  0.0430\n",
      "     44        0.3736  0.0450\n",
      "     45        0.3736  0.0430\n",
      "     46        0.3736  0.0440\n",
      "     47        0.3736  0.0430\n",
      "     48        0.3736  0.0440\n",
      "     49        0.3736  0.0430\n",
      "     50        0.3736  0.0430\n",
      "     51        0.3736  0.0440\n",
      "     52        0.3736  0.0440\n",
      "     53        0.3736  0.0440\n",
      "     54        0.3736  0.0440\n",
      "     55        0.3736  0.0430\n",
      "     56        0.3736  0.0430\n",
      "     57        0.3736  0.0430\n",
      "     58        0.3736  0.0430\n",
      "     59        0.3736  0.0440\n",
      "     60        0.3736  0.0430\n",
      "     61        0.3736  0.0430\n",
      "     62        0.3736  0.0440\n",
      "     63        0.3736  0.0430\n",
      "     64        0.3736  0.0430\n",
      "     65        0.3736  0.0430\n",
      "     66        0.3736  0.0440\n",
      "     67        0.3736  0.0450\n",
      "     68        0.3736  0.0440\n",
      "     69        0.3736  0.0430\n",
      "     70        0.3736  0.0430\n",
      "     71        0.3736  0.0430\n",
      "     72        0.3736  0.0440\n",
      "     73        0.3736  0.0430\n",
      "     74        0.3736  0.0440\n",
      "     75        0.3736  0.0440\n",
      "     76        0.3736  0.0440\n",
      "     77        0.3736  0.0430\n",
      "     78        0.3736  0.0440\n",
      "     79        0.3736  0.0430\n",
      "     80        0.3736  0.0440\n",
      "     81        0.3736  0.0440\n",
      "     82        0.3736  0.0430\n",
      "     83        0.3736  0.0430\n",
      "     84        0.3736  0.0440\n",
      "     85        0.3736  0.0440\n",
      "     86        0.3736  0.0440\n",
      "     87        0.3736  0.0430\n",
      "     88        0.3736  0.0440\n",
      "     89        0.3736  0.0440\n",
      "     90        0.3736  0.0440\n",
      "     91        0.3736  0.0430\n",
      "     92        0.3736  0.0440\n",
      "     93        0.3736  0.0440\n",
      "     94        0.3736  0.0440\n",
      "     95        0.3736  0.0430\n",
      "     96        0.3736  0.0430\n",
      "     97        0.3736  0.0440\n",
      "     98        0.3736  0.0440\n",
      "     99        0.3736  0.0440\n",
      "    100        0.3736  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "      2        1.0000  0.0440\n",
      "      3        1.0000  0.0430\n",
      "      4        1.0000  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        1.0000  0.0440\n",
      "      6        1.0000  0.0430\n",
      "      7        1.0000  0.0440\n",
      "      8        1.0000  0.0450\n",
      "      9        1.0000  0.0440\n",
      "     10        1.0000  0.0430\n",
      "     11        1.0000  0.0440\n",
      "     12        1.0000  0.0440\n",
      "     13        1.0000  0.0430\n",
      "     14        1.0000  0.0430\n",
      "     15        1.0000  0.0440\n",
      "     16        1.0000  0.0430\n",
      "     17        1.0000  0.0440\n",
      "     18        1.0000  0.0430\n",
      "     19        1.0000  0.0430\n",
      "     20        1.0000  0.0440\n",
      "     21        1.0000  0.0440\n",
      "     22        1.0000  0.0430\n",
      "     23        1.0000  0.0430\n",
      "     24        1.0000  0.0440\n",
      "     25        \u001b[36m1.0000\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.8309\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.3728\u001b[0m  0.0520\n",
      "     28        0.3728  0.0450\n",
      "     29        0.3728  0.0440\n",
      "     30        0.3728  0.0430\n",
      "     31        0.3728  0.0440\n",
      "     32        0.3728  0.0440\n",
      "     33        0.3728  0.0440\n",
      "     34        0.3728  0.0440\n",
      "     35        0.3728  0.0450\n",
      "     36        0.3728  0.0430\n",
      "     37        0.3728  0.0430\n",
      "     38        0.3728  0.0440\n",
      "     39        0.3728  0.0430\n",
      "     40        0.3728  0.0440\n",
      "     41        0.3728  0.0430\n",
      "     42        0.3728  0.0430\n",
      "     43        0.3728  0.0440\n",
      "     44        0.3728  0.0430\n",
      "     45        0.3728  0.0440\n",
      "     46        0.3728  0.0430\n",
      "     47        0.3728  0.0440\n",
      "     48        0.3728  0.0440\n",
      "     49        0.3728  0.0430\n",
      "     50        0.3728  0.0440\n",
      "     51        0.3728  0.0430\n",
      "     52        0.3728  0.0430\n",
      "     53        0.3728  0.0440\n",
      "     54        0.3728  0.0440\n",
      "     55        0.3728  0.0440\n",
      "     56        0.3728  0.0440\n",
      "     57        0.3728  0.0450\n",
      "     58        0.3728  0.0430\n",
      "     59        0.3728  0.0430\n",
      "     60        0.3728  0.0440\n",
      "     61        0.3728  0.0430\n",
      "     62        0.3728  0.0440\n",
      "     63        0.3728  0.0430\n",
      "     64        0.3728  0.0430\n",
      "     65        0.3728  0.0440\n",
      "     66        0.3728  0.0440\n",
      "     67        0.3728  0.0440\n",
      "     68        0.3728  0.0430\n",
      "     69        0.3728  0.0430\n",
      "     70        0.3728  0.0440\n",
      "     71        0.3728  0.0430\n",
      "     72        0.3728  0.0430\n",
      "     73        0.3728  0.0440\n",
      "     74        0.3728  0.0440\n",
      "     75        0.3728  0.0440\n",
      "     76        0.3728  0.0490\n",
      "     77        0.3728  0.0430\n",
      "     78        0.3728  0.0430\n",
      "     79        0.3728  0.0450\n",
      "     80        0.3728  0.0440\n",
      "     81        0.3728  0.0430\n",
      "     82        0.3728  0.0430\n",
      "     83        0.3728  0.0440\n",
      "     84        0.3728  0.0430\n",
      "     85        0.3728  0.0430\n",
      "     86        0.3728  0.0430\n",
      "     87        0.3728  0.0440\n",
      "     88        0.3728  0.0430\n",
      "     89        0.3728  0.0440\n",
      "     90        0.3728  0.0440\n",
      "     91        0.3728  0.0430\n",
      "     92        0.3728  0.0440\n",
      "     93        0.3728  0.0430\n",
      "     94        0.3728  0.0440\n",
      "     95        0.3728  0.0430\n",
      "     96        0.3728  0.0440\n",
      "     97        0.3728  0.0440\n",
      "     98        0.3728  0.0440\n",
      "     99        0.3728  0.0430\n",
      "    100        0.3728  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0340\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0340\n",
      "      4        1.0000  0.0330\n",
      "      5        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        1.0000  0.0340\n",
      "      7        1.0000  0.0330\n",
      "      8        1.0000  0.0340\n",
      "      9        1.0000  0.0330\n",
      "     10        1.0000  0.0340\n",
      "     11        1.0000  0.0330\n",
      "     12        1.0000  0.0340\n",
      "     13        1.0000  0.0330\n",
      "     14        1.0000  0.0340\n",
      "     15        1.0000  0.0330\n",
      "     16        1.0000  0.0340\n",
      "     17        1.0000  0.0340\n",
      "     18        1.0000  0.0330\n",
      "     19        1.0000  0.0350\n",
      "     20        1.0000  0.0340\n",
      "     21        1.0000  0.0340\n",
      "     22        1.0000  0.0340\n",
      "     23        1.0000  0.0340\n",
      "     24        1.0000  0.0340\n",
      "     25        1.0000  0.0330\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0340\n",
      "     28        1.0000  0.0340\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0340\n",
      "     31        1.0000  0.0340\n",
      "     32        1.0000  0.0340\n",
      "     33        1.0000  0.0340\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0330\n",
      "     36        1.0000  0.0340\n",
      "     37        1.0000  0.0330\n",
      "     38        1.0000  0.0330\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0340\n",
      "     41        1.0000  0.0330\n",
      "     42        1.0000  0.0330\n",
      "     43        1.0000  0.0380\n",
      "     44        1.0000  0.0370\n",
      "     45        1.0000  0.0480\n",
      "     46        1.0000  0.0420\n",
      "     47        1.0000  0.0350\n",
      "     48        1.0000  0.0340\n",
      "     49        1.0000  0.0330\n",
      "     50        1.0000  0.0340\n",
      "     51        1.0000  0.0340\n",
      "     52        1.0000  0.0340\n",
      "     53        1.0000  0.0350\n",
      "     54        1.0000  0.0380\n",
      "     55        1.0000  0.0350\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0340\n",
      "     58        1.0000  0.0330\n",
      "     59        1.0000  0.0340\n",
      "     60        1.0000  0.0340\n",
      "     61        1.0000  0.0350\n",
      "     62        1.0000  0.0350\n",
      "     63        1.0000  0.0340\n",
      "     64        1.0000  0.0330\n",
      "     65        1.0000  0.0340\n",
      "     66        1.0000  0.0340\n",
      "     67        1.0000  0.0330\n",
      "     68        1.0000  0.0330\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0340\n",
      "     71        1.0000  0.0340\n",
      "     72        1.0000  0.0340\n",
      "     73        1.0000  0.0340\n",
      "     74        1.0000  0.0330\n",
      "     75        1.0000  0.0350\n",
      "     76        1.0000  0.0340\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0330\n",
      "     79        1.0000  0.0340\n",
      "     80        1.0000  0.0330\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0340\n",
      "     83        1.0000  0.0340\n",
      "     84        1.0000  0.0340\n",
      "     85        1.0000  0.0340\n",
      "     86        1.0000  0.0340\n",
      "     87        1.0000  0.0340\n",
      "     88        1.0000  0.0330\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0330\n",
      "     93        1.0000  0.0340\n",
      "     94        1.0000  0.0340\n",
      "     95        1.0000  0.0340\n",
      "     96        1.0000  0.0340\n",
      "     97        1.0000  0.0330\n",
      "     98        1.0000  0.0340\n",
      "     99        1.0000  0.0340\n",
      "    100        1.0000  0.0330\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0340\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0330\n",
      "      4        1.0000  0.0340\n",
      "      5        1.0000  0.0350\n",
      "      6        1.0000  0.0340"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        1.0000  0.0340\n",
      "      8        1.0000  0.0350\n",
      "      9        1.0000  0.0340\n",
      "     10        1.0000  0.0340\n",
      "     11        1.0000  0.0350\n",
      "     12        1.0000  0.0370\n",
      "     13        1.0000  0.0360\n",
      "     14        1.0000  0.0410\n",
      "     15        1.0000  0.0370\n",
      "     16        1.0000  0.0550\n",
      "     17        1.0000  0.0440\n",
      "     18        1.0000  0.0380\n",
      "     19        1.0000  0.0330\n",
      "     20        1.0000  0.0340\n",
      "     21        1.0000  0.0340\n",
      "     22        1.0000  0.0340\n",
      "     23        1.0000  0.0340\n",
      "     24        1.0000  0.0330\n",
      "     25        1.0000  0.0330\n",
      "     26        1.0000  0.0330\n",
      "     27        1.0000  0.0340\n",
      "     28        1.0000  0.0330\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0340\n",
      "     31        1.0000  0.0340\n",
      "     32        1.0000  0.0330\n",
      "     33        1.0000  0.0340\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0330\n",
      "     36        1.0000  0.0340\n",
      "     37        1.0000  0.0340\n",
      "     38        1.0000  0.0330\n",
      "     39        1.0000  0.0370\n",
      "     40        1.0000  0.0360\n",
      "     41        1.0000  0.0330\n",
      "     42        1.0000  0.0340\n",
      "     43        1.0000  0.0330\n",
      "     44        1.0000  0.0340\n",
      "     45        1.0000  0.0330\n",
      "     46        1.0000  0.0350\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0340\n",
      "     49        1.0000  0.0340\n",
      "     50        1.0000  0.0330\n",
      "     51        1.0000  0.0340\n",
      "     52        1.0000  0.0340\n",
      "     53        1.0000  0.0340\n",
      "     54        1.0000  0.0330\n",
      "     55        1.0000  0.0340\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0340\n",
      "     58        1.0000  0.0350\n",
      "     59        1.0000  0.0340\n",
      "     60        1.0000  0.0380\n",
      "     61        1.0000  0.0340\n",
      "     62        1.0000  0.0340\n",
      "     63        1.0000  0.0330\n",
      "     64        1.0000  0.0340\n",
      "     65        1.0000  0.0330\n",
      "     66        1.0000  0.0340\n",
      "     67        1.0000  0.0340\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0340\n",
      "     71        1.0000  0.0330\n",
      "     72        1.0000  0.0330\n",
      "     73        1.0000  0.0340\n",
      "     74        1.0000  0.0340\n",
      "     75        1.0000  0.0340\n",
      "     76        1.0000  0.0340\n",
      "     77        1.0000  0.0330\n",
      "     78        1.0000  0.0350\n",
      "     79        1.0000  0.0330\n",
      "     80        1.0000  0.0340\n",
      "     81        1.0000  0.0330\n",
      "     82        1.0000  0.0340\n",
      "     83        1.0000  0.0330\n",
      "     84        1.0000  0.0330\n",
      "     85        1.0000  0.0340\n",
      "     86        1.0000  0.0330\n",
      "     87        1.0000  0.0340\n",
      "     88        1.0000  0.0330\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0340\n",
      "     93        1.0000  0.0340\n",
      "     94        1.0000  0.0340\n",
      "     95        1.0000  0.0350\n",
      "     96        1.0000  0.0350\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0340\n",
      "     99        1.0000  0.0340\n",
      "    100        1.0000  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0340\n",
      "      4        1.0000  0.0340\n",
      "      5        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        1.0000  0.0340\n",
      "      7        1.0000  0.0330\n",
      "      8        1.0000  0.0340\n",
      "      9        1.0000  0.0340\n",
      "     10        1.0000  0.0340\n",
      "     11        1.0000  0.0340\n",
      "     12        1.0000  0.0330\n",
      "     13        1.0000  0.0340\n",
      "     14        1.0000  0.0340\n",
      "     15        1.0000  0.0330\n",
      "     16        1.0000  0.0340\n",
      "     17        1.0000  0.0330\n",
      "     18        1.0000  0.0340\n",
      "     19        1.0000  0.0340\n",
      "     20        1.0000  0.0340\n",
      "     21        1.0000  0.0340\n",
      "     22        1.0000  0.0330\n",
      "     23        1.0000  0.0340\n",
      "     24        1.0000  0.0330\n",
      "     25        1.0000  0.0350\n",
      "     26        1.0000  0.0330\n",
      "     27        1.0000  0.0340\n",
      "     28        1.0000  0.0350\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0330\n",
      "     31        1.0000  0.0340\n",
      "     32        1.0000  0.0340\n",
      "     33        1.0000  0.0340\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0330\n",
      "     36        1.0000  0.0340\n",
      "     37        1.0000  0.0330\n",
      "     38        1.0000  0.0340\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0330\n",
      "     41        1.0000  0.0330\n",
      "     42        1.0000  0.0340\n",
      "     43        1.0000  0.0340\n",
      "     44        1.0000  0.0330\n",
      "     45        1.0000  0.0350\n",
      "     46        1.0000  0.0330\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0350\n",
      "     49        1.0000  0.0330\n",
      "     50        1.0000  0.0330\n",
      "     51        1.0000  0.0350\n",
      "     52        1.0000  0.0340\n",
      "     53        1.0000  0.0340\n",
      "     54        1.0000  0.0330\n",
      "     55        1.0000  0.0330\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0330\n",
      "     58        1.0000  0.0340\n",
      "     59        1.0000  0.0330\n",
      "     60        1.0000  0.0330\n",
      "     61        1.0000  0.0340\n",
      "     62        1.0000  0.0350\n",
      "     63        1.0000  0.0340\n",
      "     64        1.0000  0.0330\n",
      "     65        1.0000  0.0350\n",
      "     66        1.0000  0.0330\n",
      "     67        1.0000  0.0340\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0340\n",
      "     71        1.0000  0.0330\n",
      "     72        1.0000  0.0340\n",
      "     73        1.0000  0.0340\n",
      "     74        1.0000  0.0330\n",
      "     75        1.0000  0.0340\n",
      "     76        1.0000  0.0340\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0340\n",
      "     79        1.0000  0.0330\n",
      "     80        1.0000  0.0340\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0350\n",
      "     83        1.0000  0.0340\n",
      "     84        1.0000  0.0340\n",
      "     85        1.0000  0.0340\n",
      "     86        1.0000  0.0330\n",
      "     87        1.0000  0.0330\n",
      "     88        1.0000  0.0340\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0340\n",
      "     93        1.0000  0.0340\n",
      "     94        1.0000  0.0340\n",
      "     95        1.0000  0.0330\n",
      "     96        1.0000  0.0340\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0330\n",
      "     99        1.0000  0.0340\n",
      "    100        1.0000  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0340\n",
      "      4        1.0000  0.0330\n",
      "      5        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        1.0000  0.0340\n",
      "      7        1.0000  0.0330\n",
      "      8        1.0000  0.0340\n",
      "      9        1.0000  0.0360\n",
      "     10        1.0000  0.0340\n",
      "     11        1.0000  0.0330\n",
      "     12        1.0000  0.0340\n",
      "     13        1.0000  0.0330\n",
      "     14        1.0000  0.0340\n",
      "     15        1.0000  0.0340\n",
      "     16        1.0000  0.0340\n",
      "     17        1.0000  0.0330\n",
      "     18        1.0000  0.0340\n",
      "     19        1.0000  0.0340\n",
      "     20        1.0000  0.0340\n",
      "     21        1.0000  0.0330\n",
      "     22        1.0000  0.0340\n",
      "     23        1.0000  0.0340\n",
      "     24        1.0000  0.0340\n",
      "     25        1.0000  0.0330\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0330\n",
      "     28        1.0000  0.0340\n",
      "     29        1.0000  0.0320\n",
      "     30        1.0000  0.0340\n",
      "     31        1.0000  0.0330\n",
      "     32        1.0000  0.0350\n",
      "     33        1.0000  0.0330\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0340\n",
      "     36        1.0000  0.0330\n",
      "     37        1.0000  0.0340\n",
      "     38        1.0000  0.0330\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0340\n",
      "     41        1.0000  0.0330\n",
      "     42        1.0000  0.0340\n",
      "     43        1.0000  0.0340\n",
      "     44        1.0000  0.0330\n",
      "     45        1.0000  0.0340\n",
      "     46        1.0000  0.0340\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0340\n",
      "     49        1.0000  0.0330\n",
      "     50        1.0000  0.0340\n",
      "     51        1.0000  0.0350\n",
      "     52        1.0000  0.0330\n",
      "     53        1.0000  0.0360\n",
      "     54        1.0000  0.0340\n",
      "     55        1.0000  0.0330\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0330\n",
      "     58        1.0000  0.0340\n",
      "     59        1.0000  0.0340\n",
      "     60        1.0000  0.0330\n",
      "     61        1.0000  0.0340\n",
      "     62        1.0000  0.0340\n",
      "     63        1.0000  0.0340\n",
      "     64        1.0000  0.0330\n",
      "     65        1.0000  0.0340\n",
      "     66        1.0000  0.0330\n",
      "     67        1.0000  0.0340\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0330\n",
      "     71        1.0000  0.0330\n",
      "     72        1.0000  0.0340\n",
      "     73        1.0000  0.0330\n",
      "     74        1.0000  0.0340\n",
      "     75        1.0000  0.0330\n",
      "     76        1.0000  0.0340\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0340\n",
      "     79        1.0000  0.0340\n",
      "     80        1.0000  0.0340\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0340\n",
      "     83        1.0000  0.0330\n",
      "     84        1.0000  0.0340\n",
      "     85        1.0000  0.0340\n",
      "     86        1.0000  0.0340\n",
      "     87        1.0000  0.0350\n",
      "     88        1.0000  0.0340\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0340\n",
      "     93        1.0000  0.0330\n",
      "     94        1.0000  0.0340\n",
      "     95        1.0000  0.0330\n",
      "     96        1.0000  0.0340\n",
      "     97        1.0000  0.0350\n",
      "     98        1.0000  0.0340\n",
      "     99        1.0000  0.0330\n",
      "    100        1.0000  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0350\n",
      "      3        1.0000  0.0330\n",
      "      4        1.0000  0.0350\n",
      "      5        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        1.0000  0.0350\n",
      "      7        1.0000  0.0330\n",
      "      8        1.0000  0.0350\n",
      "      9        1.0000  0.0330\n",
      "     10        1.0000  0.0340\n",
      "     11        1.0000  0.0330\n",
      "     12        1.0000  0.0340\n",
      "     13        1.0000  0.0330\n",
      "     14        1.0000  0.0340\n",
      "     15        1.0000  0.0330\n",
      "     16        1.0000  0.0330\n",
      "     17        1.0000  0.0340\n",
      "     18        1.0000  0.0340\n",
      "     19        1.0000  0.0330\n",
      "     20        1.0000  0.0350\n",
      "     21        1.0000  0.0330\n",
      "     22        1.0000  0.0350\n",
      "     23        1.0000  0.0340\n",
      "     24        1.0000  0.0340\n",
      "     25        1.0000  0.0330\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0340\n",
      "     28        1.0000  0.0340\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0340\n",
      "     31        1.0000  0.0340\n",
      "     32        1.0000  0.0340\n",
      "     33        1.0000  0.0330\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0340\n",
      "     36        1.0000  0.0330\n",
      "     37        1.0000  0.0340\n",
      "     38        1.0000  0.0330\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0340\n",
      "     41        1.0000  0.0340\n",
      "     42        1.0000  0.0330\n",
      "     43        1.0000  0.0340\n",
      "     44        1.0000  0.0340\n",
      "     45        1.0000  0.0340\n",
      "     46        1.0000  0.0340\n",
      "     47        1.0000  0.0330\n",
      "     48        1.0000  0.0340\n",
      "     49        1.0000  0.0340\n",
      "     50        1.0000  0.0340\n",
      "     51        1.0000  0.0330\n",
      "     52        1.0000  0.0340\n",
      "     53        1.0000  0.0340\n",
      "     54        1.0000  0.0330\n",
      "     55        1.0000  0.0340\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0340\n",
      "     58        1.0000  0.0340\n",
      "     59        1.0000  0.0340\n",
      "     60        1.0000  0.0340\n",
      "     61        1.0000  0.0340\n",
      "     62        1.0000  0.0330\n",
      "     63        1.0000  0.0350\n",
      "     64        1.0000  0.0330\n",
      "     65        1.0000  0.0340\n",
      "     66        1.0000  0.0350\n",
      "     67        1.0000  0.0350\n",
      "     68        1.0000  0.0330\n",
      "     69        1.0000  0.0340\n",
      "     70        1.0000  0.0330\n",
      "     71        1.0000  0.0330\n",
      "     72        1.0000  0.0340\n",
      "     73        1.0000  0.0340\n",
      "     74        1.0000  0.0330\n",
      "     75        1.0000  0.0340\n",
      "     76        1.0000  0.0330\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0340\n",
      "     79        1.0000  0.0340\n",
      "     80        1.0000  0.0330\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0340\n",
      "     83        1.0000  0.0340\n",
      "     84        1.0000  0.0330\n",
      "     85        1.0000  0.0350\n",
      "     86        1.0000  0.0340\n",
      "     87        1.0000  0.0340\n",
      "     88        1.0000  0.0340\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0340\n",
      "     93        1.0000  0.0340\n",
      "     94        1.0000  0.0340\n",
      "     95        1.0000  0.0340\n",
      "     96        1.0000  0.0340\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0350\n",
      "     99        1.0000  0.0370\n",
      "    100        1.0000  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9978\u001b[0m  0.0430\n",
      "      2        0.9978  0.0450\n",
      "      3        0.9978  0.0430\n",
      "      4        \u001b[36m0.9842\u001b[0m  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.9648\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.9648\u001b[0m  0.0440\n",
      "      7        0.9648  0.0440\n",
      "      8        0.9648  0.0430\n",
      "      9        0.9648  0.0460\n",
      "     10        \u001b[36m0.9648\u001b[0m  0.0430\n",
      "     11        0.9648  0.0430\n",
      "     12        \u001b[36m0.9648\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.9626\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.9626\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.9626\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.9626\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.9626\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.9626\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.9604\u001b[0m  0.0440\n",
      "     20        \u001b[36m0.9604\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.9582\u001b[0m  0.0430\n",
      "     22        0.9582  0.0430\n",
      "     23        0.9582  0.0430\n",
      "     24        0.9582  0.0430\n",
      "     25        0.9582  0.0430\n",
      "     26        \u001b[36m0.9582\u001b[0m  0.0470\n",
      "     27        \u001b[36m0.9560\u001b[0m  0.0430\n",
      "     28        0.9582  0.0440\n",
      "     29        0.9582  0.0430\n",
      "     30        0.9582  0.0430\n",
      "     31        0.9582  0.0440\n",
      "     32        0.9582  0.0430\n",
      "     33        0.9582  0.0450\n",
      "     34        0.9582  0.0440\n",
      "     35        0.9582  0.0430\n",
      "     36        0.9582  0.0440\n",
      "     37        0.9582  0.0440\n",
      "     38        \u001b[36m0.9114\u001b[0m  0.0440\n",
      "     39        \u001b[36m0.7770\u001b[0m  0.0440\n",
      "     40        \u001b[36m0.7164\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.6593\u001b[0m  0.0430\n",
      "     42        \u001b[36m0.6527\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.6461\u001b[0m  0.0420\n",
      "     44        \u001b[36m0.5561\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.4872\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     54        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     60        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     63        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.4857\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.4857\u001b[0m  0.0430\n",
      "     66        \u001b[36m0.4793\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.4484\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.4242\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.4220\u001b[0m  0.0430\n",
      "     70        \u001b[36m0.4220\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.4133\u001b[0m  0.0430\n",
      "     72        \u001b[36m0.4044\u001b[0m  0.0430\n",
      "     73        0.4044  0.0440\n",
      "     74        0.4044  0.0480\n",
      "     75        0.4044  0.0450\n",
      "     76        0.4044  0.0450\n",
      "     77        0.4044  0.0440\n",
      "     78        0.4044  0.0430\n",
      "     79        0.4044  0.0430\n",
      "     80        0.4044  0.0440\n",
      "     81        0.4044  0.0440\n",
      "     82        0.4044  0.0440\n",
      "     83        0.4044  0.0430\n",
      "     84        0.4044  0.0440\n",
      "     85        0.4044  0.0430\n",
      "     86        0.4044  0.0440\n",
      "     87        0.4044  0.0450\n",
      "     88        0.4044  0.0440\n",
      "     89        0.4044  0.0440\n",
      "     90        0.4044  0.0430\n",
      "     91        0.4044  0.0430\n",
      "     92        0.4044  0.0480\n",
      "     93        0.4044  0.0440\n",
      "     94        0.4044  0.0430\n",
      "     95        0.4044  0.0440\n",
      "     96        0.4044  0.0440\n",
      "     97        0.4044  0.0440\n",
      "     98        0.4044  0.0430\n",
      "     99        0.4044  0.0450\n",
      "    100        0.4044  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9385\u001b[0m  0.0440\n",
      "      2        0.9385  0.0440\n",
      "      3        \u001b[36m0.9385\u001b[0m  0.0440\n",
      "      4        0.9407  0.0440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      5        \u001b[36m0.9341\u001b[0m  0.0430\n",
      "      6        \u001b[36m0.9143\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.9121\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.9033\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.8989\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.8945\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.8901\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.8791\u001b[0m  0.0430\n",
      "     13        0.8791  0.0430\n",
      "     14        0.8791  0.0430\n",
      "     15        \u001b[36m0.8769\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.8725\u001b[0m  0.0430\n",
      "     17        0.8725  0.0430\n",
      "     18        \u001b[36m0.8681\u001b[0m  0.0440\n",
      "     19        0.8681  0.0430\n",
      "     20        0.8681  0.0430\n",
      "     21        0.8681  0.0430\n",
      "     22        0.8681  0.0430\n",
      "     23        \u001b[36m0.8637\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.8615\u001b[0m  0.0440\n",
      "     25        0.8615  0.0430\n",
      "     26        0.8615  0.0440\n",
      "     27        0.8615  0.0430\n",
      "     28        0.8617  0.0430\n",
      "     29        \u001b[36m0.8593\u001b[0m  0.0440\n",
      "     30        0.8593  0.0430\n",
      "     31        0.8593  0.0440\n",
      "     32        0.8593  0.0450\n",
      "     33        0.8593  0.0430\n",
      "     34        0.8593  0.0430\n",
      "     35        0.8593  0.0430\n",
      "     36        0.8593  0.0430\n",
      "     37        0.8593  0.0430\n",
      "     38        0.8593  0.0430\n",
      "     39        0.8593  0.0430\n",
      "     40        0.8593  0.0430\n",
      "     41        0.8593  0.0440\n",
      "     42        0.8593  0.0440\n",
      "     43        0.8593  0.0430\n",
      "     44        0.8593  0.0450\n",
      "     45        0.8593  0.0430\n",
      "     46        0.8593  0.0440\n",
      "     47        0.8593  0.0430\n",
      "     48        0.8593  0.0440\n",
      "     49        0.8593  0.0430\n",
      "     50        0.8593  0.0440\n",
      "     51        0.8593  0.0440\n",
      "     52        0.8593  0.0440\n",
      "     53        0.8593  0.0440\n",
      "     54        0.8615  0.0430\n",
      "     55        0.8615  0.0450\n",
      "     56        0.8615  0.0430\n",
      "     57        0.8615  0.0440\n",
      "     58        0.8615  0.0430\n",
      "     59        0.8615  0.0430\n",
      "     60        0.8615  0.0440\n",
      "     61        0.8615  0.0430\n",
      "     62        0.8593  0.0430\n",
      "     63        \u001b[36m0.8570\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.8484\u001b[0m  0.0440\n",
      "     65        0.8484  0.0440\n",
      "     66        0.8484  0.0440\n",
      "     67        0.8484  0.0440\n",
      "     68        0.8505  0.0430\n",
      "     69        0.8505  0.0440\n",
      "     70        0.8505  0.0430\n",
      "     71        0.8505  0.0440\n",
      "     72        \u001b[36m0.8353\u001b[0m  0.0430\n",
      "     73        \u001b[36m0.5582\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3978\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3978\u001b[0m  0.0440\n",
      "     76        0.3978  0.0430\n",
      "     77        0.3978  0.0450\n",
      "     78        0.3978  0.0440\n",
      "     79        0.3978  0.0430\n",
      "     80        0.3978  0.0440\n",
      "     81        0.3978  0.0430\n",
      "     82        0.3978  0.0430\n",
      "     83        0.3978  0.0440\n",
      "     84        0.3978  0.0430\n",
      "     85        0.3978  0.0430\n",
      "     86        0.3978  0.0430\n",
      "     87        0.3978  0.0450\n",
      "     88        0.3978  0.0430\n",
      "     89        0.3978  0.0440\n",
      "     90        0.3978  0.0430\n",
      "     91        0.3978  0.0430\n",
      "     92        0.3978  0.0480\n",
      "     93        0.3978  0.0430\n",
      "     94        0.3978  0.0440\n",
      "     95        0.3978  0.0440\n",
      "     96        0.3978  0.0430\n",
      "     97        0.3978  0.0440\n",
      "     98        0.3978  0.0440\n",
      "     99        0.3978  0.0440\n",
      "    100        0.3978  0.0450\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9495\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.9407\u001b[0m  0.0430\n",
      "      3        0.9473  0.0430\n",
      "      4        0.9495  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.9495  0.0430\n",
      "      6        0.9495  0.0440\n",
      "      7        0.9516  0.0440\n",
      "      8        0.9516  0.0430\n",
      "      9        0.9538  0.0430\n",
      "     10        0.9560  0.0440\n",
      "     11        0.9560  0.0440\n",
      "     12        0.9560  0.0440\n",
      "     13        0.9560  0.0420\n",
      "     14        0.9560  0.0440\n",
      "     15        0.9560  0.0430\n",
      "     16        0.9560  0.0440\n",
      "     17        0.9560  0.0430\n",
      "     18        0.9560  0.0430\n",
      "     19        0.9560  0.0430\n",
      "     20        0.9560  0.0440\n",
      "     21        0.9560  0.0420\n",
      "     22        0.9560  0.0450\n",
      "     23        0.9537  0.0440\n",
      "     24        \u001b[36m0.6310\u001b[0m  0.0440\n",
      "     25        \u001b[36m0.4529\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.4268\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.4242\u001b[0m  0.0430\n",
      "     28        0.4242  0.0430\n",
      "     29        0.4242  0.0430\n",
      "     30        0.4242  0.0430\n",
      "     31        0.4242  0.0430\n",
      "     32        0.4242  0.0430\n",
      "     33        0.4242  0.0430\n",
      "     34        \u001b[36m0.4242\u001b[0m  0.0450\n",
      "     35        0.4242  0.0430\n",
      "     36        0.4242  0.0430\n",
      "     37        0.4242  0.0430\n",
      "     38        \u001b[36m0.4166\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.4132\u001b[0m  0.0430\n",
      "     40        0.4132  0.0430\n",
      "     41        0.4132  0.0430\n",
      "     42        0.4132  0.0430\n",
      "     43        0.4132  0.0440\n",
      "     44        0.4132  0.0440\n",
      "     45        \u001b[36m0.4132\u001b[0m  0.0450\n",
      "     46        \u001b[36m0.4132\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.4132\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.4110\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.4058\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     55        \u001b[36m0.3912\u001b[0m  0.0450\n",
      "     56        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3912\u001b[0m  0.0450\n",
      "     59        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     61        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     63        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     70        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3912\u001b[0m  0.0450\n",
      "     73        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     77        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     79        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     80        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     81        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     82        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     83        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     84        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     87        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     88        \u001b[36m0.3912\u001b[0m  0.0450\n",
      "     89        \u001b[36m0.3912\u001b[0m  0.0470\n",
      "     90        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     91        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     93        \u001b[36m0.3912\u001b[0m  0.0470\n",
      "     94        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     96        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     97        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "     98        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     99        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "    100        \u001b[36m0.3912\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "      2        0.3736  0.0430\n",
      "      3        0.3736  0.0430\n",
      "      4        0.3802  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.3802  0.0440\n",
      "      6        0.3824  0.0440\n",
      "      7        0.3736  0.0430\n",
      "      8        0.3736  0.0440\n",
      "      9        0.3736  0.0430\n",
      "     10        0.3736  0.0430\n",
      "     11        0.3736  0.0430\n",
      "     12        0.3736  0.0430\n",
      "     13        0.3736  0.0440\n",
      "     14        0.3744  0.0430\n",
      "     15        0.3736  0.0430\n",
      "     16        0.3736  0.0430\n",
      "     17        0.3736  0.0430\n",
      "     18        0.3736  0.0440\n",
      "     19        0.3736  0.0420\n",
      "     20        0.3736  0.0440\n",
      "     21        0.3736  0.0440\n",
      "     22        0.3736  0.0440\n",
      "     23        0.3736  0.0440\n",
      "     24        0.3736  0.0430\n",
      "     25        0.3758  0.0450\n",
      "     26        0.3758  0.0430\n",
      "     27        0.3758  0.0430\n",
      "     28        0.3758  0.0420\n",
      "     29        0.3758  0.0440\n",
      "     30        0.3824  0.0440\n",
      "     31        0.3868  0.0440\n",
      "     32        0.3758  0.0430\n",
      "     33        0.3758  0.0440\n",
      "     34        0.3758  0.0430\n",
      "     35        0.3758  0.0430\n",
      "     36        0.3758  0.0440\n",
      "     37        0.3758  0.0430\n",
      "     38        0.3758  0.0430\n",
      "     39        0.3758  0.0430\n",
      "     40        0.3758  0.0430\n",
      "     41        0.3758  0.0430\n",
      "     42        0.3758  0.0430\n",
      "     43        0.3758  0.0430\n",
      "     44        0.3758  0.0430\n",
      "     45        0.3758  0.0430\n",
      "     46        0.3758  0.0440\n",
      "     47        0.3758  0.0440\n",
      "     48        0.3758  0.0440\n",
      "     49        0.3758  0.0430\n",
      "     50        0.3758  0.0430\n",
      "     51        0.3758  0.0430\n",
      "     52        0.3758  0.0430\n",
      "     53        0.3758  0.0430\n",
      "     54        0.3758  0.0440\n",
      "     55        0.3758  0.0430\n",
      "     56        0.3758  0.0440\n",
      "     57        0.3758  0.0430\n",
      "     58        0.3758  0.0440\n",
      "     59        0.3758  0.0440\n",
      "     60        0.3758  0.0440\n",
      "     61        0.3758  0.0440\n",
      "     62        0.3758  0.0420\n",
      "     63        0.3758  0.0440\n",
      "     64        0.3758  0.0440\n",
      "     65        0.3758  0.0440\n",
      "     66        0.3758  0.0440\n",
      "     67        0.3758  0.0430\n",
      "     68        0.3758  0.0440\n",
      "     69        0.3758  0.0430\n",
      "     70        0.3758  0.0440\n",
      "     71        0.3758  0.0440\n",
      "     72        0.3758  0.0430\n",
      "     73        0.3758  0.0430\n",
      "     74        0.3758  0.0440\n",
      "     75        0.3758  0.0440\n",
      "     76        0.3758  0.0440\n",
      "     77        0.3758  0.0440\n",
      "     78        0.3758  0.0440\n",
      "     79        0.3758  0.0440\n",
      "     80        0.3758  0.0430\n",
      "     81        0.3758  0.0440\n",
      "     82        0.3758  0.0450\n",
      "     83        0.3758  0.0440\n",
      "     84        0.3758  0.0430\n",
      "     85        0.3758  0.0430\n",
      "     86        0.3758  0.0440\n",
      "     87        0.3758  0.0430\n",
      "     88        0.3758  0.0440\n",
      "     89        0.3758  0.0440\n",
      "     90        0.3758  0.0430\n",
      "     91        0.3758  0.0450\n",
      "     92        0.3758  0.0440\n",
      "     93        0.3758  0.0440\n",
      "     94        0.3758  0.0440\n",
      "     95        0.3758  0.0430\n",
      "     96        0.3758  0.0440\n",
      "     97        0.3758  0.0440\n",
      "     98        0.3758  0.0450\n",
      "     99        0.3758  0.0450\n",
      "    100        0.3758  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9715\u001b[0m  0.0430\n",
      "      2        0.9715  0.0440\n",
      "      3        0.9715  0.0430\n",
      "      4        0.9715  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.9715  0.0430\n",
      "      6        0.9715  0.0430\n",
      "      7        0.9715  0.0430\n",
      "      8        0.9715  0.0430\n",
      "      9        0.9715  0.0430\n",
      "     10        0.9715  0.0430\n",
      "     11        0.9715  0.0430\n",
      "     12        \u001b[36m0.9671\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.9539\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.9474\u001b[0m  0.0440\n",
      "     15        0.9474  0.0430\n",
      "     16        0.9474  0.0440\n",
      "     17        \u001b[36m0.9452\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.9430\u001b[0m  0.0420\n",
      "     19        0.9430  0.0430\n",
      "     20        0.9430  0.0430\n",
      "     21        \u001b[36m0.9328\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.4693\u001b[0m  0.0440\n",
      "     23        \u001b[36m0.3728\u001b[0m  0.0430\n",
      "     24        0.3728  0.0430\n",
      "     25        0.3728  0.0430\n",
      "     26        0.3728  0.0430\n",
      "     27        0.3728  0.0440\n",
      "     28        0.3728  0.0430\n",
      "     29        0.3728  0.0430\n",
      "     30        0.3728  0.0430\n",
      "     31        0.3728  0.0420\n",
      "     32        0.3728  0.0430\n",
      "     33        0.3728  0.0440\n",
      "     34        0.3728  0.0430\n",
      "     35        0.3728  0.0440\n",
      "     36        0.3728  0.0430\n",
      "     37        0.3728  0.0440\n",
      "     38        0.3728  0.0430\n",
      "     39        0.3728  0.0440\n",
      "     40        0.3728  0.0430\n",
      "     41        0.3728  0.0430\n",
      "     42        0.3728  0.0460\n",
      "     43        0.3728  0.0440\n",
      "     44        0.3728  0.0430\n",
      "     45        0.3728  0.0430\n",
      "     46        0.3728  0.0430\n",
      "     47        0.3728  0.0430\n",
      "     48        0.3728  0.0430\n",
      "     49        0.3728  0.0440\n",
      "     50        0.3728  0.0430\n",
      "     51        0.3728  0.0430\n",
      "     52        0.3728  0.0440\n",
      "     53        0.3728  0.0430\n",
      "     54        0.3728  0.0430\n",
      "     55        0.3728  0.0430\n",
      "     56        0.3728  0.0430\n",
      "     57        0.3728  0.0440\n",
      "     58        0.3728  0.0440\n",
      "     59        0.3728  0.0440\n",
      "     60        0.3728  0.0450\n",
      "     61        0.3728  0.0430\n",
      "     62        0.3728  0.0450\n",
      "     63        0.3728  0.0510\n",
      "     64        0.3728  0.0580\n",
      "     65        0.3728  0.0450\n",
      "     66        0.3728  0.0430\n",
      "     67        0.3728  0.0430\n",
      "     68        0.3728  0.0440\n",
      "     69        0.3728  0.0440\n",
      "     70        0.3728  0.0440\n",
      "     71        0.3728  0.0430\n",
      "     72        0.3728  0.0430\n",
      "     73        0.3728  0.0440\n",
      "     74        0.3728  0.0430\n",
      "     75        0.3728  0.0430\n",
      "     76        0.3728  0.0440\n",
      "     77        0.3728  0.0430\n",
      "     78        0.3728  0.0430\n",
      "     79        0.3728  0.0440\n",
      "     80        0.3728  0.0440\n",
      "     81        0.3728  0.0430\n",
      "     82        0.3728  0.0440\n",
      "     83        0.3728  0.0440\n",
      "     84        0.3728  0.0440\n",
      "     85        0.3728  0.0440\n",
      "     86        0.3728  0.0430\n",
      "     87        0.3728  0.0430\n",
      "     88        0.3728  0.0440\n",
      "     89        0.3728  0.0430\n",
      "     90        0.3728  0.0440\n",
      "     91        0.3728  0.0430\n",
      "     92        0.3728  0.0440\n",
      "     93        0.3728  0.0430\n",
      "     94        0.3728  0.0430\n",
      "     95        0.3728  0.0440\n",
      "     96        0.3728  0.0440\n",
      "     97        0.3728  0.0440\n",
      "     98        0.3728  0.0430\n",
      "     99        0.3728  0.0430\n",
      "    100        0.3728  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9933\u001b[0m  0.0390\n",
      "      2        \u001b[36m0.9933\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.9922\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.9911\u001b[0m  0.0330\n",
      "      5        \u001b[36m0.9910\u001b[0m  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.9901\u001b[0m  0.0350\n",
      "      7        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.9889\u001b[0m  0.0330\n",
      "      9        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.9889\u001b[0m  0.0330\n",
      "     11        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.9889\u001b[0m  0.0330\n",
      "     13        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.9889\u001b[0m  0.0350\n",
      "     15        \u001b[36m0.9889\u001b[0m  0.0330\n",
      "     16        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.9889\u001b[0m  0.0330\n",
      "     22        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.9889\u001b[0m  0.0350\n",
      "     24        \u001b[36m0.9889\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.9889\u001b[0m  0.0330\n",
      "     26        \u001b[36m0.9889\u001b[0m  0.0330\n",
      "     27        \u001b[36m0.9888\u001b[0m  0.0330\n",
      "     28        \u001b[36m0.9886\u001b[0m  0.0330\n",
      "     29        \u001b[36m0.9872\u001b[0m  0.0330\n",
      "     30        \u001b[36m0.9836\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.9834\u001b[0m  0.0330\n",
      "     32        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     33        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.9833\u001b[0m  0.0350\n",
      "     38        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     39        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     41        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     47        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     48        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.9833\u001b[0m  0.0380\n",
      "     50        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.9833\u001b[0m  0.0350\n",
      "     53        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     54        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     61        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     63        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     66        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     67        \u001b[36m0.9833\u001b[0m  0.0350\n",
      "     68        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.9833\u001b[0m  0.0330\n",
      "     72        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.9832\u001b[0m  0.0330\n",
      "     79        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.9832\u001b[0m  0.0330\n",
      "     81        \u001b[36m0.9832\u001b[0m  0.0350\n",
      "     82        \u001b[36m0.9832\u001b[0m  0.0330\n",
      "     83        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.9832\u001b[0m  0.0330\n",
      "     85        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.9832\u001b[0m  0.0330\n",
      "     87        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.9832\u001b[0m  0.0370\n",
      "     90        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.9832\u001b[0m  0.0330\n",
      "     93        \u001b[36m0.9832\u001b[0m  0.0350\n",
      "     94        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     95        \u001b[36m0.9832\u001b[0m  0.0330\n",
      "     96        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.9832\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9582\u001b[0m  0.0340\n",
      "      2        0.9582  0.0340\n",
      "      3        0.9582  0.0340\n",
      "      4        0.9582  0.0330\n",
      "      5        0.9582  0.0330\n",
      "      6        0.9582  0.0340"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        0.9582  0.0330\n",
      "      8        0.9582  0.0340\n",
      "      9        0.9582  0.0340\n",
      "     10        0.9582  0.0340\n",
      "     11        0.9582  0.0330\n",
      "     12        0.9582  0.0330\n",
      "     13        0.9582  0.0330\n",
      "     14        0.9582  0.0340\n",
      "     15        0.9582  0.0330\n",
      "     16        0.9582  0.0340\n",
      "     17        0.9582  0.0330\n",
      "     18        0.9582  0.0340\n",
      "     19        0.9582  0.0330\n",
      "     20        0.9582  0.0330\n",
      "     21        0.9582  0.0330\n",
      "     22        0.9582  0.0340\n",
      "     23        0.9582  0.0340\n",
      "     24        0.9582  0.0330\n",
      "     25        0.9582  0.0340\n",
      "     26        0.9582  0.0330\n",
      "     27        0.9582  0.0330\n",
      "     28        0.9582  0.0330\n",
      "     29        0.9582  0.0340\n",
      "     30        0.9582  0.0340\n",
      "     31        0.9582  0.0330\n",
      "     32        0.9582  0.0340\n",
      "     33        0.9582  0.0340\n",
      "     34        0.9582  0.0330\n",
      "     35        0.9582  0.0340\n",
      "     36        0.9582  0.0340\n",
      "     37        0.9582  0.0330\n",
      "     38        0.9582  0.0330\n",
      "     39        0.9582  0.0330\n",
      "     40        0.9582  0.0340\n",
      "     41        0.9582  0.0340\n",
      "     42        0.9582  0.0330\n",
      "     43        0.9582  0.0340\n",
      "     44        0.9582  0.0330\n",
      "     45        0.9582  0.0330\n",
      "     46        0.9582  0.0320\n",
      "     47        0.9582  0.0330\n",
      "     48        0.9582  0.0330\n",
      "     49        0.9582  0.0340\n",
      "     50        0.9582  0.0340\n",
      "     51        0.9582  0.0340\n",
      "     52        0.9582  0.0340\n",
      "     53        0.9582  0.0330\n",
      "     54        0.9582  0.0350\n",
      "     55        0.9582  0.0330\n",
      "     56        0.9582  0.0340\n",
      "     57        0.9582  0.0330\n",
      "     58        0.9582  0.0340\n",
      "     59        0.9582  0.0330\n",
      "     60        0.9582  0.0340\n",
      "     61        0.9582  0.0330\n",
      "     62        0.9582  0.0340\n",
      "     63        0.9582  0.0340\n",
      "     64        0.9582  0.0330\n",
      "     65        0.9582  0.0340\n",
      "     66        0.9582  0.0330\n",
      "     67        0.9582  0.0340\n",
      "     68        0.9582  0.0340\n",
      "     69        0.9582  0.0350\n",
      "     70        0.9582  0.0340\n",
      "     71        0.9582  0.0340\n",
      "     72        0.9582  0.0330\n",
      "     73        0.9582  0.0330\n",
      "     74        0.9582  0.0340\n",
      "     75        0.9582  0.0340\n",
      "     76        0.9582  0.0330\n",
      "     77        0.9582  0.0340\n",
      "     78        0.9582  0.0330\n",
      "     79        0.9582  0.0340\n",
      "     80        0.9582  0.0340\n",
      "     81        0.9582  0.0350\n",
      "     82        0.9582  0.0350\n",
      "     83        0.9582  0.0360\n",
      "     84        0.9582  0.0350\n",
      "     85        0.9582  0.0330\n",
      "     86        0.9582  0.0340\n",
      "     87        0.9582  0.0370\n",
      "     88        0.9582  0.0340\n",
      "     89        0.9582  0.0350\n",
      "     90        0.9582  0.0340\n",
      "     91        0.9582  0.0340\n",
      "     92        0.9582  0.0340\n",
      "     93        0.9582  0.0330\n",
      "     94        0.9582  0.0340\n",
      "     95        0.9582  0.0340\n",
      "     96        0.9582  0.0340\n",
      "     97        0.9582  0.0340\n",
      "     98        0.9582  0.0350\n",
      "     99        0.9582  0.0330\n",
      "    100        0.9582  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9604\u001b[0m  0.0330\n",
      "      2        0.9604  0.0330\n",
      "      3        0.9604  0.0340\n",
      "      4        0.9604  0.0340\n",
      "      5        0.9604  0.0330\n",
      "      6        0.9604  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        0.9604  0.0330\n",
      "      8        0.9604  0.0340\n",
      "      9        0.9604  0.0330\n",
      "     10        0.9604  0.0340\n",
      "     11        0.9604  0.0330\n",
      "     12        0.9604  0.0340\n",
      "     13        0.9604  0.0330\n",
      "     14        0.9604  0.0330\n",
      "     15        0.9604  0.0340\n",
      "     16        0.9604  0.0330\n",
      "     17        0.9604  0.0340\n",
      "     18        0.9604  0.0330\n",
      "     19        0.9604  0.0330\n",
      "     20        0.9604  0.0340\n",
      "     21        0.9604  0.0340\n",
      "     22        0.9604  0.0330\n",
      "     23        0.9604  0.0330\n",
      "     24        0.9604  0.0340\n",
      "     25        0.9604  0.0330\n",
      "     26        0.9604  0.0340\n",
      "     27        0.9604  0.0330\n",
      "     28        0.9604  0.0340\n",
      "     29        0.9604  0.0330\n",
      "     30        0.9604  0.0340\n",
      "     31        0.9604  0.0330\n",
      "     32        0.9604  0.0330\n",
      "     33        0.9604  0.0340\n",
      "     34        0.9604  0.0340\n",
      "     35        0.9604  0.0340\n",
      "     36        0.9604  0.0330\n",
      "     37        0.9604  0.0340\n",
      "     38        0.9604  0.0330\n",
      "     39        0.9604  0.0330\n",
      "     40        0.9604  0.0340\n",
      "     41        0.9604  0.0360\n",
      "     42        0.9604  0.0330\n",
      "     43        0.9604  0.0340\n",
      "     44        0.9604  0.0330\n",
      "     45        0.9604  0.0340\n",
      "     46        0.9604  0.0340\n",
      "     47        0.9604  0.0340\n",
      "     48        0.9604  0.0340\n",
      "     49        0.9604  0.0340\n",
      "     50        0.9604  0.0340\n",
      "     51        0.9604  0.0330\n",
      "     52        0.9604  0.0340\n",
      "     53        0.9604  0.0340\n",
      "     54        0.9604  0.0330\n",
      "     55        0.9604  0.0340\n",
      "     56        0.9604  0.0340\n",
      "     57        0.9604  0.0350\n",
      "     58        0.9604  0.0340\n",
      "     59        0.9604  0.0340\n",
      "     60        0.9604  0.0330\n",
      "     61        0.9604  0.0330\n",
      "     62        0.9604  0.0340\n",
      "     63        0.9604  0.0330\n",
      "     64        0.9604  0.0340\n",
      "     65        0.9604  0.0330\n",
      "     66        0.9604  0.0330\n",
      "     67        0.9604  0.0340\n",
      "     68        0.9604  0.0330\n",
      "     69        0.9604  0.0330\n",
      "     70        0.9604  0.0340\n",
      "     71        0.9604  0.0340\n",
      "     72        0.9604  0.0330\n",
      "     73        0.9604  0.0340\n",
      "     74        0.9604  0.0340\n",
      "     75        0.9604  0.0340\n",
      "     76        0.9604  0.0340\n",
      "     77        0.9604  0.0340\n",
      "     78        0.9604  0.0340\n",
      "     79        0.9604  0.0340\n",
      "     80        0.9604  0.0330\n",
      "     81        0.9604  0.0340\n",
      "     82        0.9604  0.0340\n",
      "     83        0.9604  0.0340\n",
      "     84        0.9604  0.0330\n",
      "     85        0.9604  0.0340\n",
      "     86        0.9604  0.0350\n",
      "     87        0.9604  0.0340\n",
      "     88        0.9604  0.0340\n",
      "     89        0.9604  0.0330\n",
      "     90        0.9604  0.0340\n",
      "     91        0.9604  0.0330\n",
      "     92        0.9604  0.0340\n",
      "     93        0.9604  0.0330\n",
      "     94        0.9604  0.0350\n",
      "     95        0.9604  0.0340\n",
      "     96        0.9604  0.0330\n",
      "     97        0.9604  0.0340\n",
      "     98        0.9604  0.0330\n",
      "     99        0.9604  0.0340\n",
      "    100        0.9604  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5275\u001b[0m  0.0330\n",
      "      2        0.5275  0.0340\n",
      "      3        0.5275  0.0330\n",
      "      4        0.5275  0.0340\n",
      "      5        0.5275  0.0340\n",
      "      6        0.5275  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        0.5275  0.0340\n",
      "      8        0.5275  0.0340\n",
      "      9        0.5275  0.0330\n",
      "     10        0.5275  0.0340\n",
      "     11        0.5275  0.0330\n",
      "     12        0.5275  0.0340\n",
      "     13        0.5275  0.0340\n",
      "     14        0.5275  0.0330\n",
      "     15        0.5275  0.0350\n",
      "     16        0.5275  0.0330\n",
      "     17        0.5275  0.0340\n",
      "     18        0.5275  0.0330\n",
      "     19        0.5275  0.0330\n",
      "     20        0.5275  0.0340\n",
      "     21        0.5275  0.0330\n",
      "     22        0.5275  0.0330\n",
      "     23        0.5275  0.0340\n",
      "     24        0.5275  0.0340\n",
      "     25        0.5275  0.0340\n",
      "     26        0.5275  0.0350\n",
      "     27        0.5275  0.0350\n",
      "     28        0.5275  0.0350\n",
      "     29        0.5275  0.0360\n",
      "     30        0.5275  0.0340\n",
      "     31        0.5275  0.0330\n",
      "     32        0.5275  0.0340\n",
      "     33        0.5275  0.0330\n",
      "     34        0.5275  0.0340\n",
      "     35        0.5275  0.0330\n",
      "     36        0.5275  0.0350\n",
      "     37        0.5275  0.0330\n",
      "     38        0.5275  0.0340\n",
      "     39        0.5275  0.0330\n",
      "     40        0.5275  0.0330\n",
      "     41        0.5275  0.0340\n",
      "     42        0.5275  0.0340\n",
      "     43        0.5275  0.0340\n",
      "     44        0.5275  0.0350\n",
      "     45        0.5275  0.0340\n",
      "     46        0.5275  0.0340\n",
      "     47        0.5275  0.0340\n",
      "     48        0.5275  0.0330\n",
      "     49        0.5275  0.0340\n",
      "     50        0.5275  0.0340\n",
      "     51        0.5275  0.0330\n",
      "     52        0.5275  0.0330\n",
      "     53        0.5275  0.0340\n",
      "     54        0.5275  0.0340\n",
      "     55        0.5275  0.0340\n",
      "     56        0.5275  0.0340\n",
      "     57        0.5275  0.0340\n",
      "     58        0.5275  0.0340\n",
      "     59        0.5275  0.0340\n",
      "     60        0.5275  0.0340\n",
      "     61        0.5275  0.0340\n",
      "     62        0.5275  0.0330\n",
      "     63        0.5275  0.0350\n",
      "     64        0.5275  0.0340\n",
      "     65        0.5275  0.0350\n",
      "     66        0.5275  0.0330\n",
      "     67        0.5275  0.0330\n",
      "     68        0.5275  0.0340\n",
      "     69        0.5275  0.0340\n",
      "     70        0.5275  0.0330\n",
      "     71        0.5275  0.0340\n",
      "     72        0.5275  0.0380\n",
      "     73        0.5275  0.0350\n",
      "     74        0.5275  0.0340\n",
      "     75        0.5275  0.0330\n",
      "     76        0.5275  0.0340\n",
      "     77        0.5275  0.0340\n",
      "     78        0.5275  0.0340\n",
      "     79        0.5275  0.0330\n",
      "     80        0.5275  0.0340\n",
      "     81        0.5275  0.0330\n",
      "     82        0.5275  0.0340\n",
      "     83        0.5275  0.0340\n",
      "     84        0.5275  0.0340\n",
      "     85        0.5275  0.0340\n",
      "     86        0.5275  0.0340\n",
      "     87        0.5275  0.0340\n",
      "     88        0.5275  0.0340\n",
      "     89        0.5275  0.0340\n",
      "     90        0.5275  0.0330\n",
      "     91        0.5275  0.0340\n",
      "     92        0.5275  0.0340\n",
      "     93        0.5275  0.0340\n",
      "     94        0.5275  0.0360\n",
      "     95        0.5275  0.0340\n",
      "     96        0.5275  0.0340\n",
      "     97        0.5275  0.0340\n",
      "     98        0.5275  0.0330\n",
      "     99        0.5275  0.0350\n",
      "    100        0.5275  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4742\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.3728\u001b[0m  0.0340\n",
      "      3        0.3728  0.0330\n",
      "      4        0.3728  0.0330\n",
      "      5        0.3728  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        0.3728  0.0340\n",
      "      7        0.3728  0.0340\n",
      "      8        0.3728  0.0330\n",
      "      9        0.3728  0.0330\n",
      "     10        0.3728  0.0340\n",
      "     11        0.3728  0.0350\n",
      "     12        0.3728  0.0350\n",
      "     13        0.3728  0.0330\n",
      "     14        0.3728  0.0340\n",
      "     15        0.3728  0.0340\n",
      "     16        0.3728  0.0330\n",
      "     17        0.3728  0.0340\n",
      "     18        0.3728  0.0340\n",
      "     19        0.3728  0.0340\n",
      "     20        0.3728  0.0340\n",
      "     21        0.3728  0.0340\n",
      "     22        0.3728  0.0340\n",
      "     23        0.3728  0.0340\n",
      "     24        0.3728  0.0330\n",
      "     25        0.3728  0.0340\n",
      "     26        0.3728  0.0340\n",
      "     27        0.3728  0.0330\n",
      "     28        0.3728  0.0330\n",
      "     29        0.3728  0.0340\n",
      "     30        0.3728  0.0330\n",
      "     31        0.3728  0.0350\n",
      "     32        0.3728  0.0340\n",
      "     33        0.3728  0.0330\n",
      "     34        0.3728  0.0330\n",
      "     35        0.3728  0.0340\n",
      "     36        0.3728  0.0340\n",
      "     37        0.3728  0.0330\n",
      "     38        0.3728  0.0340\n",
      "     39        0.3728  0.0330\n",
      "     40        0.3728  0.0340\n",
      "     41        0.3728  0.0340\n",
      "     42        0.3728  0.0330\n",
      "     43        0.3728  0.0340\n",
      "     44        0.3728  0.0340\n",
      "     45        0.3728  0.0340\n",
      "     46        0.3728  0.0330\n",
      "     47        0.3728  0.0330\n",
      "     48        0.3728  0.0340\n",
      "     49        0.3728  0.0340\n",
      "     50        0.3728  0.0340\n",
      "     51        0.3728  0.0330\n",
      "     52        0.3728  0.0340\n",
      "     53        0.3728  0.0330\n",
      "     54        0.3728  0.0330\n",
      "     55        0.3728  0.0340\n",
      "     56        0.3728  0.0330\n",
      "     57        0.3728  0.0340\n",
      "     58        0.3728  0.0340\n",
      "     59        0.3728  0.0340\n",
      "     60        0.3728  0.0340\n",
      "     61        0.3728  0.0340\n",
      "     62        0.3728  0.0340\n",
      "     63        0.3728  0.0340\n",
      "     64        0.3728  0.0340\n",
      "     65        0.3728  0.0330\n",
      "     66        0.3728  0.0340\n",
      "     67        0.3728  0.0340\n",
      "     68        0.3728  0.0330\n",
      "     69        0.3728  0.0340\n",
      "     70        0.3728  0.0340\n",
      "     71        0.3728  0.0330\n",
      "     72        0.3728  0.0340\n",
      "     73        0.3728  0.0340\n",
      "     74        0.3728  0.0340\n",
      "     75        0.3728  0.0340\n",
      "     76        0.3728  0.0330\n",
      "     77        0.3728  0.0340\n",
      "     78        0.3728  0.0340\n",
      "     79        0.3728  0.0340\n",
      "     80        0.3728  0.0340\n",
      "     81        0.3728  0.0350\n",
      "     82        0.3728  0.0340\n",
      "     83        0.3728  0.0330\n",
      "     84        0.3728  0.0340\n",
      "     85        0.3728  0.0340\n",
      "     86        0.3728  0.0330\n",
      "     87        0.3728  0.0340\n",
      "     88        0.3728  0.0340\n",
      "     89        0.3728  0.0340\n",
      "     90        0.3728  0.0330\n",
      "     91        0.3728  0.0340\n",
      "     92        0.3728  0.0330\n",
      "     93        0.3728  0.0340\n",
      "     94        0.3728  0.0330\n",
      "     95        0.3728  0.0340\n",
      "     96        0.3728  0.0340\n",
      "     97        0.3728  0.0330\n",
      "     98        0.3728  0.0340\n",
      "     99        0.3728  0.0330\n",
      "    100        0.3728  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3802\u001b[0m  0.0440\n",
      "      2        0.3868  0.0460\n",
      "      3        0.3912  0.0470\n",
      "      4        0.3934  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.4000  0.0440\n",
      "      6        0.4022  0.0440\n",
      "      7        0.4044  0.0430\n",
      "      8        0.4110  0.0450\n",
      "      9        0.4154  0.0440\n",
      "     10        0.4198  0.0440\n",
      "     11        0.4088  0.0430\n",
      "     12        0.3934  0.0440\n",
      "     13        0.3934  0.0430\n",
      "     14        0.3956  0.0440\n",
      "     15        0.3956  0.0440\n",
      "     16        0.3956  0.0440\n",
      "     17        0.3956  0.0440\n",
      "     18        0.3956  0.0430\n",
      "     19        0.3956  0.0430\n",
      "     20        0.3956  0.0430\n",
      "     21        0.3956  0.0430\n",
      "     22        0.3956  0.0440\n",
      "     23        0.3956  0.0430\n",
      "     24        0.3956  0.0440\n",
      "     25        0.3956  0.0430\n",
      "     26        0.3956  0.0440\n",
      "     27        0.3956  0.0440\n",
      "     28        0.3956  0.0440\n",
      "     29        0.3956  0.0440\n",
      "     30        0.3956  0.0430\n",
      "     31        0.3956  0.0450\n",
      "     32        0.3956  0.0430\n",
      "     33        0.3956  0.0440\n",
      "     34        0.3956  0.0430\n",
      "     35        0.3956  0.0440\n",
      "     36        0.3934  0.0450\n",
      "     37        0.3934  0.0450\n",
      "     38        0.3934  0.0450\n",
      "     39        0.3890  0.0440\n",
      "     40        0.3824  0.0440\n",
      "     41        0.3824  0.0440\n",
      "     42        0.3824  0.0440\n",
      "     43        \u001b[36m0.3780\u001b[0m  0.0440\n",
      "     44        0.3780  0.0440\n",
      "     45        0.3780  0.0440\n",
      "     46        0.3780  0.0450\n",
      "     47        0.3780  0.0440\n",
      "     48        0.3780  0.0450\n",
      "     49        0.3780  0.0440\n",
      "     50        0.3780  0.0440\n",
      "     51        0.3780  0.0440\n",
      "     52        0.3780  0.0470\n",
      "     53        0.3780  0.0460\n",
      "     54        0.3780  0.0440\n",
      "     55        0.3780  0.0450\n",
      "     56        0.3780  0.0440\n",
      "     57        0.3780  0.0450\n",
      "     58        0.3780  0.0500\n",
      "     59        0.3780  0.0450\n",
      "     60        0.3780  0.0460\n",
      "     61        0.3780  0.0440\n",
      "     62        0.3780  0.0450\n",
      "     63        \u001b[36m0.3758\u001b[0m  0.0440\n",
      "     64        0.3758  0.0440\n",
      "     65        0.3758  0.0450\n",
      "     66        0.3758  0.0440\n",
      "     67        \u001b[36m0.3751\u001b[0m  0.0450\n",
      "     68        \u001b[36m0.3714\u001b[0m  0.0450\n",
      "     69        0.3714  0.0440\n",
      "     70        0.3714  0.0450\n",
      "     71        0.3714  0.0450\n",
      "     72        0.3714  0.0450\n",
      "     73        0.3714  0.0450\n",
      "     74        0.3714  0.0450\n",
      "     75        0.3714  0.0460\n",
      "     76        0.3714  0.0450\n",
      "     77        0.3714  0.0450\n",
      "     78        0.3714  0.0440\n",
      "     79        0.3714  0.0450\n",
      "     80        0.3714  0.0450\n",
      "     81        0.3714  0.0450\n",
      "     82        0.3714  0.0450\n",
      "     83        0.3714  0.0450\n",
      "     84        0.3714  0.0450\n",
      "     85        0.3714  0.0450\n",
      "     86        0.3714  0.0450\n",
      "     87        0.3714  0.0450\n",
      "     88        0.3714  0.0450\n",
      "     89        0.3714  0.0450\n",
      "     90        0.3714  0.0460\n",
      "     91        0.3714  0.0460\n",
      "     92        0.3714  0.0450\n",
      "     93        0.3714  0.0450\n",
      "     94        0.3714  0.0460\n",
      "     95        0.3714  0.0450\n",
      "     96        0.3714  0.0470\n",
      "     97        0.3714  0.0460\n",
      "     98        0.3714  0.0460\n",
      "     99        0.3714  0.0460\n",
      "    100        0.3714  0.0450\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.8593\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.8220\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.7319\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.7297\u001b[0m  0.0440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      5        0.7319  0.0440\n",
      "      6        \u001b[36m0.5495\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4791\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.4769\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.4592\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.4242\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.4132\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.4044\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     14        0.3912  0.0440\n",
      "     15        \u001b[36m0.3846\u001b[0m  0.0430\n",
      "     16        0.3846  0.0440\n",
      "     17        \u001b[36m0.3802\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.3780\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3758\u001b[0m  0.0440\n",
      "     20        0.3758  0.0430\n",
      "     21        0.3758  0.0430\n",
      "     22        0.3758  0.0440\n",
      "     23        0.3758  0.0440\n",
      "     24        0.3758  0.0440\n",
      "     25        0.3758  0.0430\n",
      "     26        0.3758  0.0450\n",
      "     27        0.3758  0.0440\n",
      "     28        0.3758  0.0440\n",
      "     29        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     30        0.3736  0.0430\n",
      "     31        0.3736  0.0440\n",
      "     32        0.3736  0.0440\n",
      "     33        0.3736  0.0440\n",
      "     34        0.3736  0.0450\n",
      "     35        0.3736  0.0460\n",
      "     36        0.3736  0.0440\n",
      "     37        0.3736  0.0440\n",
      "     38        0.3736  0.0440\n",
      "     39        0.3736  0.0440\n",
      "     40        0.3736  0.0440\n",
      "     41        0.3736  0.0450\n",
      "     42        0.3736  0.0450\n",
      "     43        0.3736  0.0440\n",
      "     44        0.3736  0.0440\n",
      "     45        0.3736  0.0440\n",
      "     46        0.3736  0.0450\n",
      "     47        0.3736  0.0460\n",
      "     48        0.3736  0.0450\n",
      "     49        0.3736  0.0440\n",
      "     50        0.3736  0.0440\n",
      "     51        0.3736  0.0450\n",
      "     52        0.3736  0.0440\n",
      "     53        0.3736  0.0470\n",
      "     54        0.3736  0.0450\n",
      "     55        0.3736  0.0450\n",
      "     56        0.3736  0.0450\n",
      "     57        \u001b[36m0.3714\u001b[0m  0.0450\n",
      "     58        0.3714  0.0450\n",
      "     59        0.3714  0.0450\n",
      "     60        0.3714  0.0450\n",
      "     61        0.3714  0.0440\n",
      "     62        0.3714  0.0450\n",
      "     63        0.3714  0.0440\n",
      "     64        0.3714  0.0450\n",
      "     65        0.3714  0.0450\n",
      "     66        0.3714  0.0450\n",
      "     67        0.3714  0.0450\n",
      "     68        0.3714  0.0450\n",
      "     69        0.3714  0.0450\n",
      "     70        0.3714  0.0460\n",
      "     71        0.3714  0.0450\n",
      "     72        0.3714  0.0450\n",
      "     73        0.3714  0.0450\n",
      "     74        0.3714  0.0450\n",
      "     75        0.3714  0.0450\n",
      "     76        0.3714  0.0460\n",
      "     77        0.3714  0.0450\n",
      "     78        0.3714  0.0470\n",
      "     79        0.3714  0.0460\n",
      "     80        0.3714  0.0460\n",
      "     81        0.3714  0.0450\n",
      "     82        0.3714  0.0450\n",
      "     83        0.3714  0.0450\n",
      "     84        0.3714  0.0490\n",
      "     85        0.3736  0.0460\n",
      "     86        0.3736  0.0460\n",
      "     87        0.3736  0.0460\n",
      "     88        0.3736  0.0450\n",
      "     89        0.3736  0.0450\n",
      "     90        0.3736  0.0470\n",
      "     91        0.3736  0.0460\n",
      "     92        0.3736  0.0460\n",
      "     93        0.3736  0.0460\n",
      "     94        0.3736  0.0450\n",
      "     95        0.3736  0.0460\n",
      "     96        0.3736  0.0450\n",
      "     97        0.3736  0.0460\n",
      "     98        0.3736  0.0450\n",
      "     99        0.3736  0.0470\n",
      "    100        0.3736  0.0460\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7824\u001b[0m  0.0430\n",
      "      2        0.8022  0.0430\n",
      "      3        0.8088  0.0450\n",
      "      4        0.8066  0.0430"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      5        \u001b[36m0.7692\u001b[0m  0.0430\n",
      "      6        \u001b[36m0.7516\u001b[0m  0.0440\n",
      "      7        0.7516  0.0430\n",
      "      8        0.7604  0.0440\n",
      "      9        0.7626  0.0430\n",
      "     10        0.7626  0.0440\n",
      "     11        0.7648  0.0430\n",
      "     12        0.7670  0.0430\n",
      "     13        \u001b[36m0.7429\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.7407\u001b[0m  0.0440\n",
      "     15        0.7407  0.0430\n",
      "     16        0.7451  0.0430\n",
      "     17        0.7407  0.0440\n",
      "     18        \u001b[36m0.7407\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.5934\u001b[0m  0.0440\n",
      "     20        \u001b[36m0.5890\u001b[0m  0.0430\n",
      "     21        0.5890  0.0430\n",
      "     22        0.5890  0.0430\n",
      "     23        0.5890  0.0440\n",
      "     24        0.5890  0.0440\n",
      "     25        0.5890  0.0440\n",
      "     26        \u001b[36m0.5885\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3760\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     29        0.3736  0.0440\n",
      "     30        0.3736  0.0440\n",
      "     31        0.3736  0.0440\n",
      "     32        0.3736  0.0440\n",
      "     33        0.3736  0.0440\n",
      "     34        0.3736  0.0440\n",
      "     35        0.3736  0.0450\n",
      "     36        0.3736  0.0440\n",
      "     37        0.3736  0.0440\n",
      "     38        0.3736  0.0440\n",
      "     39        0.3736  0.0440\n",
      "     40        0.3736  0.0440\n",
      "     41        0.3736  0.0460\n",
      "     42        0.3736  0.0460\n",
      "     43        0.3736  0.0450\n",
      "     44        0.3736  0.0450\n",
      "     45        0.3736  0.0440\n",
      "     46        0.3736  0.0460\n",
      "     47        0.3736  0.0440\n",
      "     48        0.3736  0.0450\n",
      "     49        0.3736  0.0450\n",
      "     50        0.3736  0.0440\n",
      "     51        0.3736  0.0440\n",
      "     52        0.3736  0.0450\n",
      "     53        0.3736  0.0440\n",
      "     54        0.3736  0.0450\n",
      "     55        0.3736  0.0450\n",
      "     56        0.3736  0.0440\n",
      "     57        0.3736  0.0450\n",
      "     58        0.3736  0.0450\n",
      "     59        0.3736  0.0460\n",
      "     60        0.3736  0.0450\n",
      "     61        0.3736  0.0440\n",
      "     62        0.3736  0.0440\n",
      "     63        0.3736  0.0450\n",
      "     64        0.3736  0.0450\n",
      "     65        0.3736  0.0450\n",
      "     66        0.3736  0.0450\n",
      "     67        0.3736  0.0450\n",
      "     68        0.3736  0.0460\n",
      "     69        0.3736  0.0450\n",
      "     70        0.3736  0.0450\n",
      "     71        0.3736  0.0460\n",
      "     72        0.3736  0.0450\n",
      "     73        0.3736  0.0450\n",
      "     74        0.3736  0.0460\n",
      "     75        0.3736  0.0450\n",
      "     76        0.3736  0.0450\n",
      "     77        0.3736  0.0460\n",
      "     78        0.3736  0.0450\n",
      "     79        0.3736  0.0460\n",
      "     80        0.3736  0.0450\n",
      "     81        0.3736  0.0460\n",
      "     82        0.3736  0.0460\n",
      "     83        0.3736  0.0450\n",
      "     84        0.3736  0.0460\n",
      "     85        0.3736  0.0450\n",
      "     86        0.3736  0.0460\n",
      "     87        0.3736  0.0450\n",
      "     88        0.3736  0.0460\n",
      "     89        0.3736  0.0450\n",
      "     90        0.3736  0.0460\n",
      "     91        0.3736  0.0460\n",
      "     92        0.3736  0.0460\n",
      "     93        0.3736  0.0450\n",
      "     94        0.3736  0.0460\n",
      "     95        0.3736  0.0460\n",
      "     96        0.3736  0.0500\n",
      "     97        0.3736  0.0460\n",
      "     98        0.3736  0.0450\n",
      "     99        0.3736  0.0460\n",
      "    100        0.3736  0.0460\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.8110\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.8022\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.7934\u001b[0m  0.0450\n",
      "      4        0.8000  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.8000  0.0430\n",
      "      6        \u001b[36m0.7868\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.7363\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.7297\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.6427\u001b[0m  0.0430\n",
      "     10        \u001b[36m0.5078\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.4791\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.4747\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.4703\u001b[0m  0.0440\n",
      "     14        \u001b[36m0.4681\u001b[0m  0.0430\n",
      "     15        0.4681  0.0430\n",
      "     16        \u001b[36m0.4637\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.4615\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.4593\u001b[0m  0.0440\n",
      "     19        0.4593  0.0440\n",
      "     20        \u001b[36m0.4593\u001b[0m  0.0440\n",
      "     21        \u001b[36m0.4549\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.4527\u001b[0m  0.0430\n",
      "     23        0.4527  0.0440\n",
      "     24        0.4527  0.0430\n",
      "     25        0.4527  0.0450\n",
      "     26        0.4527  0.0430\n",
      "     27        0.4527  0.0430\n",
      "     28        0.4527  0.0430\n",
      "     29        \u001b[36m0.4505\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.4505\u001b[0m  0.0430\n",
      "     31        0.4505  0.0430\n",
      "     32        0.4505  0.0430\n",
      "     33        0.4505  0.0440\n",
      "     34        0.4505  0.0430\n",
      "     35        0.4505  0.0440\n",
      "     36        \u001b[36m0.4409\u001b[0m  0.0450\n",
      "     37        \u001b[36m0.3868\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3846\u001b[0m  0.0440\n",
      "     39        0.3846  0.0430\n",
      "     40        0.3846  0.0430\n",
      "     41        \u001b[36m0.3846\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3824\u001b[0m  0.0440\n",
      "     43        0.3824  0.0440\n",
      "     44        0.3824  0.0430\n",
      "     45        0.3824  0.0440\n",
      "     46        0.3824  0.0440\n",
      "     47        0.3824  0.0440\n",
      "     48        0.3824  0.0450\n",
      "     49        0.3824  0.0430\n",
      "     50        0.3824  0.0440\n",
      "     51        0.3824  0.0430\n",
      "     52        0.3824  0.0440\n",
      "     53        0.3824  0.0440\n",
      "     54        0.3824  0.0440\n",
      "     55        0.3824  0.0440\n",
      "     56        0.3824  0.0440\n",
      "     57        0.3824  0.0450\n",
      "     58        0.3824  0.0440\n",
      "     59        0.3824  0.0440\n",
      "     60        0.3824  0.0440\n",
      "     61        0.3824  0.0440\n",
      "     62        0.3824  0.0440\n",
      "     63        0.3824  0.0450\n",
      "     64        0.3824  0.0440\n",
      "     65        0.3824  0.0440\n",
      "     66        0.3824  0.0460\n",
      "     67        0.3824  0.0450\n",
      "     68        0.3824  0.0440\n",
      "     69        0.3824  0.0450\n",
      "     70        0.3824  0.0450\n",
      "     71        0.3824  0.0460\n",
      "     72        0.3824  0.0440\n",
      "     73        0.3824  0.0450\n",
      "     74        0.3824  0.0440\n",
      "     75        0.3824  0.0450\n",
      "     76        0.3824  0.0440\n",
      "     77        0.3824  0.0450\n",
      "     78        0.3824  0.0450\n",
      "     79        0.3824  0.0450\n",
      "     80        0.3824  0.0450\n",
      "     81        0.3824  0.0440\n",
      "     82        0.3824  0.0450\n",
      "     83        0.3824  0.0440\n",
      "     84        0.3824  0.0450\n",
      "     85        0.3824  0.0450\n",
      "     86        0.3824  0.0450\n",
      "     87        0.3824  0.0440\n",
      "     88        0.3824  0.0450\n",
      "     89        0.3824  0.0450\n",
      "     90        0.3824  0.0450\n",
      "     91        0.3824  0.0460\n",
      "     92        0.3824  0.0480\n",
      "     93        0.3824  0.0470\n",
      "     94        0.3824  0.0440\n",
      "     95        0.3824  0.0450\n",
      "     96        0.3824  0.0450\n",
      "     97        0.3824  0.0450\n",
      "     98        0.3824  0.0440\n",
      "     99        0.3824  0.0450\n",
      "    100        0.3824  0.0450\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3772\u001b[0m  0.0440\n",
      "      2        \u001b[36m0.3728\u001b[0m  0.0440\n",
      "      3        0.3728  0.0440\n",
      "      4        0.3728  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        0.3728  0.0430\n",
      "      6        0.3728  0.0440\n",
      "      7        0.3728  0.0430\n",
      "      8        0.3728  0.0440\n",
      "      9        0.3728  0.0430\n",
      "     10        0.3728  0.0440\n",
      "     11        0.3728  0.0440\n",
      "     12        0.3728  0.0430\n",
      "     13        0.3728  0.0430\n",
      "     14        0.3728  0.0430\n",
      "     15        0.3728  0.0440\n",
      "     16        0.3728  0.0440\n",
      "     17        0.3728  0.0440\n",
      "     18        0.3728  0.0440\n",
      "     19        0.3728  0.0430\n",
      "     20        0.3728  0.0430\n",
      "     21        0.3728  0.0430\n",
      "     22        0.3728  0.0430\n",
      "     23        0.3728  0.0430\n",
      "     24        0.3728  0.0450\n",
      "     25        0.3728  0.0440\n",
      "     26        0.3728  0.0440\n",
      "     27        0.3728  0.0430\n",
      "     28        0.3728  0.0430\n",
      "     29        0.3728  0.0440\n",
      "     30        0.3728  0.0430\n",
      "     31        0.3728  0.0440\n",
      "     32        0.3728  0.0440\n",
      "     33        0.3728  0.0440\n",
      "     34        0.3728  0.0440\n",
      "     35        0.3728  0.0440\n",
      "     36        0.3728  0.0440\n",
      "     37        0.3728  0.0450\n",
      "     38        0.3728  0.0440\n",
      "     39        0.3728  0.0430\n",
      "     40        0.3728  0.0440\n",
      "     41        0.3728  0.0440\n",
      "     42        0.3728  0.0440\n",
      "     43        0.3728  0.0440\n",
      "     44        0.3728  0.0440\n",
      "     45        0.3728  0.0450\n",
      "     46        0.3728  0.0440\n",
      "     47        0.3728  0.0440\n",
      "     48        0.3728  0.0450\n",
      "     49        0.3728  0.0440\n",
      "     50        0.3728  0.0440\n",
      "     51        0.3728  0.0440\n",
      "     52        0.3728  0.0440\n",
      "     53        0.3728  0.0440\n",
      "     54        0.3728  0.0440\n",
      "     55        0.3728  0.0450\n",
      "     56        0.3728  0.0430\n",
      "     57        0.3728  0.0440\n",
      "     58        0.3728  0.0440\n",
      "     59        0.3728  0.0440\n",
      "     60        0.3728  0.0450\n",
      "     61        0.3728  0.0440\n",
      "     62        0.3728  0.0440\n",
      "     63        0.3728  0.0440\n",
      "     64        0.3728  0.0440\n",
      "     65        0.3728  0.0440\n",
      "     66        0.3728  0.0440\n",
      "     67        0.3728  0.0440\n",
      "     68        0.3728  0.0450\n",
      "     69        0.3728  0.0450\n",
      "     70        0.3728  0.0440\n",
      "     71        0.3728  0.0440\n",
      "     72        0.3728  0.0450\n",
      "     73        0.3728  0.0430\n",
      "     74        0.3728  0.0430\n",
      "     75        0.3728  0.0450\n",
      "     76        0.3728  0.0450\n",
      "     77        0.3728  0.0440\n",
      "     78        0.3728  0.0440\n",
      "     79        0.3728  0.0440\n",
      "     80        0.3728  0.0450\n",
      "     81        0.3728  0.0450\n",
      "     82        0.3728  0.0440\n",
      "     83        0.3728  0.0440\n",
      "     84        0.3728  0.0440\n",
      "     85        0.3728  0.0440\n",
      "     86        0.3728  0.0440\n",
      "     87        0.3728  0.0440\n",
      "     88        0.3728  0.0440\n",
      "     89        0.3728  0.0440\n",
      "     90        0.3728  0.0450\n",
      "     91        0.3728  0.0440\n",
      "     92        0.3728  0.0460\n",
      "     93        0.3728  0.0440\n",
      "     94        0.3728  0.0440\n",
      "     95        0.3728  0.0440\n",
      "     96        0.3728  0.0450\n",
      "     97        0.3728  0.0440\n",
      "     98        0.3728  0.0450\n",
      "     99        0.3728  0.0450\n",
      "    100        0.3728  0.0450\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5802\u001b[0m  0.0330\n",
      "      2        0.5802  0.0340\n",
      "      3        0.5802  0.0330\n",
      "      4        0.5802  0.0340\n",
      "      5        0.5802  0.0340\n",
      "      6        0.5802  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        0.5802  0.0340\n",
      "      8        0.5802  0.0340\n",
      "      9        0.5802  0.0340\n",
      "     10        0.5802  0.0340\n",
      "     11        0.5802  0.0330\n",
      "     12        0.5802  0.0340\n",
      "     13        0.5802  0.0340\n",
      "     14        0.5802  0.0340\n",
      "     15        0.5802  0.0340\n",
      "     16        0.5802  0.0340\n",
      "     17        0.5802  0.0330\n",
      "     18        0.5802  0.0350\n",
      "     19        0.5802  0.0340\n",
      "     20        0.5802  0.0340\n",
      "     21        0.5802  0.0340\n",
      "     22        0.5802  0.0340\n",
      "     23        0.5802  0.0340\n",
      "     24        0.5802  0.0330\n",
      "     25        0.5802  0.0330\n",
      "     26        0.5802  0.0350\n",
      "     27        0.5802  0.0340\n",
      "     28        0.5802  0.0330\n",
      "     29        0.5802  0.0340\n",
      "     30        0.5802  0.0330\n",
      "     31        0.5802  0.0330\n",
      "     32        0.5802  0.0330\n",
      "     33        0.5802  0.0340\n",
      "     34        0.5802  0.0350\n",
      "     35        0.5802  0.0340\n",
      "     36        0.5802  0.0340\n",
      "     37        0.5802  0.0330\n",
      "     38        0.5802  0.0340\n",
      "     39        0.5802  0.0320\n",
      "     40        0.5802  0.0340\n",
      "     41        0.5802  0.0330\n",
      "     42        0.5802  0.0340\n",
      "     43        0.5802  0.0330\n",
      "     44        0.5802  0.0340\n",
      "     45        0.5802  0.0340\n",
      "     46        0.5802  0.0350\n",
      "     47        0.5802  0.0390\n",
      "     48        0.5802  0.0350\n",
      "     49        0.5802  0.0330\n",
      "     50        0.5802  0.0340\n",
      "     51        0.5802  0.0350\n",
      "     52        0.5802  0.0330\n",
      "     53        0.5802  0.0340\n",
      "     54        0.5802  0.0340\n",
      "     55        0.5802  0.0340\n",
      "     56        0.5802  0.0340\n",
      "     57        0.5802  0.0340\n",
      "     58        0.5802  0.0330\n",
      "     59        0.5802  0.0330\n",
      "     60        0.5802  0.0340\n",
      "     61        0.5802  0.0330\n",
      "     62        0.5802  0.0340\n",
      "     63        0.5802  0.0340\n",
      "     64        0.5802  0.0340\n",
      "     65        0.5802  0.0330\n",
      "     66        0.5802  0.0380\n",
      "     67        0.5802  0.0330\n",
      "     68        0.5802  0.0340\n",
      "     69        0.5802  0.0340\n",
      "     70        0.5802  0.0330\n",
      "     71        0.5802  0.0330\n",
      "     72        0.5802  0.0330\n",
      "     73        0.5802  0.0350\n",
      "     74        0.5802  0.0340\n",
      "     75        0.5802  0.0340\n",
      "     76        0.5802  0.0330\n",
      "     77        0.5802  0.0340\n",
      "     78        0.5802  0.0340\n",
      "     79        0.5802  0.0340\n",
      "     80        0.5802  0.0350\n",
      "     81        0.5802  0.0350\n",
      "     82        0.5802  0.0340\n",
      "     83        0.5802  0.0340\n",
      "     84        0.5802  0.0340\n",
      "     85        0.5802  0.0340\n",
      "     86        0.5802  0.0350\n",
      "     87        0.5802  0.0340\n",
      "     88        0.5802  0.0340\n",
      "     89        0.5802  0.0340\n",
      "     90        0.5802  0.0330\n",
      "     91        0.5802  0.0340\n",
      "     92        0.5802  0.0350\n",
      "     93        0.5802  0.0330\n",
      "     94        0.5802  0.0340\n",
      "     95        0.5802  0.0340\n",
      "     96        0.5802  0.0340\n",
      "     97        0.5802  0.0340\n",
      "     98        0.5802  0.0330\n",
      "     99        0.5802  0.0340\n",
      "    100        0.5802  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0350\n",
      "      4        1.0000  0.0340\n",
      "      5        1.0000  0.0340\n",
      "      6        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        1.0000  0.0340\n",
      "      8        1.0000  0.0330\n",
      "      9        1.0000  0.0340\n",
      "     10        1.0000  0.0330\n",
      "     11        1.0000  0.0340\n",
      "     12        1.0000  0.0330\n",
      "     13        1.0000  0.0350\n",
      "     14        1.0000  0.0360\n",
      "     15        1.0000  0.0340\n",
      "     16        1.0000  0.0350\n",
      "     17        1.0000  0.0340\n",
      "     18        1.0000  0.0340\n",
      "     19        1.0000  0.0330\n",
      "     20        1.0000  0.0330\n",
      "     21        1.0000  0.0350\n",
      "     22        1.0000  0.0340\n",
      "     23        1.0000  0.0330\n",
      "     24        1.0000  0.0340\n",
      "     25        1.0000  0.0350\n",
      "     26        1.0000  0.0330\n",
      "     27        1.0000  0.0340\n",
      "     28        1.0000  0.0340\n",
      "     29        1.0000  0.0330\n",
      "     30        1.0000  0.0340\n",
      "     31        1.0000  0.0340\n",
      "     32        1.0000  0.0340\n",
      "     33        1.0000  0.0340\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0330\n",
      "     36        1.0000  0.0340\n",
      "     37        1.0000  0.0330\n",
      "     38        1.0000  0.0350\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0340\n",
      "     41        1.0000  0.0340\n",
      "     42        1.0000  0.0340\n",
      "     43        1.0000  0.0340\n",
      "     44        1.0000  0.0330\n",
      "     45        1.0000  0.0350\n",
      "     46        1.0000  0.0340\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0350\n",
      "     49        1.0000  0.0330\n",
      "     50        1.0000  0.0350\n",
      "     51        1.0000  0.0330\n",
      "     52        1.0000  0.0340\n",
      "     53        1.0000  0.0340\n",
      "     54        1.0000  0.0340\n",
      "     55        1.0000  0.0330\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0330\n",
      "     58        1.0000  0.0330\n",
      "     59        1.0000  0.0330\n",
      "     60        1.0000  0.0340\n",
      "     61        1.0000  0.0340\n",
      "     62        1.0000  0.0340\n",
      "     63        1.0000  0.0340\n",
      "     64        1.0000  0.0340\n",
      "     65        1.0000  0.0340\n",
      "     66        1.0000  0.0330\n",
      "     67        1.0000  0.0340\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0330\n",
      "     70        1.0000  0.0340\n",
      "     71        1.0000  0.0340\n",
      "     72        1.0000  0.0350\n",
      "     73        1.0000  0.0340\n",
      "     74        1.0000  0.0340\n",
      "     75        1.0000  0.0340\n",
      "     76        1.0000  0.0330\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0330\n",
      "     79        1.0000  0.0350\n",
      "     80        1.0000  0.0330\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0340\n",
      "     83        1.0000  0.0340\n",
      "     84        1.0000  0.0340\n",
      "     85        1.0000  0.0340\n",
      "     86        1.0000  0.0330\n",
      "     87        1.0000  0.0340\n",
      "     88        1.0000  0.0340\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0340\n",
      "     93        1.0000  0.0330\n",
      "     94        1.0000  0.0340\n",
      "     95        1.0000  0.0340\n",
      "     96        1.0000  0.0330\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0340\n",
      "     99        1.0000  0.0330\n",
      "    100        1.0000  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4330\u001b[0m  0.0330\n",
      "      2        0.4330  0.0340\n",
      "      3        0.4330  0.0340\n",
      "      4        0.4330  0.0340\n",
      "      5        0.4330  0.0330\n",
      "      6        0.4330  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        0.4330  0.0330\n",
      "      8        0.4330  0.0350\n",
      "      9        0.4330  0.0340\n",
      "     10        0.4330  0.0330\n",
      "     11        0.4330  0.0340\n",
      "     12        0.4330  0.0330\n",
      "     13        0.4330  0.0340\n",
      "     14        0.4330  0.0330\n",
      "     15        0.4330  0.0340\n",
      "     16        0.4330  0.0330\n",
      "     17        0.4330  0.0340\n",
      "     18        0.4330  0.0330\n",
      "     19        0.4330  0.0350\n",
      "     20        0.4330  0.0340\n",
      "     21        0.4330  0.0340\n",
      "     22        0.4330  0.0350\n",
      "     23        0.4330  0.0330\n",
      "     24        0.4330  0.0340\n",
      "     25        0.4330  0.0330\n",
      "     26        0.4330  0.0340\n",
      "     27        0.4330  0.0340\n",
      "     28        0.4330  0.0330\n",
      "     29        0.4330  0.0340\n",
      "     30        0.4330  0.0330\n",
      "     31        0.4330  0.0340\n",
      "     32        0.4330  0.0340\n",
      "     33        0.4330  0.0340\n",
      "     34        0.4330  0.0340\n",
      "     35        0.4330  0.0340\n",
      "     36        0.4330  0.0340\n",
      "     37        0.4330  0.0350\n",
      "     38        0.4330  0.0330\n",
      "     39        0.4330  0.0340\n",
      "     40        0.4330  0.0340\n",
      "     41        0.4330  0.0340\n",
      "     42        0.4330  0.0340\n",
      "     43        0.4330  0.0340\n",
      "     44        0.4330  0.0340\n",
      "     45        0.4330  0.0330\n",
      "     46        0.4330  0.0340\n",
      "     47        0.4330  0.0340\n",
      "     48        0.4330  0.0380\n",
      "     49        0.4330  0.0340\n",
      "     50        0.4330  0.0340\n",
      "     51        0.4330  0.0340\n",
      "     52        0.4330  0.0330\n",
      "     53        0.4330  0.0340\n",
      "     54        0.4330  0.0330\n",
      "     55        0.4330  0.0340\n",
      "     56        0.4330  0.0340\n",
      "     57        0.4330  0.0340\n",
      "     58        0.4330  0.0350\n",
      "     59        0.4330  0.0330\n",
      "     60        0.4330  0.0350\n",
      "     61        0.4330  0.0340\n",
      "     62        0.4330  0.0330\n",
      "     63        0.4330  0.0330\n",
      "     64        0.4330  0.0340\n",
      "     65        0.4330  0.0340\n",
      "     66        0.4330  0.0340\n",
      "     67        0.4330  0.0340\n",
      "     68        0.4330  0.0330\n",
      "     69        0.4330  0.0340\n",
      "     70        0.4330  0.0340\n",
      "     71        0.4330  0.0340\n",
      "     72        0.4330  0.0330\n",
      "     73        0.4330  0.0340\n",
      "     74        0.4330  0.0330\n",
      "     75        0.4330  0.0340\n",
      "     76        0.4330  0.0340\n",
      "     77        0.4330  0.0340\n",
      "     78        0.4330  0.0360\n",
      "     79        0.4330  0.0330\n",
      "     80        0.4330  0.0340\n",
      "     81        0.4330  0.0330\n",
      "     82        0.4330  0.0340\n",
      "     83        0.4330  0.0340\n",
      "     84        0.4330  0.0350\n",
      "     85        0.4330  0.0340\n",
      "     86        0.4330  0.0340\n",
      "     87        0.4330  0.0340\n",
      "     88        0.4330  0.0340\n",
      "     89        0.4330  0.0340\n",
      "     90        0.4330  0.0340\n",
      "     91        0.4330  0.0330\n",
      "     92        0.4330  0.0330\n",
      "     93        0.4330  0.0340\n",
      "     94        0.4330  0.0340\n",
      "     95        0.4330  0.0350\n",
      "     96        0.4330  0.0340\n",
      "     97        0.4330  0.0330\n",
      "     98        0.4330  0.0330\n",
      "     99        0.4330  0.0350\n",
      "    100        0.4330  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m1.0000\u001b[0m  0.0330\n",
      "      2        1.0000  0.0340\n",
      "      3        1.0000  0.0330\n",
      "      4        1.0000  0.0330\n",
      "      5        1.0000  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        1.0000  0.0350\n",
      "      7        1.0000  0.0340\n",
      "      8        1.0000  0.0330\n",
      "      9        1.0000  0.0340\n",
      "     10        1.0000  0.0340\n",
      "     11        1.0000  0.0340\n",
      "     12        1.0000  0.0340\n",
      "     13        1.0000  0.0340\n",
      "     14        1.0000  0.0340\n",
      "     15        1.0000  0.0340\n",
      "     16        1.0000  0.0340\n",
      "     17        1.0000  0.0340\n",
      "     18        1.0000  0.0330\n",
      "     19        1.0000  0.0340\n",
      "     20        1.0000  0.0340\n",
      "     21        1.0000  0.0340\n",
      "     22        1.0000  0.0340\n",
      "     23        1.0000  0.0330\n",
      "     24        1.0000  0.0340\n",
      "     25        1.0000  0.0340\n",
      "     26        1.0000  0.0340\n",
      "     27        1.0000  0.0330\n",
      "     28        1.0000  0.0330\n",
      "     29        1.0000  0.0340\n",
      "     30        1.0000  0.0340\n",
      "     31        1.0000  0.0330\n",
      "     32        1.0000  0.0340\n",
      "     33        1.0000  0.0330\n",
      "     34        1.0000  0.0340\n",
      "     35        1.0000  0.0340\n",
      "     36        1.0000  0.0340\n",
      "     37        1.0000  0.0340\n",
      "     38        1.0000  0.0340\n",
      "     39        1.0000  0.0340\n",
      "     40        1.0000  0.0340\n",
      "     41        1.0000  0.0340\n",
      "     42        1.0000  0.0330\n",
      "     43        1.0000  0.0340\n",
      "     44        1.0000  0.0340\n",
      "     45        1.0000  0.0350\n",
      "     46        1.0000  0.0330\n",
      "     47        1.0000  0.0340\n",
      "     48        1.0000  0.0350\n",
      "     49        1.0000  0.0340\n",
      "     50        1.0000  0.0350\n",
      "     51        1.0000  0.0340\n",
      "     52        1.0000  0.0340\n",
      "     53        1.0000  0.0340\n",
      "     54        1.0000  0.0340\n",
      "     55        1.0000  0.0330\n",
      "     56        1.0000  0.0340\n",
      "     57        1.0000  0.0330\n",
      "     58        1.0000  0.0340\n",
      "     59        1.0000  0.0330\n",
      "     60        1.0000  0.0340\n",
      "     61        1.0000  0.0330\n",
      "     62        1.0000  0.0330\n",
      "     63        1.0000  0.0350\n",
      "     64        1.0000  0.0360\n",
      "     65        1.0000  0.0340\n",
      "     66        1.0000  0.0330\n",
      "     67        1.0000  0.0330\n",
      "     68        1.0000  0.0340\n",
      "     69        1.0000  0.0330\n",
      "     70        1.0000  0.0340\n",
      "     71        1.0000  0.0340\n",
      "     72        1.0000  0.0330\n",
      "     73        1.0000  0.0350\n",
      "     74        1.0000  0.0340\n",
      "     75        1.0000  0.0340\n",
      "     76        1.0000  0.0340\n",
      "     77        1.0000  0.0340\n",
      "     78        1.0000  0.0340\n",
      "     79        1.0000  0.0340\n",
      "     80        1.0000  0.0340\n",
      "     81        1.0000  0.0340\n",
      "     82        1.0000  0.0350\n",
      "     83        1.0000  0.0340\n",
      "     84        1.0000  0.0330\n",
      "     85        1.0000  0.0340\n",
      "     86        1.0000  0.0330\n",
      "     87        1.0000  0.0340\n",
      "     88        1.0000  0.0330\n",
      "     89        1.0000  0.0340\n",
      "     90        1.0000  0.0340\n",
      "     91        1.0000  0.0340\n",
      "     92        1.0000  0.0340\n",
      "     93        1.0000  0.0330\n",
      "     94        1.0000  0.0330\n",
      "     95        1.0000  0.0340\n",
      "     96        1.0000  0.0340\n",
      "     97        1.0000  0.0340\n",
      "     98        1.0000  0.0340\n",
      "     99        1.0000  0.0350\n",
      "    100        1.0000  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.3728\u001b[0m  0.0330\n",
      "      2        0.3728  0.0330\n",
      "      3        0.3728  0.0350\n",
      "      4        0.3728  0.0340\n",
      "      5        0.3728  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        0.3728  0.0430\n",
      "      7        0.3728  0.0360\n",
      "      8        0.3728  0.0330\n",
      "      9        0.3728  0.0330\n",
      "     10        0.3728  0.0340\n",
      "     11        0.3728  0.0330\n",
      "     12        0.3728  0.0340\n",
      "     13        0.3728  0.0330\n",
      "     14        0.3728  0.0340\n",
      "     15        0.3728  0.0340\n",
      "     16        0.3728  0.0340\n",
      "     17        0.3728  0.0340\n",
      "     18        0.3728  0.0340\n",
      "     19        0.3728  0.0330\n",
      "     20        0.3728  0.0340\n",
      "     21        0.3728  0.0340\n",
      "     22        0.3728  0.0360\n",
      "     23        0.3728  0.0340\n",
      "     24        0.3728  0.0330\n",
      "     25        0.3728  0.0340\n",
      "     26        0.3728  0.0340\n",
      "     27        0.3728  0.0340\n",
      "     28        0.3728  0.0330\n",
      "     29        0.3728  0.0340\n",
      "     30        0.3728  0.0330\n",
      "     31        0.3728  0.0340\n",
      "     32        0.3728  0.0350\n",
      "     33        0.3728  0.0330\n",
      "     34        0.3728  0.0380\n",
      "     35        0.3728  0.0340\n",
      "     36        0.3728  0.0340\n",
      "     37        0.3728  0.0340\n",
      "     38        0.3728  0.0340\n",
      "     39        0.3728  0.0340\n",
      "     40        0.3728  0.0340\n",
      "     41        0.3728  0.0330\n",
      "     42        0.3728  0.0330\n",
      "     43        0.3728  0.0340\n",
      "     44        0.3728  0.0330\n",
      "     45        0.3728  0.0340\n",
      "     46        0.3728  0.0330\n",
      "     47        0.3728  0.0340\n",
      "     48        0.3728  0.0330\n",
      "     49        0.3728  0.0340\n",
      "     50        0.3728  0.0340\n",
      "     51        0.3728  0.0350\n",
      "     52        0.3728  0.0340\n",
      "     53        0.3728  0.0340\n",
      "     54        0.3728  0.0340\n",
      "     55        0.3728  0.0340\n",
      "     56        0.3728  0.0330\n",
      "     57        0.3728  0.0340\n",
      "     58        0.3728  0.0340\n",
      "     59        0.3728  0.0340\n",
      "     60        0.3728  0.0340\n",
      "     61        0.3728  0.0340\n",
      "     62        0.3728  0.0330\n",
      "     63        0.3728  0.0340\n",
      "     64        0.3728  0.0340\n",
      "     65        0.3728  0.0340\n",
      "     66        0.3728  0.0330\n",
      "     67        0.3728  0.0340\n",
      "     68        0.3728  0.0340\n",
      "     69        0.3728  0.0330\n",
      "     70        0.3728  0.0350\n",
      "     71        0.3728  0.0330\n",
      "     72        0.3728  0.0340\n",
      "     73        0.3728  0.0330\n",
      "     74        0.3728  0.0340\n",
      "     75        0.3728  0.0340\n",
      "     76        0.3728  0.0340\n",
      "     77        0.3728  0.0340\n",
      "     78        0.3728  0.0330\n",
      "     79        0.3728  0.0340\n",
      "     80        0.3728  0.0340\n",
      "     81        0.3728  0.0340\n",
      "     82        0.3728  0.0330\n",
      "     83        0.3728  0.0350\n",
      "     84        0.3728  0.0330\n",
      "     85        0.3728  0.0340\n",
      "     86        0.3728  0.0340\n",
      "     87        0.3728  0.0340\n",
      "     88        0.3728  0.0340\n",
      "     89        0.3728  0.0350\n",
      "     90        0.3728  0.0340\n",
      "     91        0.3728  0.0330\n",
      "     92        0.3728  0.0340\n",
      "     93        0.3728  0.0340\n",
      "     94        0.3728  0.0340\n",
      "     95        0.3728  0.0340\n",
      "     96        0.3728  0.0340\n",
      "     97        0.3728  0.0330\n",
      "     98        0.3728  0.0340\n",
      "     99        0.3728  0.0340\n",
      "    100        0.3728  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9813\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.9708\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.9482\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.8734\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.6802\u001b[0m  0.0430\n",
      "      6        \u001b[36m0.5118\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4373\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.4124\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.4024\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.3965\u001b[0m  0.0420\n",
      "     11        \u001b[36m0.3925\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3895\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3872\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3853\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.3838\u001b[0m  0.0420\n",
      "     16        \u001b[36m0.3826\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3815\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.3806\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.3798\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3791\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3785\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3780\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3775\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3771\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3767\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3764\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3761\u001b[0m  0.0420\n",
      "     28        \u001b[36m0.3758\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3755\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3753\u001b[0m  0.0440\n",
      "     31        \u001b[36m0.3751\u001b[0m  0.0440\n",
      "     32        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     33        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     35        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     36        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     38        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     42        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3732\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3731\u001b[0m  0.0420\n",
      "     49        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "     51        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3728\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3728\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3727\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3727\u001b[0m  0.0450\n",
      "     57        \u001b[36m0.3726\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     59        \u001b[36m0.3726\u001b[0m  0.0480\n",
      "     60        \u001b[36m0.3725\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.3725\u001b[0m  0.0420\n",
      "     62        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3724\u001b[0m  0.0420\n",
      "     64        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3724\u001b[0m  0.0420\n",
      "     66        \u001b[36m0.3723\u001b[0m  0.0450\n",
      "     67        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.3723\u001b[0m  0.0439\n",
      "     70        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     72        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     73        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     80        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     81        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3720\u001b[0m  0.0450\n",
      "     83        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     84        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     87        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     88        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     90        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     91        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     94        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     95        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     96        \u001b[36m0.3718\u001b[0m  0.0460\n",
      "     97        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     98        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     99        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "    100        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9700\u001b[0m  0.0420\n",
      "      2        \u001b[36m0.9555\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.9295\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.8713\u001b[0m  0.0430"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      5        \u001b[36m0.7106\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.5202\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.4520\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.4273\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.4141\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.4056\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.3997\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3954\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.3921\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3895\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.3874\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.3857\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3842\u001b[0m  0.0420\n",
      "     18        \u001b[36m0.3830\u001b[0m  0.0420\n",
      "     19        \u001b[36m0.3820\u001b[0m  0.0440\n",
      "     20        \u001b[36m0.3811\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3803\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3796\u001b[0m  0.0460\n",
      "     23        \u001b[36m0.3790\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3784\u001b[0m  0.0440\n",
      "     25        \u001b[36m0.3780\u001b[0m  0.0440\n",
      "     26        \u001b[36m0.3775\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3771\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.3768\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3762\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3759\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3757\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3752\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     41        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3740\u001b[0m  0.0420\n",
      "     43        \u001b[36m0.3739\u001b[0m  0.0450\n",
      "     44        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     47        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     51        \u001b[36m0.3732\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3732\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3730\u001b[0m  0.0420\n",
      "     55        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3728\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3728\u001b[0m  0.0440\n",
      "     60        \u001b[36m0.3727\u001b[0m  0.0430\n",
      "     61        \u001b[36m0.3727\u001b[0m  0.0470\n",
      "     62        \u001b[36m0.3727\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3725\u001b[0m  0.0430\n",
      "     66        \u001b[36m0.3725\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.3725\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     70        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3724\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     73        \u001b[36m0.3723\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3723\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     78        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     80        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     81        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     84        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     88        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3720\u001b[0m  0.0420\n",
      "     90        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     92        \u001b[36m0.3720\u001b[0m  0.0450\n",
      "     93        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     95        \u001b[36m0.3719\u001b[0m  0.0500\n",
      "     96        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     98        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     99        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9831\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.9741\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.9567\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.9108\u001b[0m  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.7334\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.4908\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.4282\u001b[0m  0.0420\n",
      "      8        \u001b[36m0.4105\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.3982\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.3902\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.3873\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.3856\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3844\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3833\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3825\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.3817\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3811\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.3805\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.3800\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3795\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3791\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3788\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3785\u001b[0m  0.0440\n",
      "     24        \u001b[36m0.3782\u001b[0m  0.0420\n",
      "     25        \u001b[36m0.3779\u001b[0m  0.0440\n",
      "     26        \u001b[36m0.3777\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3774\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3772\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3770\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3769\u001b[0m  0.0420\n",
      "     31        \u001b[36m0.3767\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3764\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3763\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3762\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3760\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3759\u001b[0m  0.0450\n",
      "     38        \u001b[36m0.3758\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3757\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3757\u001b[0m  0.0430\n",
      "     41        \u001b[36m0.3756\u001b[0m  0.0450\n",
      "     42        \u001b[36m0.3755\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3753\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.3753\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3752\u001b[0m  0.0420\n",
      "     47        \u001b[36m0.3752\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3751\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3751\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     54        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     55        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     56        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     57        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     58        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     59        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3746\u001b[0m  0.0470\n",
      "     61        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     62        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     66        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     70        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     71        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     72        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     73        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     75        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     77        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3742\u001b[0m  0.0450\n",
      "     79        \u001b[36m0.3742\u001b[0m  0.0450\n",
      "     80        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     81        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     82        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     83        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     89        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     90        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     91        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     95        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     96        \u001b[36m0.3740\u001b[0m  0.0470\n",
      "     97        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     98        \u001b[36m0.3740\u001b[0m  0.0450\n",
      "     99        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9905\u001b[0m  0.0450\n",
      "      2        \u001b[36m0.9846\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.9684\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.8901\u001b[0m  0.0440\n",
      "      5        \u001b[36m0.6659\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.4932\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4304\u001b[0m  0.0440\n",
      "      8        \u001b[36m0.4044\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.3940\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.3899\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.3876\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3859\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3846\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3836\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3826\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.3819\u001b[0m  0.0440\n",
      "     17        \u001b[36m0.3812\u001b[0m  0.0440\n",
      "     18        \u001b[36m0.3806\u001b[0m  0.0420\n",
      "     19        \u001b[36m0.3801\u001b[0m  0.0440\n",
      "     20        \u001b[36m0.3796\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3792\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3788\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3785\u001b[0m  0.0440\n",
      "     24        \u001b[36m0.3782\u001b[0m  0.0440\n",
      "     25        \u001b[36m0.3779\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3777\u001b[0m  0.0450\n",
      "     27        \u001b[36m0.3775\u001b[0m  0.0460\n",
      "     28        \u001b[36m0.3772\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3771\u001b[0m  0.0440\n",
      "     30        \u001b[36m0.3769\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3767\u001b[0m  0.0440\n",
      "     32        \u001b[36m0.3766\u001b[0m  0.0440\n",
      "     33        \u001b[36m0.3764\u001b[0m  0.0440\n",
      "     34        \u001b[36m0.3763\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3762\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3760\u001b[0m  0.0440\n",
      "     37        \u001b[36m0.3759\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3758\u001b[0m  0.0440\n",
      "     39        \u001b[36m0.3757\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3757\u001b[0m  0.0430\n",
      "     41        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     42        \u001b[36m0.3755\u001b[0m  0.0480\n",
      "     43        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3753\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3753\u001b[0m  0.0440\n",
      "     46        \u001b[36m0.3752\u001b[0m  0.0440\n",
      "     47        \u001b[36m0.3752\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3751\u001b[0m  0.0440\n",
      "     49        \u001b[36m0.3750\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     54        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     56        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     58        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3746\u001b[0m  0.0440\n",
      "     60        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     61        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     62        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3744\u001b[0m  0.0450\n",
      "     70        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3743\u001b[0m  0.0450\n",
      "     72        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     73        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     75        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     79        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     80        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     81        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     84        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3741\u001b[0m  0.0470\n",
      "     88        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     90        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     92        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     93        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     94        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     96        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     98        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     99        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3740\u001b[0m  0.0450\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9655\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.9471\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.9139\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.8390\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.6679\u001b[0m  0.0430\n",
      "      6        \u001b[36m0.5101\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.4500\u001b[0m  0.0440\n",
      "      8        \u001b[36m0.4255\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.4137\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.4063\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.4011\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.3971\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3940\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3915\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.3895\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.3878\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3863\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.3851\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.3840\u001b[0m  0.0440\n",
      "     20        \u001b[36m0.3831\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3823\u001b[0m  0.0450\n",
      "     22        \u001b[36m0.3816\u001b[0m  0.0440\n",
      "     23        \u001b[36m0.3809\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3804\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3799\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3794\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3790\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3786\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3783\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3780\u001b[0m  0.0440\n",
      "     31        \u001b[36m0.3777\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3774\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3772\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3769\u001b[0m  0.0440\n",
      "     35        \u001b[36m0.3767\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3764\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3762\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3759\u001b[0m  0.0440\n",
      "     40        \u001b[36m0.3758\u001b[0m  0.0430\n",
      "     41        \u001b[36m0.3757\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3755\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3753\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3752\u001b[0m  0.0500\n",
      "     46        \u001b[36m0.3751\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3750\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3750\u001b[0m  0.0440\n",
      "     49        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3745\u001b[0m  0.0450\n",
      "     56        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.3742\u001b[0m  0.0450\n",
      "     62        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     63        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     66        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     70        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     72        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     73        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     75        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     77        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     78        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     79        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     80        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     81        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     83        \u001b[36m0.3735\u001b[0m  0.0450\n",
      "     84        \u001b[36m0.3735\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3735\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     89        \u001b[36m0.3734\u001b[0m  0.0480\n",
      "     90        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     91        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     92        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "     96        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "     97        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "     98        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     99        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "    100        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9856\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.9855\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.9854\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.9853\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.9852\u001b[0m  0.0340\n",
      "      6        \u001b[36m0.9852\u001b[0m  0.0330\n",
      "      7        \u001b[36m0.9851\u001b[0m  0.0360\n",
      "      8        \u001b[36m0.9850\u001b[0m  0.0330\n",
      "      9        \u001b[36m0.9849\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.9848\u001b[0m  0.0340\n",
      "     11        \u001b[36m0.9847\u001b[0m  0.0330\n",
      "     12        \u001b[36m0.9846\u001b[0m  0.0350\n",
      "     13        \u001b[36m0.9845\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.9844\u001b[0m  0.0330\n",
      "     15        \u001b[36m0.9843\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.9842\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.9841\u001b[0m  0.0350\n",
      "     18        \u001b[36m0.9841\u001b[0m  0.0330\n",
      "     19        \u001b[36m0.9840\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.9839\u001b[0m  0.0330\n",
      "     21        \u001b[36m0.9837\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.9836\u001b[0m  0.0330\n",
      "     23        \u001b[36m0.9835\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.9834\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.9833\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.9832\u001b[0m  0.0330\n",
      "     27        \u001b[36m0.9831\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.9830\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.9829\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.9828\u001b[0m  0.0330\n",
      "     31        \u001b[36m0.9827\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.9825\u001b[0m  0.0330\n",
      "     33        \u001b[36m0.9824\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.9823\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.9822\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.9821\u001b[0m  0.0350\n",
      "     37        \u001b[36m0.9819\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.9818\u001b[0m  0.0330\n",
      "     39        \u001b[36m0.9817\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.9815\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.9814\u001b[0m  0.0330\n",
      "     42        \u001b[36m0.9813\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.9811\u001b[0m  0.0330\n",
      "     44        \u001b[36m0.9810\u001b[0m  0.0330\n",
      "     45        \u001b[36m0.9809\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.9807\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.9806\u001b[0m  0.0380\n",
      "     48        \u001b[36m0.9804\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.9803\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.9801\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.9800\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.9798\u001b[0m  0.0330\n",
      "     53        \u001b[36m0.9797\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.9795\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.9793\u001b[0m  0.0330\n",
      "     56        \u001b[36m0.9792\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.9790\u001b[0m  0.0330\n",
      "     58        \u001b[36m0.9788\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.9787\u001b[0m  0.0330\n",
      "     60        \u001b[36m0.9785\u001b[0m  0.0340\n",
      "     61        \u001b[36m0.9783\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.9781\u001b[0m  0.0330\n",
      "     63        \u001b[36m0.9779\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.9778\u001b[0m  0.0330\n",
      "     65        \u001b[36m0.9776\u001b[0m  0.0360\n",
      "     66        \u001b[36m0.9774\u001b[0m  0.0330\n",
      "     67        \u001b[36m0.9772\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.9770\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.9768\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.9766\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.9764\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.9761\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.9759\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.9757\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.9755\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.9752\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.9750\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.9748\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.9745\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.9743\u001b[0m  0.0330\n",
      "     81        \u001b[36m0.9740\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.9738\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.9735\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.9733\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.9730\u001b[0m  0.0330\n",
      "     86        \u001b[36m0.9727\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.9724\u001b[0m  0.0380\n",
      "     88        \u001b[36m0.9721\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.9718\u001b[0m  0.0350\n",
      "     90        \u001b[36m0.9715\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.9712\u001b[0m  0.0330\n",
      "     92        \u001b[36m0.9709\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.9706\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.9703\u001b[0m  0.0340\n",
      "     95        \u001b[36m0.9700\u001b[0m  0.0330\n",
      "     96        \u001b[36m0.9696\u001b[0m  0.0330\n",
      "     97        \u001b[36m0.9693\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.9689\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.9686\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.9682\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9794\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.9793\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.9791\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.9789\u001b[0m  0.0330\n",
      "      5        \u001b[36m0.9788\u001b[0m  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.9786\u001b[0m  0.0340\n",
      "      7        \u001b[36m0.9784\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.9782\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.9780\u001b[0m  0.0330\n",
      "     10        \u001b[36m0.9779\u001b[0m  0.0340\n",
      "     11        \u001b[36m0.9777\u001b[0m  0.0330\n",
      "     12        \u001b[36m0.9775\u001b[0m  0.0340\n",
      "     13        \u001b[36m0.9773\u001b[0m  0.0330\n",
      "     14        \u001b[36m0.9771\u001b[0m  0.0330\n",
      "     15        \u001b[36m0.9769\u001b[0m  0.0330\n",
      "     16        \u001b[36m0.9767\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.9765\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.9763\u001b[0m  0.0330\n",
      "     19        \u001b[36m0.9760\u001b[0m  0.0330\n",
      "     20        \u001b[36m0.9758\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.9756\u001b[0m  0.0330\n",
      "     22        \u001b[36m0.9754\u001b[0m  0.0350\n",
      "     23        \u001b[36m0.9751\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.9749\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.9747\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.9744\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.9742\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.9739\u001b[0m  0.0350\n",
      "     29        \u001b[36m0.9736\u001b[0m  0.0330\n",
      "     30        \u001b[36m0.9734\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.9731\u001b[0m  0.0330\n",
      "     32        \u001b[36m0.9728\u001b[0m  0.0340\n",
      "     33        \u001b[36m0.9726\u001b[0m  0.0330\n",
      "     34        \u001b[36m0.9723\u001b[0m  0.0350\n",
      "     35        \u001b[36m0.9720\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.9717\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.9714\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.9711\u001b[0m  0.0390\n",
      "     39        \u001b[36m0.9708\u001b[0m  0.0380\n",
      "     40        \u001b[36m0.9704\u001b[0m  0.0380\n",
      "     41        \u001b[36m0.9701\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.9698\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.9694\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.9691\u001b[0m  0.0330\n",
      "     45        \u001b[36m0.9687\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.9683\u001b[0m  0.0330\n",
      "     47        \u001b[36m0.9680\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.9676\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.9672\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.9668\u001b[0m  0.0350\n",
      "     51        \u001b[36m0.9664\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.9660\u001b[0m  0.0340\n",
      "     53        \u001b[36m0.9655\u001b[0m  0.0330\n",
      "     54        \u001b[36m0.9651\u001b[0m  0.0350\n",
      "     55        \u001b[36m0.9646\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.9642\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.9637\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.9632\u001b[0m  0.0330\n",
      "     59        \u001b[36m0.9627\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.9622\u001b[0m  0.0340\n",
      "     61        \u001b[36m0.9617\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.9611\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.9606\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.9600\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.9594\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.9588\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.9582\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.9576\u001b[0m  0.0330\n",
      "     69        \u001b[36m0.9569\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.9563\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.9556\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.9549\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.9541\u001b[0m  0.0330\n",
      "     74        \u001b[36m0.9534\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.9526\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.9518\u001b[0m  0.0330\n",
      "     77        \u001b[36m0.9510\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.9501\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.9493\u001b[0m  0.0330\n",
      "     80        \u001b[36m0.9483\u001b[0m  0.0350\n",
      "     81        \u001b[36m0.9474\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.9464\u001b[0m  0.0330\n",
      "     83        \u001b[36m0.9454\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.9444\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.9433\u001b[0m  0.0330\n",
      "     86        \u001b[36m0.9422\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.9411\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.9399\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.9386\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.9374\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.9360\u001b[0m  0.0330\n",
      "     92        \u001b[36m0.9346\u001b[0m  0.0370\n",
      "     93        \u001b[36m0.9332\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.9317\u001b[0m  0.0340\n",
      "     95        \u001b[36m0.9302\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.9286\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.9269\u001b[0m  0.0330\n",
      "     98        \u001b[36m0.9251\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.9233\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.9214\u001b[0m  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9939\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.9938\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.9938\u001b[0m  0.0330\n",
      "      4        \u001b[36m0.9938\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.9938\u001b[0m  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.9938\u001b[0m  0.0350\n",
      "      7        \u001b[36m0.9938\u001b[0m  0.0330\n",
      "      8        \u001b[36m0.9937\u001b[0m  0.0350\n",
      "      9        \u001b[36m0.9937\u001b[0m  0.0330\n",
      "     10        \u001b[36m0.9937\u001b[0m  0.0340\n",
      "     11        \u001b[36m0.9937\u001b[0m  0.0330\n",
      "     12        \u001b[36m0.9937\u001b[0m  0.0340\n",
      "     13        \u001b[36m0.9937\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.9936\u001b[0m  0.0330\n",
      "     15        \u001b[36m0.9936\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.9936\u001b[0m  0.0330\n",
      "     17        \u001b[36m0.9936\u001b[0m  0.0330\n",
      "     18        \u001b[36m0.9936\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.9936\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.9935\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.9935\u001b[0m  0.0350\n",
      "     22        \u001b[36m0.9935\u001b[0m  0.0330\n",
      "     23        \u001b[36m0.9935\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.9935\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.9935\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.9934\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.9934\u001b[0m  0.0330\n",
      "     28        \u001b[36m0.9934\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.9934\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.9934\u001b[0m  0.0330\n",
      "     31        \u001b[36m0.9934\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.9933\u001b[0m  0.0340\n",
      "     33        \u001b[36m0.9933\u001b[0m  0.0330\n",
      "     34        \u001b[36m0.9933\u001b[0m  0.0340\n",
      "     35        \u001b[36m0.9933\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.9933\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.9933\u001b[0m  0.0330\n",
      "     38        \u001b[36m0.9932\u001b[0m  0.0350\n",
      "     39        \u001b[36m0.9932\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.9932\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.9932\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.9932\u001b[0m  0.0330\n",
      "     43        \u001b[36m0.9931\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.9931\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.9931\u001b[0m  0.0330\n",
      "     46        \u001b[36m0.9931\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.9931\u001b[0m  0.0330\n",
      "     48        \u001b[36m0.9930\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.9930\u001b[0m  0.0340\n",
      "     50        \u001b[36m0.9930\u001b[0m  0.0330\n",
      "     51        \u001b[36m0.9930\u001b[0m  0.0330\n",
      "     52        \u001b[36m0.9930\u001b[0m  0.0340\n",
      "     53        \u001b[36m0.9929\u001b[0m  0.0330\n",
      "     54        \u001b[36m0.9929\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.9929\u001b[0m  0.0330\n",
      "     56        \u001b[36m0.9929\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.9929\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.9928\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.9928\u001b[0m  0.0330\n",
      "     60        \u001b[36m0.9928\u001b[0m  0.0340\n",
      "     61        \u001b[36m0.9928\u001b[0m  0.0330\n",
      "     62        \u001b[36m0.9928\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.9927\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.9927\u001b[0m  0.0330\n",
      "     65        \u001b[36m0.9927\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.9927\u001b[0m  0.0330\n",
      "     67        \u001b[36m0.9927\u001b[0m  0.0350\n",
      "     68        \u001b[36m0.9926\u001b[0m  0.0330\n",
      "     69        \u001b[36m0.9926\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.9926\u001b[0m  0.0330\n",
      "     71        \u001b[36m0.9926\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.9925\u001b[0m  0.0330\n",
      "     73        \u001b[36m0.9925\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.9925\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.9925\u001b[0m  0.0370\n",
      "     76        \u001b[36m0.9924\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.9924\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.9924\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.9924\u001b[0m  0.0330\n",
      "     80        \u001b[36m0.9924\u001b[0m  0.0330\n",
      "     81        \u001b[36m0.9923\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.9923\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.9923\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.9923\u001b[0m  0.0330\n",
      "     85        \u001b[36m0.9922\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.9922\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.9922\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.9922\u001b[0m  0.0330\n",
      "     89        \u001b[36m0.9921\u001b[0m  0.0330\n",
      "     90        \u001b[36m0.9921\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.9921\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.9921\u001b[0m  0.0330\n",
      "     93        \u001b[36m0.9920\u001b[0m  0.0330\n",
      "     94        \u001b[36m0.9920\u001b[0m  0.0350\n",
      "     95        \u001b[36m0.9920\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.9920\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.9919\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.9919\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.9919\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.9919\u001b[0m  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9518\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.9510\u001b[0m  0.0330\n",
      "      3        \u001b[36m0.9501\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.9493\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.9484\u001b[0m  0.0330\n",
      "      6        \u001b[36m0.9474\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.9465\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.9455\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.9444\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.9434\u001b[0m  0.0330\n",
      "     11        \u001b[36m0.9423\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.9411\u001b[0m  0.0330\n",
      "     13        \u001b[36m0.9399\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.9387\u001b[0m  0.0330\n",
      "     15        \u001b[36m0.9374\u001b[0m  0.0350\n",
      "     16        \u001b[36m0.9361\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.9347\u001b[0m  0.0330\n",
      "     18        \u001b[36m0.9333\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.9318\u001b[0m  0.0380\n",
      "     20        \u001b[36m0.9303\u001b[0m  0.0330\n",
      "     21        \u001b[36m0.9287\u001b[0m  0.0330\n",
      "     22        \u001b[36m0.9270\u001b[0m  0.0350\n",
      "     23        \u001b[36m0.9253\u001b[0m  0.0330\n",
      "     24        \u001b[36m0.9235\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.9216\u001b[0m  0.0350\n",
      "     26        \u001b[36m0.9196\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.9176\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.9154\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.9132\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.9109\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.9084\u001b[0m  0.0330\n",
      "     32        \u001b[36m0.9059\u001b[0m  0.0330\n",
      "     33        \u001b[36m0.9032\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.9004\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.8975\u001b[0m  0.0350\n",
      "     36        \u001b[36m0.8944\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.8912\u001b[0m  0.0330\n",
      "     38        \u001b[36m0.8878\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.8842\u001b[0m  0.0330\n",
      "     40        \u001b[36m0.8805\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.8766\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.8725\u001b[0m  0.0330\n",
      "     43        \u001b[36m0.8681\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.8636\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.8588\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.8538\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.8486\u001b[0m  0.0330\n",
      "     48        \u001b[36m0.8430\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.8372\u001b[0m  0.0340\n",
      "     50        \u001b[36m0.8312\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.8248\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.8182\u001b[0m  0.0330\n",
      "     53        \u001b[36m0.8112\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.8039\u001b[0m  0.0330\n",
      "     55        \u001b[36m0.7964\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.7885\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.7803\u001b[0m  0.0330\n",
      "     58        \u001b[36m0.7719\u001b[0m  0.0330\n",
      "     59        \u001b[36m0.7632\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.7542\u001b[0m  0.0330\n",
      "     61        \u001b[36m0.7449\u001b[0m  0.0350\n",
      "     62        \u001b[36m0.7355\u001b[0m  0.0330\n",
      "     63        \u001b[36m0.7259\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.7161\u001b[0m  0.0330\n",
      "     65        \u001b[36m0.7062\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.6962\u001b[0m  0.0330\n",
      "     67        \u001b[36m0.6862\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.6762\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.6662\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.6563\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.6466\u001b[0m  0.0330\n",
      "     72        \u001b[36m0.6370\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.6275\u001b[0m  0.0330\n",
      "     74        \u001b[36m0.6183\u001b[0m  0.0350\n",
      "     75        \u001b[36m0.6094\u001b[0m  0.0330\n",
      "     76        \u001b[36m0.6007\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.5922\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.5841\u001b[0m  0.0330\n",
      "     79        \u001b[36m0.5763\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.5687\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.5615\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.5546\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.5480\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.5417\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.5356\u001b[0m  0.0380\n",
      "     86        \u001b[36m0.5299\u001b[0m  0.0330\n",
      "     87        \u001b[36m0.5244\u001b[0m  0.0350\n",
      "     88        \u001b[36m0.5191\u001b[0m  0.0330\n",
      "     89        \u001b[36m0.5142\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.5094\u001b[0m  0.0330\n",
      "     91        \u001b[36m0.5049\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.5006\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.4965\u001b[0m  0.0330\n",
      "     94        \u001b[36m0.4926\u001b[0m  0.0350\n",
      "     95        \u001b[36m0.4889\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.4854\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.4820\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.4788\u001b[0m  0.0330\n",
      "     99        \u001b[36m0.4758\u001b[0m  0.0330\n",
      "    100        \u001b[36m0.4729\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9693\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.9690\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.9686\u001b[0m  0.0330\n",
      "      4        \u001b[36m0.9682\u001b[0m  0.0330\n",
      "      5        \u001b[36m0.9679\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.9675\u001b[0m  0.0330\n",
      "      7        \u001b[36m0.9671\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.9667\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.9662\u001b[0m  0.0330\n",
      "     10        \u001b[36m0.9658\u001b[0m  0.0330\n",
      "     11        \u001b[36m0.9654\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.9649\u001b[0m  0.0330\n",
      "     13        \u001b[36m0.9645\u001b[0m  0.0350\n",
      "     14        \u001b[36m0.9640\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.9635\u001b[0m  0.0330\n",
      "     16        \u001b[36m0.9630\u001b[0m  0.0330\n",
      "     17        \u001b[36m0.9625\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.9620\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.9615\u001b[0m  0.0330\n",
      "     20        \u001b[36m0.9609\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.9603\u001b[0m  0.0330\n",
      "     22        \u001b[36m0.9598\u001b[0m  0.0330\n",
      "     23        \u001b[36m0.9592\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.9585\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.9579\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.9573\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.9566\u001b[0m  0.0330\n",
      "     28        \u001b[36m0.9559\u001b[0m  0.0330\n",
      "     29        \u001b[36m0.9552\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.9545\u001b[0m  0.0330\n",
      "     31        \u001b[36m0.9537\u001b[0m  0.0330\n",
      "     32        \u001b[36m0.9529\u001b[0m  0.0340\n",
      "     33        \u001b[36m0.9521\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.9513\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.9505\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.9496\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.9487\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.9478\u001b[0m  0.0370\n",
      "     39        \u001b[36m0.9468\u001b[0m  0.0330\n",
      "     40        \u001b[36m0.9458\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.9447\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.9437\u001b[0m  0.0350\n",
      "     43        \u001b[36m0.9426\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.9414\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.9402\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.9390\u001b[0m  0.0330\n",
      "     47        \u001b[36m0.9377\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.9364\u001b[0m  0.0330\n",
      "     49        \u001b[36m0.9350\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.9336\u001b[0m  0.0330\n",
      "     51        \u001b[36m0.9321\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.9305\u001b[0m  0.0340\n",
      "     53        \u001b[36m0.9289\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.9272\u001b[0m  0.0330\n",
      "     55        \u001b[36m0.9255\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.9236\u001b[0m  0.0330\n",
      "     57        \u001b[36m0.9217\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.9197\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.9177\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.9155\u001b[0m  0.0330\n",
      "     61        \u001b[36m0.9132\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.9108\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.9084\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.9058\u001b[0m  0.0330\n",
      "     65        \u001b[36m0.9030\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.9002\u001b[0m  0.0350\n",
      "     67        \u001b[36m0.8972\u001b[0m  0.0330\n",
      "     68        \u001b[36m0.8940\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.8908\u001b[0m  0.0330\n",
      "     70        \u001b[36m0.8873\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.8837\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.8798\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.8758\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.8716\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.8672\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.8625\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.8576\u001b[0m  0.0330\n",
      "     78        \u001b[36m0.8525\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.8470\u001b[0m  0.0330\n",
      "     80        \u001b[36m0.8414\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.8354\u001b[0m  0.0350\n",
      "     82        \u001b[36m0.8291\u001b[0m  0.0330\n",
      "     83        \u001b[36m0.8226\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.8157\u001b[0m  0.0330\n",
      "     85        \u001b[36m0.8085\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.8010\u001b[0m  0.0330\n",
      "     87        \u001b[36m0.7932\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.7851\u001b[0m  0.0380\n",
      "     89        \u001b[36m0.7767\u001b[0m  0.0330\n",
      "     90        \u001b[36m0.7679\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.7589\u001b[0m  0.0330\n",
      "     92        \u001b[36m0.7497\u001b[0m  0.0330\n",
      "     93        \u001b[36m0.7402\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.7305\u001b[0m  0.0330\n",
      "     95        \u001b[36m0.7206\u001b[0m  0.0330\n",
      "     96        \u001b[36m0.7106\u001b[0m  0.0330\n",
      "     97        \u001b[36m0.7005\u001b[0m  0.0330\n",
      "     98        \u001b[36m0.6903\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.6801\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.6699\u001b[0m  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9998\u001b[0m  0.0440\n",
      "      2        \u001b[36m0.9995\u001b[0m  0.0450\n",
      "      3        \u001b[36m0.9984\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.9923\u001b[0m  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.9509\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.7160\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4626\u001b[0m  0.0440\n",
      "      8        \u001b[36m0.3982\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.3876\u001b[0m  0.0430\n",
      "     10        \u001b[36m0.3841\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.3822\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3807\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3796\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3787\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.3779\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.3772\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3766\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.3761\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3757\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3751\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     23        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     25        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     30        \u001b[36m0.3735\u001b[0m  0.0440\n",
      "     31        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     32        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     33        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     34        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "     36        \u001b[36m0.3729\u001b[0m  0.0440\n",
      "     37        \u001b[36m0.3728\u001b[0m  0.0440\n",
      "     38        \u001b[36m0.3728\u001b[0m  0.0440\n",
      "     39        \u001b[36m0.3727\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3727\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3726\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3725\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3725\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3724\u001b[0m  0.0440\n",
      "     47        \u001b[36m0.3724\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3722\u001b[0m  0.0480\n",
      "     53        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     54        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     58        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     59        \u001b[36m0.3720\u001b[0m  0.0450\n",
      "     60        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     62        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     68        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     70        \u001b[36m0.3719\u001b[0m  0.0450\n",
      "     71        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     73        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     76        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     78        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     80        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     81        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     90        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3717\u001b[0m  0.0480\n",
      "     92        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     98        \u001b[36m0.3717\u001b[0m  0.0450\n",
      "     99        \u001b[36m0.3717\u001b[0m  0.0450\n",
      "    100        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9991\u001b[0m  0.0440\n",
      "      2        \u001b[36m0.9980\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.9936\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.9731\u001b[0m  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.8766\u001b[0m  0.0450\n",
      "      6        \u001b[36m0.6463\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4697\u001b[0m  0.0450\n",
      "      8        \u001b[36m0.4124\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.3975\u001b[0m  0.0430\n",
      "     10        \u001b[36m0.3844\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.3820\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.3808\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.3799\u001b[0m  0.0440\n",
      "     14        \u001b[36m0.3790\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.3780\u001b[0m  0.0480\n",
      "     16        \u001b[36m0.3774\u001b[0m  0.0450\n",
      "     17        \u001b[36m0.3770\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3762\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3758\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3755\u001b[0m  0.0450\n",
      "     22        \u001b[36m0.3753\u001b[0m  0.0450\n",
      "     23        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.3742\u001b[0m  0.0450\n",
      "     28        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     33        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     35        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     36        \u001b[36m0.3732\u001b[0m  0.0460\n",
      "     37        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3730\u001b[0m  0.0450\n",
      "     39        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3729\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3728\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3728\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3727\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3727\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3726\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     47        \u001b[36m0.3725\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3725\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3725\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.3724\u001b[0m  0.0450\n",
      "     51        \u001b[36m0.3724\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3723\u001b[0m  0.0440\n",
      "     54        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     55        \u001b[36m0.3723\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     59        \u001b[36m0.3722\u001b[0m  0.0440\n",
      "     60        \u001b[36m0.3721\u001b[0m  0.0450\n",
      "     61        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3721\u001b[0m  0.0480\n",
      "     63        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     70        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     73        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     76        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     77        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     79        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     80        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     81        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     82        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     84        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     90        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     91        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     97        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     98        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     99        \u001b[36m0.3717\u001b[0m  0.0470\n",
      "    100        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9999\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.9997\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.9990\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.9953\u001b[0m  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.9705\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.8289\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.5107\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.3998\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.3916\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.3879\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.3854\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3836\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.3823\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3812\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3801\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.3785\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3780\u001b[0m  0.0440\n",
      "     18        \u001b[36m0.3777\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.3774\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3771\u001b[0m  0.0440\n",
      "     21        \u001b[36m0.3769\u001b[0m  0.0440\n",
      "     22        \u001b[36m0.3766\u001b[0m  0.0420\n",
      "     23        \u001b[36m0.3765\u001b[0m  0.0440\n",
      "     24        \u001b[36m0.3763\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3761\u001b[0m  0.0440\n",
      "     26        \u001b[36m0.3760\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.3758\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3757\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3755\u001b[0m  0.0440\n",
      "     31        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3753\u001b[0m  0.0440\n",
      "     33        \u001b[36m0.3752\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3752\u001b[0m  0.0440\n",
      "     35        \u001b[36m0.3751\u001b[0m  0.0440\n",
      "     36        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3750\u001b[0m  0.0440\n",
      "     38        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     39        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     41        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3746\u001b[0m  0.0450\n",
      "     45        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     47        \u001b[36m0.3745\u001b[0m  0.0450\n",
      "     48        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     49        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     55        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     57        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     58        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     60        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     61        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     63        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3741\u001b[0m  0.0480\n",
      "     67        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     68        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     70        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3740\u001b[0m  0.0450\n",
      "     72        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     73        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     77        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     80        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     81        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3739\u001b[0m  0.0450\n",
      "     87        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     89        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     90        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     92        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3739\u001b[0m  0.0450\n",
      "     97        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     98        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     99        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "    100        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9998\u001b[0m  0.0480\n",
      "      2        \u001b[36m0.9995\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.9982\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.9916\u001b[0m  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.9499\u001b[0m  0.0430\n",
      "      6        \u001b[36m0.7586\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4642\u001b[0m  0.0440\n",
      "      8        \u001b[36m0.3913\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.3820\u001b[0m  0.0430\n",
      "     10        \u001b[36m0.3797\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.3791\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3786\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.3783\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3779\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3776\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.3773\u001b[0m  0.0450\n",
      "     17        \u001b[36m0.3764\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.3761\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3760\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3759\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3758\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3757\u001b[0m  0.0440\n",
      "     23        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3755\u001b[0m  0.0440\n",
      "     25        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3754\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.3753\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3752\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3751\u001b[0m  0.0440\n",
      "     30        \u001b[36m0.3751\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3750\u001b[0m  0.0450\n",
      "     32        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     37        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     39        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     40        \u001b[36m0.3746\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3746\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     46        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     51        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3743\u001b[0m  0.0450\n",
      "     53        \u001b[36m0.3743\u001b[0m  0.0450\n",
      "     54        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     57        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     59        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     61        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3741\u001b[0m  0.0450\n",
      "     63        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     66        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     68        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.3740\u001b[0m  0.0470\n",
      "     70        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     71        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     73        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     75        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     78        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     80        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     81        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     82        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     84        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     87        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3739\u001b[0m  0.0470\n",
      "     89        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     90        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     92        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     93        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     96        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     98        \u001b[36m0.3738\u001b[0m  0.0450\n",
      "     99        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9997\u001b[0m  0.0470\n",
      "      2        \u001b[36m0.9993\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.9977\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.9888\u001b[0m  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.9191\u001b[0m  0.0450\n",
      "      6        \u001b[36m0.6336\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.4528\u001b[0m  0.0450\n",
      "      8        \u001b[36m0.4018\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.3898\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.3818\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.3794\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.3783\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3779\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3775\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3771\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.3768\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3766\u001b[0m  0.0440\n",
      "     18        \u001b[36m0.3763\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3761\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3759\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3757\u001b[0m  0.0450\n",
      "     22        \u001b[36m0.3756\u001b[0m  0.0440\n",
      "     23        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3753\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3751\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3747\u001b[0m  0.0460\n",
      "     30        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     32        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     34        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     36        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     37        \u001b[36m0.3741\u001b[0m  0.0450\n",
      "     38        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     51        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3735\u001b[0m  0.0440\n",
      "     53        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3735\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     56        \u001b[36m0.3735\u001b[0m  0.0420\n",
      "     57        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     58        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     60        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     62        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     63        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "     66        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     68        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.3732\u001b[0m  0.0480\n",
      "     70        \u001b[36m0.3732\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3732\u001b[0m  0.0430\n",
      "     73        \u001b[36m0.3732\u001b[0m  0.0460\n",
      "     74        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     76        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     80        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     81        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     90        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     92        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     94        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "     96        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "     98        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "     99        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9985\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.9985\u001b[0m  0.0370\n",
      "      4        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "      6        \u001b[36m0.9985\u001b[0m  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.9985\u001b[0m  0.0370\n",
      "     11        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.9985\u001b[0m  0.0330\n",
      "     13        \u001b[36m0.9985\u001b[0m  0.0330\n",
      "     14        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.9985\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.9985\u001b[0m  0.0330\n",
      "     19        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.9984\u001b[0m  0.0350\n",
      "     27        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     32        \u001b[36m0.9984\u001b[0m  0.0350\n",
      "     33        \u001b[36m0.9984\u001b[0m  0.0350\n",
      "     34        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     35        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     37        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.9984\u001b[0m  0.0350\n",
      "     41        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     42        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     45        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     47        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     50        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     52        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     53        \u001b[36m0.9984\u001b[0m  0.0350\n",
      "     54        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.9984\u001b[0m  0.0350\n",
      "     56        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     57        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     59        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     61        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     66        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.9984\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.9984\u001b[0m  0.0330\n",
      "     69        \u001b[36m0.9983\u001b[0m  0.0370\n",
      "     70        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.9983\u001b[0m  0.0330\n",
      "     76        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.9983\u001b[0m  0.0330\n",
      "     78        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.9983\u001b[0m  0.0330\n",
      "     81        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.9983\u001b[0m  0.0360\n",
      "     83        \u001b[36m0.9983\u001b[0m  0.0330\n",
      "     84        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.9983\u001b[0m  0.0330\n",
      "     90        \u001b[36m0.9983\u001b[0m  0.0350\n",
      "     91        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     95        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.9983\u001b[0m  0.0330\n",
      "     97        \u001b[36m0.9983\u001b[0m  0.0350\n",
      "     98        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.9983\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.9983\u001b[0m  0.0410\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "      2        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "      3        0.9998  0.0340\n",
      "      4        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.9998\u001b[0m  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "      7        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "      8        0.9998  0.0340\n",
      "      9        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     11        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     12        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     13        0.9998  0.0340\n",
      "     14        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     16        0.9998  0.0340\n",
      "     17        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     20        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     21        0.9998  0.0340\n",
      "     22        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     26        0.9998  0.0330\n",
      "     27        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     29        0.9998  0.0340\n",
      "     30        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     31        0.9998  0.0340\n",
      "     32        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     33        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     34        0.9998  0.0340\n",
      "     35        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     37        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     39        0.9998  0.0340\n",
      "     40        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     44        0.9998  0.0340\n",
      "     45        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     47        0.9998  0.0340\n",
      "     48        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     49        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     50        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     52        0.9998  0.0340\n",
      "     53        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     54        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     55        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     57        0.9998  0.0340\n",
      "     58        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     60        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     61        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     62        0.9998  0.0340\n",
      "     63        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     65        0.9998  0.0340\n",
      "     66        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     68        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     69        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     70        0.9998  0.0330\n",
      "     71        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     72        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     74        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     75        0.9998  0.0340\n",
      "     76        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     80        0.9998  0.0440\n",
      "     81        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     84        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     85        0.9998  0.0340\n",
      "     86        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     87        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     88        0.9998  0.0340\n",
      "     89        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "     91        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     93        0.9998  0.0340\n",
      "     94        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     95        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.9998\u001b[0m  0.0350\n",
      "     98        0.9998  0.0340\n",
      "     99        \u001b[36m0.9998\u001b[0m  0.0330\n",
      "    100        \u001b[36m0.9998\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "      2        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "      4        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "      5        \u001b[36m0.9995\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "      7        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     11        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     13        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     15        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     19        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     22        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     23        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     27        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     29        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     31        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     32        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     33        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     37        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     39        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.9995\u001b[0m  0.0370\n",
      "     44        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     45        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     46        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     48        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     53        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     54        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     55        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     61        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     62        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     65        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     69        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     71        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     72        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     83        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     84        \u001b[36m0.9995\u001b[0m  0.0360\n",
      "     85        \u001b[36m0.9995\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     89        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     92        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.9995\u001b[0m  0.0330\n",
      "     94        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     95        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     97        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "     98        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.9995\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.9995\u001b[0m  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "      2        0.9999  0.0340\n",
      "      3        0.9999  0.0340\n",
      "      4        0.9999  0.0340\n",
      "      5        0.9999  0.0340\n",
      "      6        \u001b[36m0.9999\u001b[0m  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.9999\u001b[0m  0.0350\n",
      "      8        0.9999  0.0330\n",
      "      9        0.9999  0.0340\n",
      "     10        0.9999  0.0330\n",
      "     11        0.9999  0.0340\n",
      "     12        0.9999  0.0350\n",
      "     13        0.9999  0.0350\n",
      "     14        0.9999  0.0340\n",
      "     15        0.9999  0.0330\n",
      "     16        0.9999  0.0350\n",
      "     17        0.9999  0.0340\n",
      "     18        \u001b[36m0.9999\u001b[0m  0.0330\n",
      "     19        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     20        0.9999  0.0350\n",
      "     21        0.9999  0.0330\n",
      "     22        0.9999  0.0340\n",
      "     23        0.9999  0.0340\n",
      "     24        0.9999  0.0330\n",
      "     25        0.9999  0.0350\n",
      "     26        0.9999  0.0340\n",
      "     27        0.9999  0.0350\n",
      "     28        0.9999  0.0330\n",
      "     29        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     31        0.9999  0.0340\n",
      "     32        0.9999  0.0330\n",
      "     33        0.9999  0.0340\n",
      "     34        0.9999  0.0340\n",
      "     35        0.9999  0.0330\n",
      "     36        0.9999  0.0340\n",
      "     37        0.9999  0.0330\n",
      "     38        0.9999  0.0340\n",
      "     39        0.9999  0.0340\n",
      "     40        0.9999  0.0340\n",
      "     41        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     43        0.9999  0.0340\n",
      "     44        0.9999  0.0340\n",
      "     45        0.9999  0.0340\n",
      "     46        0.9999  0.0340\n",
      "     47        0.9999  0.0330\n",
      "     48        0.9999  0.0340\n",
      "     49        0.9999  0.0350\n",
      "     50        0.9999  0.0340\n",
      "     51        0.9999  0.0340\n",
      "     52        \u001b[36m0.9999\u001b[0m  0.0350\n",
      "     53        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     54        0.9999  0.0330\n",
      "     55        0.9999  0.0340\n",
      "     56        0.9999  0.0340\n",
      "     57        0.9999  0.0330\n",
      "     58        0.9999  0.0350\n",
      "     59        0.9999  0.0340\n",
      "     60        0.9999  0.0340\n",
      "     61        0.9999  0.0340\n",
      "     62        0.9999  0.0340\n",
      "     63        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.9999\u001b[0m  0.0370\n",
      "     65        0.9999  0.0340\n",
      "     66        0.9999  0.0340\n",
      "     67        0.9999  0.0330\n",
      "     68        0.9999  0.0340\n",
      "     69        0.9999  0.0340\n",
      "     70        0.9999  0.0360\n",
      "     71        0.9999  0.0340\n",
      "     72        0.9999  0.0330\n",
      "     73        0.9999  0.0340\n",
      "     74        0.9999  0.0370\n",
      "     75        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     77        0.9999  0.0340\n",
      "     78        0.9999  0.0340\n",
      "     79        0.9999  0.0340\n",
      "     80        0.9999  0.0340\n",
      "     81        0.9999  0.0340\n",
      "     82        0.9999  0.0340\n",
      "     83        0.9999  0.0350\n",
      "     84        0.9999  0.0340\n",
      "     85        0.9999  0.0340\n",
      "     86        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     88        0.9999  0.0330\n",
      "     89        0.9999  0.0340\n",
      "     90        0.9999  0.0340\n",
      "     91        0.9999  0.0340\n",
      "     92        0.9999  0.0330\n",
      "     93        0.9999  0.0340\n",
      "     94        0.9999  0.0330\n",
      "     95        0.9999  0.0330\n",
      "     96        0.9999  0.0350\n",
      "     97        0.9999  0.0330\n",
      "     98        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     99        0.9999  0.0340\n",
      "    100        0.9999  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "      2        0.9999  0.0340\n",
      "      3        0.9999  0.0340\n",
      "      4        0.9999  0.0340\n",
      "      5        0.9999  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        0.9999  0.0330\n",
      "      7        0.9999  0.0340\n",
      "      8        0.9999  0.0340\n",
      "      9        0.9999  0.0340\n",
      "     10        0.9999  0.0340\n",
      "     11        \u001b[36m0.9999\u001b[0m  0.0360\n",
      "     12        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     13        0.9999  0.0340\n",
      "     14        0.9999  0.0340\n",
      "     15        0.9999  0.0330\n",
      "     16        0.9999  0.0340\n",
      "     17        0.9999  0.0340\n",
      "     18        0.9999  0.0330\n",
      "     19        0.9999  0.0340\n",
      "     20        0.9999  0.0350\n",
      "     21        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.9999\u001b[0m  0.0330\n",
      "     23        0.9999  0.0330\n",
      "     24        0.9999  0.0340\n",
      "     25        0.9999  0.0340\n",
      "     26        0.9999  0.0350\n",
      "     27        0.9999  0.0360\n",
      "     28        0.9999  0.0340\n",
      "     29        0.9999  0.0340\n",
      "     30        0.9999  0.0340\n",
      "     31        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     33        0.9999  0.0330\n",
      "     34        0.9999  0.0340\n",
      "     35        0.9999  0.0330\n",
      "     36        0.9999  0.0340\n",
      "     37        0.9999  0.0340\n",
      "     38        0.9999  0.0340\n",
      "     39        0.9999  0.0340\n",
      "     40        0.9999  0.0340\n",
      "     41        0.9999  0.0340\n",
      "     42        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     44        0.9999  0.0340\n",
      "     45        0.9999  0.0340\n",
      "     46        0.9999  0.0330\n",
      "     47        0.9999  0.0350\n",
      "     48        0.9999  0.0340\n",
      "     49        0.9999  0.0340\n",
      "     50        0.9999  0.0340\n",
      "     51        0.9999  0.0340\n",
      "     52        \u001b[36m0.9999\u001b[0m  0.0330\n",
      "     53        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     54        0.9999  0.0340\n",
      "     55        0.9999  0.0330\n",
      "     56        0.9999  0.0400\n",
      "     57        0.9999  0.0340\n",
      "     58        0.9999  0.0330\n",
      "     59        0.9999  0.0340\n",
      "     60        0.9999  0.0340\n",
      "     61        0.9999  0.0340\n",
      "     62        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     64        0.9999  0.0330\n",
      "     65        0.9999  0.0340\n",
      "     66        0.9999  0.0340\n",
      "     67        0.9999  0.0340\n",
      "     68        0.9999  0.0400\n",
      "     69        0.9999  0.0370\n",
      "     70        0.9999  0.0350\n",
      "     71        0.9999  0.0350\n",
      "     72        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     74        0.9999  0.0340\n",
      "     75        0.9999  0.0340\n",
      "     76        0.9999  0.0340\n",
      "     77        0.9999  0.0330\n",
      "     78        0.9999  0.0340\n",
      "     79        0.9999  0.0330\n",
      "     80        0.9999  0.0340\n",
      "     81        0.9999  0.0340\n",
      "     82        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     84        0.9999  0.0340\n",
      "     85        0.9999  0.0360\n",
      "     86        0.9999  0.0340\n",
      "     87        0.9999  0.0340\n",
      "     88        0.9999  0.0340\n",
      "     89        0.9999  0.0340\n",
      "     90        0.9999  0.0340\n",
      "     91        0.9999  0.0330\n",
      "     92        0.9999  0.0340\n",
      "     93        \u001b[36m0.9999\u001b[0m  0.0340\n",
      "     94        0.9999  0.0340\n",
      "     95        0.9999  0.0330\n",
      "     96        0.9999  0.0350\n",
      "     97        0.9999  0.0360\n",
      "     98        0.9999  0.0330\n",
      "     99        0.9999  0.0340\n",
      "    100        0.9999  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6805\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.6358\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.5946\u001b[0m  0.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4        \u001b[36m0.5580\u001b[0m  0.0430\n",
      "      5        \u001b[36m0.5269\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.5009\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.4775\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.4605\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.4471\u001b[0m  0.0430\n",
      "     10        \u001b[36m0.4373\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.4297\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.4232\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.4161\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.4115\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.4075\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.4033\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3998\u001b[0m  0.0420\n",
      "     18        \u001b[36m0.3971\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3943\u001b[0m  0.0440\n",
      "     20        \u001b[36m0.3920\u001b[0m  0.0440\n",
      "     21        \u001b[36m0.3900\u001b[0m  0.0450\n",
      "     22        \u001b[36m0.3882\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3862\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3844\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3833\u001b[0m  0.0440\n",
      "     26        \u001b[36m0.3823\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3816\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.3809\u001b[0m  0.0420\n",
      "     29        \u001b[36m0.3803\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3798\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3793\u001b[0m  0.0440\n",
      "     32        \u001b[36m0.3789\u001b[0m  0.0450\n",
      "     33        \u001b[36m0.3785\u001b[0m  0.0440\n",
      "     34        \u001b[36m0.3781\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3778\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3774\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3771\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3768\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3764\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3761\u001b[0m  0.0430\n",
      "     41        \u001b[36m0.3759\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3754\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3753\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3751\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3750\u001b[0m  0.0440\n",
      "     47        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     49        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3744\u001b[0m  0.0450\n",
      "     51        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     52        0.3745  0.0430\n",
      "     53        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     56        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     57        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3734\u001b[0m  0.0420\n",
      "     60        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.3732\u001b[0m  0.0430\n",
      "     62        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3729\u001b[0m  0.0480\n",
      "     66        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.3728\u001b[0m  0.0440\n",
      "     68        \u001b[36m0.3727\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     70        \u001b[36m0.3725\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     72        \u001b[36m0.3723\u001b[0m  0.0450\n",
      "     73        \u001b[36m0.3723\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3723\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3723\u001b[0m  0.0440\n",
      "     76        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     77        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     78        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     79        \u001b[36m0.3722\u001b[0m  0.0450\n",
      "     80        0.3744  0.0440\n",
      "     81        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     82        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     87        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     88        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     90        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     91        0.3722  0.0430\n",
      "     92        0.3722  0.0440\n",
      "     93        0.3721  0.0430\n",
      "     94        0.3720  0.0430\n",
      "     95        0.3720  0.0430\n",
      "     96        0.3720  0.0440\n",
      "     97        0.3719  0.0430\n",
      "     98        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     99        \u001b[36m0.3718\u001b[0m  0.0490\n",
      "    100        \u001b[36m0.3718\u001b[0m  0.0450\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5821\u001b[0m  0.0470\n",
      "      2        \u001b[36m0.4869\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.4435\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.4270\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.4178\u001b[0m  0.0430\n",
      "      6        \u001b[36m0.4122\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.4087\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.4063\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.4044\u001b[0m  0.0430\n",
      "     10        \u001b[36m0.4022\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.4000\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3974\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.3951\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3930\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3912\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.3896\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3880\u001b[0m  0.0440\n",
      "     18        \u001b[36m0.3864\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3851\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3838\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3827\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3817\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3808\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3800\u001b[0m  0.0450\n",
      "     25        \u001b[36m0.3792\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3786\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.3780\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3775\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3771\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3766\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3762\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3759\u001b[0m  0.0440\n",
      "     33        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3753\u001b[0m  0.0450\n",
      "     35        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3735\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "     42        \u001b[36m0.3731\u001b[0m  0.0440\n",
      "     43        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3728\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.3727\u001b[0m  0.0440\n",
      "     46        \u001b[36m0.3726\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3725\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     55        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     57        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     58        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3717\u001b[0m  0.0470\n",
      "     63        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     66        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     70        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     73        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     74        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     75        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     78        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     80        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     81        0.3716  0.0430\n",
      "     82        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     89        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     90        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     92        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     93        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     95        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     98        \u001b[36m0.3716\u001b[0m  0.0480\n",
      "     99        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "    100        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6540\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.5594\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.4904\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.4481\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.4232\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.4103\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.4034\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.3984\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.3949\u001b[0m  0.0430\n",
      "     10        \u001b[36m0.3924\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.3904\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.3887\u001b[0m  0.0430\n",
      "     13        0.3888  0.0430\n",
      "     14        \u001b[36m0.3875\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3864\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.3862\u001b[0m  0.0430\n",
      "     17        0.3870  0.0440\n",
      "     18        \u001b[36m0.3857\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3835\u001b[0m  0.0430\n",
      "     20        0.3837  0.0430\n",
      "     21        0.3842  0.0440\n",
      "     22        \u001b[36m0.3835\u001b[0m  0.0430\n",
      "     23        0.3837  0.0440\n",
      "     24        \u001b[36m0.3820\u001b[0m  0.0440\n",
      "     25        \u001b[36m0.3807\u001b[0m  0.0440\n",
      "     26        \u001b[36m0.3798\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3798\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3796\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3791\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3787\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3782\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3779\u001b[0m  0.0420\n",
      "     33        \u001b[36m0.3776\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3773\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3771\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3769\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3767\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     39        0.3766  0.0430\n",
      "     40        0.3766  0.0440\n",
      "     41        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     42        \u001b[36m0.3760\u001b[0m  0.0420\n",
      "     43        \u001b[36m0.3758\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3757\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3755\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3755\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3754\u001b[0m  0.0440\n",
      "     49        \u001b[36m0.3753\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3752\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3751\u001b[0m  0.0450\n",
      "     52        \u001b[36m0.3751\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3750\u001b[0m  0.0440\n",
      "     54        \u001b[36m0.3750\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     56        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     59        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3747\u001b[0m  0.0450\n",
      "     61        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3746\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3746\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     69        \u001b[36m0.3745\u001b[0m  0.0470\n",
      "     70        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     71        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3744\u001b[0m  0.0450\n",
      "     73        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     76        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     77        \u001b[36m0.3743\u001b[0m  0.0450\n",
      "     78        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     80        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     81        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     84        0.3745  0.0440\n",
      "     85        0.3744  0.0440\n",
      "     86        0.3743  0.0440\n",
      "     87        0.3743  0.0430\n",
      "     88        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     90        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     91        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     93        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     98        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     99        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6271\u001b[0m  0.0420\n",
      "      2        \u001b[36m0.5233\u001b[0m  0.0490\n",
      "      3        \u001b[36m0.4556\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.4279\u001b[0m  0.0440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      5        \u001b[36m0.4138\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.4017\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.3970\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.3904\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.3878\u001b[0m  0.0430\n",
      "     10        \u001b[36m0.3863\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.3850\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3839\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.3831\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3817\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.3811\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.3801\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3796\u001b[0m  0.0440\n",
      "     18        \u001b[36m0.3791\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.3787\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3783\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3780\u001b[0m  0.0440\n",
      "     22        \u001b[36m0.3777\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3774\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3771\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3769\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3767\u001b[0m  0.0450\n",
      "     27        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3763\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3762\u001b[0m  0.0440\n",
      "     30        \u001b[36m0.3760\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3759\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3758\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3756\u001b[0m  0.0440\n",
      "     34        \u001b[36m0.3755\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3754\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3753\u001b[0m  0.0440\n",
      "     37        \u001b[36m0.3752\u001b[0m  0.0440\n",
      "     38        \u001b[36m0.3752\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3751\u001b[0m  0.0420\n",
      "     40        0.3755  0.0440\n",
      "     41        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     42        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     46        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     47        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3746\u001b[0m  0.0450\n",
      "     50        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     53        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     56        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     58        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     59        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     68        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     70        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     71        0.3742  0.0470\n",
      "     72        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     73        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     76        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     77        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     78        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     79        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     80        0.3740  0.0440\n",
      "     81        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     83        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3740\u001b[0m  0.0450\n",
      "     88        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     89        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     90        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     92        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     94        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3739\u001b[0m  0.0420\n",
      "     97        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     98        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     99        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6732\u001b[0m  0.0440\n",
      "      2        \u001b[36m0.5928\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.5390\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.5031\u001b[0m  0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.4794\u001b[0m  0.0480\n",
      "      6        \u001b[36m0.4651\u001b[0m  0.0430\n",
      "      7        \u001b[36m0.4536\u001b[0m  0.0440\n",
      "      8        \u001b[36m0.4450\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.4372\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.4309\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.4252\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.4206\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.4156\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.4108\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.4068\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.4033\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3997\u001b[0m  0.0450\n",
      "     18        \u001b[36m0.3968\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3940\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3918\u001b[0m  0.0430\n",
      "     21        \u001b[36m0.3893\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3875\u001b[0m  0.0440\n",
      "     23        \u001b[36m0.3863\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3852\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3843\u001b[0m  0.0440\n",
      "     26        \u001b[36m0.3834\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.3827\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.3805\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3797\u001b[0m  0.0450\n",
      "     30        \u001b[36m0.3793\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3788\u001b[0m  0.0440\n",
      "     32        \u001b[36m0.3783\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3780\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3777\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3774\u001b[0m  0.0440\n",
      "     36        \u001b[36m0.3772\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3769\u001b[0m  0.0420\n",
      "     38        \u001b[36m0.3767\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3763\u001b[0m  0.0430\n",
      "     41        \u001b[36m0.3761\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3760\u001b[0m  0.0440\n",
      "     43        \u001b[36m0.3758\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.3754\u001b[0m  0.0440\n",
      "     46        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3748\u001b[0m  0.0440\n",
      "     49        \u001b[36m0.3747\u001b[0m  0.0420\n",
      "     50        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3746\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     54        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     55        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     58        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     61        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     62        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     63        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3740\u001b[0m  0.0450\n",
      "     66        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     68        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     70        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     71        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     72        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     73        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3738\u001b[0m  0.0480\n",
      "     76        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     80        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     81        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     83        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     84        0.3738  0.0440\n",
      "     85        0.3738  0.0430\n",
      "     86        0.3737  0.0440\n",
      "     87        0.3737  0.0440\n",
      "     88        0.3737  0.0440\n",
      "     89        0.3736  0.0440\n",
      "     90        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3735\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3734\u001b[0m  0.0450\n",
      "     94        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     95        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     98        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     99        \u001b[36m0.3732\u001b[0m  0.0430\n",
      "    100        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.4241\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.4234\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.4232\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.4231\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.4229\u001b[0m  0.0370\n",
      "      6        \u001b[36m0.4228\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.4226\u001b[0m  0.0330\n",
      "      8        \u001b[36m0.4225\u001b[0m  0.0330\n",
      "      9        \u001b[36m0.4223\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.4222\u001b[0m  0.0330\n",
      "     11        \u001b[36m0.4220\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.4217\u001b[0m  0.0330\n",
      "     13        \u001b[36m0.4216\u001b[0m  0.0350\n",
      "     14        \u001b[36m0.4212\u001b[0m  0.0330\n",
      "     15        \u001b[36m0.4211\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.4210\u001b[0m  0.0330\n",
      "     17        \u001b[36m0.4209\u001b[0m  0.0330\n",
      "     18        \u001b[36m0.4207\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.4206\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.4204\u001b[0m  0.0330\n",
      "     21        \u001b[36m0.4203\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.4202\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.4200\u001b[0m  0.0330\n",
      "     24        \u001b[36m0.4199\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.4198\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.4197\u001b[0m  0.0330\n",
      "     27        \u001b[36m0.4196\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.4194\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.4193\u001b[0m  0.0330\n",
      "     30        \u001b[36m0.4192\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.4191\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.4190\u001b[0m  0.0350\n",
      "     33        \u001b[36m0.4189\u001b[0m  0.0330\n",
      "     34        \u001b[36m0.4187\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.4186\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.4185\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.4184\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.4183\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.4182\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.4179\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.4178\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.4177\u001b[0m  0.0370\n",
      "     43        \u001b[36m0.4176\u001b[0m  0.0330\n",
      "     44        \u001b[36m0.4175\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.4174\u001b[0m  0.0370\n",
      "     46        \u001b[36m0.4173\u001b[0m  0.0330\n",
      "     47        \u001b[36m0.4172\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.4171\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.4170\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.4169\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.4168\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.4167\u001b[0m  0.0330\n",
      "     53        \u001b[36m0.4166\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.4165\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.4164\u001b[0m  0.0330\n",
      "     56        \u001b[36m0.4163\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.4162\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.4161\u001b[0m  0.0330\n",
      "     59        \u001b[36m0.4160\u001b[0m  0.0330\n",
      "     60        \u001b[36m0.4159\u001b[0m  0.0340\n",
      "     61        \u001b[36m0.4158\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.4157\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.4156\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.4155\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.4154\u001b[0m  0.0350\n",
      "     66        \u001b[36m0.4153\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.4152\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.4152\u001b[0m  0.0330\n",
      "     69        \u001b[36m0.4151\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.4150\u001b[0m  0.0350\n",
      "     71        \u001b[36m0.4149\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.4148\u001b[0m  0.0330\n",
      "     73        \u001b[36m0.4147\u001b[0m  0.0350\n",
      "     74        \u001b[36m0.4146\u001b[0m  0.0330\n",
      "     75        \u001b[36m0.4145\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.4144\u001b[0m  0.0330\n",
      "     77        \u001b[36m0.4143\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.4143\u001b[0m  0.0370\n",
      "     79        \u001b[36m0.4142\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.4141\u001b[0m  0.0330\n",
      "     81        \u001b[36m0.4140\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.4139\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.4138\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.4137\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.4137\u001b[0m  0.0330\n",
      "     86        \u001b[36m0.4136\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.4135\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.4134\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.4133\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.4132\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.4132\u001b[0m  0.0330\n",
      "     92        \u001b[36m0.4131\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.4130\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.4129\u001b[0m  0.0340\n",
      "     95        \u001b[36m0.4128\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.4128\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.4127\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.4126\u001b[0m  0.0330\n",
      "     99        \u001b[36m0.4125\u001b[0m  0.0350\n",
      "    100        \u001b[36m0.4124\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5275\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.5205\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.5186\u001b[0m  0.0330\n",
      "      4        \u001b[36m0.5171\u001b[0m  0.0370\n",
      "      5        \u001b[36m0.5157\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.5143\u001b[0m  0.0340\n",
      "      7        \u001b[36m0.5129\u001b[0m  0.0330\n",
      "      8        \u001b[36m0.5115\u001b[0m  0.0330\n",
      "      9        \u001b[36m0.5102\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.5088\u001b[0m  0.0350\n",
      "     11        \u001b[36m0.5075\u001b[0m  0.0330\n",
      "     12        \u001b[36m0.5063\u001b[0m  0.0340\n",
      "     13        \u001b[36m0.5050\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.5038\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.5026\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.5013\u001b[0m  0.0330\n",
      "     17        \u001b[36m0.4999\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.4988\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.4976\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.4965\u001b[0m  0.0330\n",
      "     21        \u001b[36m0.4954\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.4943\u001b[0m  0.0330\n",
      "     23        \u001b[36m0.4932\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.4921\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.4911\u001b[0m  0.0330\n",
      "     26        \u001b[36m0.4901\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.4891\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.4880\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.4871\u001b[0m  0.0330\n",
      "     30        \u001b[36m0.4861\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.4851\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.4842\u001b[0m  0.0330\n",
      "     33        \u001b[36m0.4833\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.4824\u001b[0m  0.0340\n",
      "     35        \u001b[36m0.4815\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.4806\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.4797\u001b[0m  0.0330\n",
      "     38        \u001b[36m0.4788\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.4780\u001b[0m  0.0330\n",
      "     40        \u001b[36m0.4771\u001b[0m  0.0370\n",
      "     41        \u001b[36m0.4763\u001b[0m  0.0350\n",
      "     42        \u001b[36m0.4755\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.4747\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.4739\u001b[0m  0.0330\n",
      "     45        \u001b[36m0.4731\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.4723\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.4715\u001b[0m  0.0350\n",
      "     48        \u001b[36m0.4708\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.4700\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.4693\u001b[0m  0.0330\n",
      "     51        \u001b[36m0.4685\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.4678\u001b[0m  0.0350\n",
      "     53        \u001b[36m0.4671\u001b[0m  0.0330\n",
      "     54        \u001b[36m0.4664\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.4657\u001b[0m  0.0330\n",
      "     56        \u001b[36m0.4650\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.4644\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.4637\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.4630\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.4624\u001b[0m  0.0340\n",
      "     61        \u001b[36m0.4617\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.4611\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.4605\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.4598\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.4592\u001b[0m  0.0330\n",
      "     66        \u001b[36m0.4586\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.4580\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.4574\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.4568\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.4562\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.4557\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.4551\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.4545\u001b[0m  0.0330\n",
      "     74        \u001b[36m0.4540\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.4534\u001b[0m  0.0330\n",
      "     76        \u001b[36m0.4529\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.4523\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.4518\u001b[0m  0.0330\n",
      "     79        \u001b[36m0.4513\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.4508\u001b[0m  0.0330\n",
      "     81        \u001b[36m0.4503\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.4497\u001b[0m  0.0330\n",
      "     83        \u001b[36m0.4492\u001b[0m  0.0330\n",
      "     84        \u001b[36m0.4487\u001b[0m  0.0330\n",
      "     85        \u001b[36m0.4483\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.4478\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.4473\u001b[0m  0.0380\n",
      "     88        \u001b[36m0.4468\u001b[0m  0.0350\n",
      "     89        \u001b[36m0.4463\u001b[0m  0.0350\n",
      "     90        \u001b[36m0.4459\u001b[0m  0.0330\n",
      "     91        \u001b[36m0.4454\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.4449\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.4445\u001b[0m  0.0330\n",
      "     94        \u001b[36m0.4440\u001b[0m  0.0330\n",
      "     95        \u001b[36m0.4436\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.4431\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.4427\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.4423\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.4419\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.4414\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7824\u001b[0m  0.0340\n",
      "      2        \u001b[36m0.7780\u001b[0m  0.0330\n",
      "      3        \u001b[36m0.7752\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.7716\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.7690\u001b[0m  0.0350\n",
      "      6        \u001b[36m0.7665\u001b[0m  0.0330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      7        \u001b[36m0.7641\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.7618\u001b[0m  0.0330\n",
      "      9        \u001b[36m0.7596\u001b[0m  0.0330\n",
      "     10        \u001b[36m0.7574\u001b[0m  0.0340\n",
      "     11        \u001b[36m0.7554\u001b[0m  0.0330\n",
      "     12        \u001b[36m0.7534\u001b[0m  0.0330\n",
      "     13        \u001b[36m0.7515\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.7498\u001b[0m  0.0330\n",
      "     15        \u001b[36m0.7480\u001b[0m  0.0350\n",
      "     16        \u001b[36m0.7464\u001b[0m  0.0330\n",
      "     17        \u001b[36m0.7449\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.7434\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.7420\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.7406\u001b[0m  0.0350\n",
      "     21        \u001b[36m0.7393\u001b[0m  0.0330\n",
      "     22        \u001b[36m0.7381\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.7369\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.7358\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.7347\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.7337\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.7327\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.7318\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.7309\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.7300\u001b[0m  0.0330\n",
      "     31        \u001b[36m0.7292\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.7284\u001b[0m  0.0330\n",
      "     33        \u001b[36m0.7277\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.7269\u001b[0m  0.0340\n",
      "     35        \u001b[36m0.7262\u001b[0m  0.0330\n",
      "     36        \u001b[36m0.7255\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.7249\u001b[0m  0.0330\n",
      "     38        \u001b[36m0.7242\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.7236\u001b[0m  0.0330\n",
      "     40        \u001b[36m0.7230\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.7225\u001b[0m  0.0330\n",
      "     42        \u001b[36m0.7219\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.7214\u001b[0m  0.0330\n",
      "     44        \u001b[36m0.7209\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.7204\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.7199\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.7194\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.7189\u001b[0m  0.0330\n",
      "     49        \u001b[36m0.7185\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.7181\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.7176\u001b[0m  0.0330\n",
      "     52        \u001b[36m0.7172\u001b[0m  0.0330\n",
      "     53        \u001b[36m0.7168\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.7164\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.7160\u001b[0m  0.0330\n",
      "     56        \u001b[36m0.7157\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.7153\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.7149\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.7146\u001b[0m  0.0330\n",
      "     60        \u001b[36m0.7142\u001b[0m  0.0330\n",
      "     61        \u001b[36m0.7139\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.7136\u001b[0m  0.0330\n",
      "     63        \u001b[36m0.7133\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.7129\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.7126\u001b[0m  0.0330\n",
      "     66        \u001b[36m0.7123\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.7120\u001b[0m  0.0330\n",
      "     68        \u001b[36m0.7117\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.7114\u001b[0m  0.0370\n",
      "     70        \u001b[36m0.7112\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.7109\u001b[0m  0.0330\n",
      "     72        \u001b[36m0.7106\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.7103\u001b[0m  0.0330\n",
      "     74        \u001b[36m0.7101\u001b[0m  0.0350\n",
      "     75        \u001b[36m0.7098\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.7095\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.7093\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.7090\u001b[0m  0.0330\n",
      "     79        \u001b[36m0.7088\u001b[0m  0.0330\n",
      "     80        \u001b[36m0.7085\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.7083\u001b[0m  0.0330\n",
      "     82        \u001b[36m0.7080\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.7078\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.7076\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.7073\u001b[0m  0.0330\n",
      "     86        \u001b[36m0.7071\u001b[0m  0.0350\n",
      "     87        \u001b[36m0.7069\u001b[0m  0.0350\n",
      "     88        \u001b[36m0.7066\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.7064\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.7062\u001b[0m  0.0330\n",
      "     91        \u001b[36m0.7060\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.7057\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.7055\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.7053\u001b[0m  0.0330\n",
      "     95        \u001b[36m0.7051\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.7049\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.7047\u001b[0m  0.0330\n",
      "     98        \u001b[36m0.7045\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.7042\u001b[0m  0.0330\n",
      "    100        \u001b[36m0.7040\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.8436\u001b[0m  0.0340\n",
      "      2        \u001b[36m0.8425\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.8414\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.8403\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.8392\u001b[0m  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.8380\u001b[0m  0.0340\n",
      "      7        \u001b[36m0.8369\u001b[0m  0.0330\n",
      "      8        \u001b[36m0.8357\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.8345\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.8333\u001b[0m  0.0330\n",
      "     11        \u001b[36m0.8320\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.8308\u001b[0m  0.0340\n",
      "     13        \u001b[36m0.8295\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.8283\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.8270\u001b[0m  0.0330\n",
      "     16        \u001b[36m0.8257\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.8244\u001b[0m  0.0330\n",
      "     18        \u001b[36m0.8231\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.8217\u001b[0m  0.0330\n",
      "     20        \u001b[36m0.8204\u001b[0m  0.0330\n",
      "     21        \u001b[36m0.8190\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.8176\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.8162\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.8148\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.8134\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.8120\u001b[0m  0.0330\n",
      "     27        \u001b[36m0.8105\u001b[0m  0.0350\n",
      "     28        \u001b[36m0.8091\u001b[0m  0.0330\n",
      "     29        \u001b[36m0.8076\u001b[0m  0.0330\n",
      "     30        \u001b[36m0.8061\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.8046\u001b[0m  0.0380\n",
      "     32        \u001b[36m0.8031\u001b[0m  0.0330\n",
      "     33        \u001b[36m0.8016\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.8001\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.7985\u001b[0m  0.0350\n",
      "     36        \u001b[36m0.7970\u001b[0m  0.0340\n",
      "     37        \u001b[36m0.7954\u001b[0m  0.0330\n",
      "     38        \u001b[36m0.7939\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.7923\u001b[0m  0.0330\n",
      "     40        \u001b[36m0.7907\u001b[0m  0.0350\n",
      "     41        \u001b[36m0.7890\u001b[0m  0.0330\n",
      "     42        \u001b[36m0.7874\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.7858\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.7841\u001b[0m  0.0330\n",
      "     45        \u001b[36m0.7824\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.7807\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.7790\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.7773\u001b[0m  0.0330\n",
      "     49        \u001b[36m0.7755\u001b[0m  0.0340\n",
      "     50        \u001b[36m0.7738\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.7720\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.7702\u001b[0m  0.0340\n",
      "     53        \u001b[36m0.7683\u001b[0m  0.0330\n",
      "     54        \u001b[36m0.7665\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.7646\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.7626\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.7607\u001b[0m  0.0330\n",
      "     58        \u001b[36m0.7587\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.7567\u001b[0m  0.0330\n",
      "     60        \u001b[36m0.7546\u001b[0m  0.0350\n",
      "     61        \u001b[36m0.7526\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.7504\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.7482\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.7460\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.7438\u001b[0m  0.0330\n",
      "     66        \u001b[36m0.7414\u001b[0m  0.0330\n",
      "     67        \u001b[36m0.7390\u001b[0m  0.0330\n",
      "     68        \u001b[36m0.7366\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.7341\u001b[0m  0.0330\n",
      "     70        \u001b[36m0.7315\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.7288\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.7261\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.7232\u001b[0m  0.0330\n",
      "     74        \u001b[36m0.7203\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.7173\u001b[0m  0.0330\n",
      "     76        \u001b[36m0.7142\u001b[0m  0.0330\n",
      "     77        \u001b[36m0.7109\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.7075\u001b[0m  0.0330\n",
      "     79        \u001b[36m0.7041\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.7004\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.6967\u001b[0m  0.0330\n",
      "     82        \u001b[36m0.6928\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.6887\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.6845\u001b[0m  0.0330\n",
      "     85        \u001b[36m0.6801\u001b[0m  0.0330\n",
      "     86        \u001b[36m0.6756\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.6710\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.6661\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.6612\u001b[0m  0.0330\n",
      "     90        \u001b[36m0.6561\u001b[0m  0.0350\n",
      "     91        \u001b[36m0.6509\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.6455\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.6401\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.6346\u001b[0m  0.0400\n",
      "     95        \u001b[36m0.6290\u001b[0m  0.0360\n",
      "     96        \u001b[36m0.6234\u001b[0m  0.0350\n",
      "     97        \u001b[36m0.6178\u001b[0m  0.0350\n",
      "     98        \u001b[36m0.6122\u001b[0m  0.0330\n",
      "     99        \u001b[36m0.6066\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.6010\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5215\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.5193\u001b[0m  0.0330\n",
      "      3        \u001b[36m0.5176\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.5159\u001b[0m  0.0350\n",
      "      5        \u001b[36m0.5142\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.5123\u001b[0m  0.0330\n",
      "      7        \u001b[36m0.5107\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.5092\u001b[0m  0.0350\n",
      "      9        \u001b[36m0.5077\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.5062\u001b[0m  0.0330\n",
      "     11        \u001b[36m0.5048\u001b[0m  0.0330\n",
      "     12        \u001b[36m0.5034\u001b[0m  0.0350\n",
      "     13        \u001b[36m0.5020\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.5007\u001b[0m  0.0360\n",
      "     15        \u001b[36m0.4993\u001b[0m  0.0330\n",
      "     16        \u001b[36m0.4981\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.4968\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.4956\u001b[0m  0.0350\n",
      "     19        \u001b[36m0.4944\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.4932\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.4921\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.4909\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.4898\u001b[0m  0.0330\n",
      "     24        \u001b[36m0.4888\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.4877\u001b[0m  0.0330\n",
      "     26        \u001b[36m0.4867\u001b[0m  0.0330\n",
      "     27        \u001b[36m0.4857\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.4847\u001b[0m  0.0330\n",
      "     29        \u001b[36m0.4837\u001b[0m  0.0330\n",
      "     30        \u001b[36m0.4828\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.4819\u001b[0m  0.0330\n",
      "     32        \u001b[36m0.4810\u001b[0m  0.0350\n",
      "     33        \u001b[36m0.4801\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.4792\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.4784\u001b[0m  0.0330\n",
      "     36        \u001b[36m0.4775\u001b[0m  0.0340\n",
      "     37        \u001b[36m0.4767\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.4759\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.4752\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.4744\u001b[0m  0.0330\n",
      "     41        \u001b[36m0.4736\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.4729\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.4722\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.4715\u001b[0m  0.0330\n",
      "     45        \u001b[36m0.4708\u001b[0m  0.0330\n",
      "     46        \u001b[36m0.4701\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.4694\u001b[0m  0.0350\n",
      "     48        \u001b[36m0.4688\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.4681\u001b[0m  0.0340\n",
      "     50        \u001b[36m0.4675\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.4669\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.4663\u001b[0m  0.0340\n",
      "     53        \u001b[36m0.4657\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.4651\u001b[0m  0.0330\n",
      "     55        \u001b[36m0.4645\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.4639\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.4634\u001b[0m  0.0330\n",
      "     58        \u001b[36m0.4628\u001b[0m  0.0350\n",
      "     59        \u001b[36m0.4623\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.4617\u001b[0m  0.0330\n",
      "     61        \u001b[36m0.4612\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.4607\u001b[0m  0.0330\n",
      "     63        \u001b[36m0.4602\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.4597\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.4592\u001b[0m  0.0330\n",
      "     66        \u001b[36m0.4587\u001b[0m  0.0350\n",
      "     67        \u001b[36m0.4583\u001b[0m  0.0350\n",
      "     68        \u001b[36m0.4578\u001b[0m  0.0330\n",
      "     69        \u001b[36m0.4573\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.4569\u001b[0m  0.0330\n",
      "     71        \u001b[36m0.4564\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.4560\u001b[0m  0.0380\n",
      "     73        \u001b[36m0.4556\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.4552\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.4547\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.4543\u001b[0m  0.0330\n",
      "     77        \u001b[36m0.4539\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.4535\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.4531\u001b[0m  0.0330\n",
      "     80        \u001b[36m0.4527\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.4524\u001b[0m  0.0330\n",
      "     82        \u001b[36m0.4520\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.4516\u001b[0m  0.0330\n",
      "     84        \u001b[36m0.4512\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.4509\u001b[0m  0.0330\n",
      "     86        \u001b[36m0.4505\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.4502\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.4498\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.4495\u001b[0m  0.0330\n",
      "     90        \u001b[36m0.4491\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.4488\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.4485\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.4482\u001b[0m  0.0330\n",
      "     94        \u001b[36m0.4478\u001b[0m  0.0330\n",
      "     95        \u001b[36m0.4475\u001b[0m  0.0350\n",
      "     96        \u001b[36m0.4472\u001b[0m  0.0330\n",
      "     97        \u001b[36m0.4469\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.4466\u001b[0m  0.0330\n",
      "     99        \u001b[36m0.4463\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.4460\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6874\u001b[0m  0.0440\n",
      "      2        \u001b[36m0.5558\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.5138\u001b[0m  0.0430\n",
      "      4        \u001b[36m0.4902\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.4706\u001b[0m  0.0440\n",
      "      6        \u001b[36m0.4472\u001b[0m  0.0450\n",
      "      7        \u001b[36m0.4350\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.4259\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.4190\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.4142\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.4113\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.4092\u001b[0m  0.0440\n",
      "     13        \u001b[36m0.4075\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.4058\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.4041\u001b[0m  0.0430\n",
      "     16        \u001b[36m0.4027\u001b[0m  0.0490\n",
      "     17        \u001b[36m0.4010\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.3989\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.3958\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3915\u001b[0m  0.0440\n",
      "     21        \u001b[36m0.3872\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3840\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3818\u001b[0m  0.0440\n",
      "     24        \u001b[36m0.3801\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3785\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3773\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.3765\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.3757\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3752\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     32        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     35        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     36        \u001b[36m0.3733\u001b[0m  0.0440\n",
      "     37        \u001b[36m0.3731\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3729\u001b[0m  0.0440\n",
      "     39        \u001b[36m0.3728\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3727\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     43        \u001b[36m0.3725\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3725\u001b[0m  0.0450\n",
      "     45        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3723\u001b[0m  0.0450\n",
      "     47        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     51        0.3722  0.0440\n",
      "     52        0.3721  0.0440\n",
      "     53        0.3722  0.0430\n",
      "     54        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     57        \u001b[36m0.3719\u001b[0m  0.0430\n",
      "     58        0.3719  0.0440\n",
      "     59        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     60        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     61        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     65        0.3719  0.0430\n",
      "     66        0.3718  0.0430\n",
      "     67        0.3717  0.0430\n",
      "     68        0.3717  0.0460\n",
      "     69        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     70        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     71        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     72        0.3717  0.0440\n",
      "     73        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     76        0.3717  0.0430\n",
      "     77        0.3717  0.0430\n",
      "     78        0.3717  0.0430\n",
      "     79        0.3717  0.0440\n",
      "     80        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     81        0.3717  0.0440\n",
      "     82        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     83        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     87        \u001b[36m0.3716\u001b[0m  0.0450\n",
      "     88        \u001b[36m0.3715\u001b[0m  0.0470\n",
      "     89        \u001b[36m0.3715\u001b[0m  0.0430\n",
      "     90        \u001b[36m0.3715\u001b[0m  0.0430\n",
      "     91        \u001b[36m0.3715\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3715\u001b[0m  0.0440\n",
      "     93        \u001b[36m0.3715\u001b[0m  0.0430\n",
      "     94        \u001b[36m0.3715\u001b[0m  0.0440\n",
      "     95        0.3715  0.0440\n",
      "     96        0.3715  0.0450\n",
      "     97        \u001b[36m0.3715\u001b[0m  0.0430\n",
      "     98        0.3715  0.0430\n",
      "     99        \u001b[36m0.3715\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3715\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7672\u001b[0m  0.0440\n",
      "      2        \u001b[36m0.6076\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.5256\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.4749\u001b[0m  0.0430"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      5        \u001b[36m0.4353\u001b[0m  0.0430\n",
      "      6        \u001b[36m0.4131\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4021\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.3949\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.3912\u001b[0m  0.0450\n",
      "     10        \u001b[36m0.3890\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.3875\u001b[0m  0.0440\n",
      "     12        \u001b[36m0.3847\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3820\u001b[0m  0.0440\n",
      "     14        \u001b[36m0.3803\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3790\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.3782\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3775\u001b[0m  0.0450\n",
      "     18        \u001b[36m0.3769\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3767\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3762\u001b[0m  0.0440\n",
      "     21        \u001b[36m0.3759\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3755\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3750\u001b[0m  0.0440\n",
      "     24        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     25        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     26        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     30        \u001b[36m0.3735\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3733\u001b[0m  0.0430\n",
      "     32        \u001b[36m0.3732\u001b[0m  0.0440\n",
      "     33        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     34        0.3729  0.0440\n",
      "     35        \u001b[36m0.3728\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3727\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3726\u001b[0m  0.0440\n",
      "     38        \u001b[36m0.3725\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3725\u001b[0m  0.0440\n",
      "     40        \u001b[36m0.3724\u001b[0m  0.0440\n",
      "     41        0.3724  0.0430\n",
      "     42        \u001b[36m0.3724\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3724\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.3723\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3722\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3721\u001b[0m  0.0450\n",
      "     49        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3721\u001b[0m  0.0440\n",
      "     51        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     52        \u001b[36m0.3721\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     54        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3720\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3720\u001b[0m  0.0440\n",
      "     58        0.3720  0.0440\n",
      "     59        0.3720  0.0440\n",
      "     60        0.3720  0.0440\n",
      "     61        0.3720  0.0430\n",
      "     62        \u001b[36m0.3719\u001b[0m  0.0530\n",
      "     63        \u001b[36m0.3719\u001b[0m  0.0440\n",
      "     64        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     65        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     66        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     68        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     69        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     70        \u001b[36m0.3718\u001b[0m  0.0450\n",
      "     71        \u001b[36m0.3718\u001b[0m  0.0430\n",
      "     72        \u001b[36m0.3718\u001b[0m  0.0440\n",
      "     73        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     74        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3717\u001b[0m  0.0450\n",
      "     78        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     80        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     81        \u001b[36m0.3717\u001b[0m  0.0440\n",
      "     82        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     83        \u001b[36m0.3717\u001b[0m  0.0430\n",
      "     84        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     85        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     87        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     88        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     89        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     90        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     91        \u001b[36m0.3716\u001b[0m  0.0450\n",
      "     92        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     93        \u001b[36m0.3716\u001b[0m  0.0440\n",
      "     94        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "     95        0.3716  0.0480\n",
      "     96        0.3716  0.0440\n",
      "     97        0.3716  0.0430\n",
      "     98        0.3716  0.0440\n",
      "     99        \u001b[36m0.3716\u001b[0m  0.0430\n",
      "    100        \u001b[36m0.3716\u001b[0m  0.0500\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7851\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.7061\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.6225\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.5692\u001b[0m  0.0440"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      5        \u001b[36m0.5162\u001b[0m  0.0450\n",
      "      6        \u001b[36m0.4429\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4359\u001b[0m  0.0440\n",
      "      8        \u001b[36m0.4334\u001b[0m  0.0430\n",
      "      9        \u001b[36m0.4316\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.4295\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.4258\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.4158\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3926\u001b[0m  0.0440\n",
      "     14        \u001b[36m0.3813\u001b[0m  0.0430\n",
      "     15        \u001b[36m0.3791\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.3780\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3774\u001b[0m  0.0450\n",
      "     18        \u001b[36m0.3769\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.3765\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3762\u001b[0m  0.0420\n",
      "     21        \u001b[36m0.3759\u001b[0m  0.0430\n",
      "     22        \u001b[36m0.3757\u001b[0m  0.0440\n",
      "     23        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     24        \u001b[36m0.3754\u001b[0m  0.0450\n",
      "     25        0.3756  0.0430\n",
      "     26        0.3757  0.0440\n",
      "     27        0.3755  0.0430\n",
      "     28        \u001b[36m0.3749\u001b[0m  0.0430\n",
      "     29        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     30        \u001b[36m0.3746\u001b[0m  0.0450\n",
      "     31        \u001b[36m0.3746\u001b[0m  0.0440\n",
      "     32        \u001b[36m0.3745\u001b[0m  0.0440\n",
      "     33        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     36        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     38        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     39        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     46        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     48        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     50        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     53        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     58        0.3739  0.0440\n",
      "     59        0.3739  0.0440\n",
      "     60        0.3739  0.0500\n",
      "     61        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     63        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     67        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     68        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     69        0.3740  0.0430\n",
      "     70        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     71        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     72        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     73        0.3738  0.0440\n",
      "     74        0.3738  0.0430\n",
      "     75        0.3738  0.0450\n",
      "     76        0.3738  0.0430\n",
      "     77        0.3738  0.0440\n",
      "     78        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     79        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     80        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     81        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     82        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     83        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     86        \u001b[36m0.3738\u001b[0m  0.0450\n",
      "     87        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     88        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     89        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     90        \u001b[36m0.3737\u001b[0m  0.0450\n",
      "     91        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     93        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     94        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     95        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     97        \u001b[36m0.3737\u001b[0m  0.0520\n",
      "     98        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "     99        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3737\u001b[0m  0.0430\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7583\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.6230\u001b[0m  0.0430\n",
      "      3        \u001b[36m0.5436\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.5157\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.4925\u001b[0m  0.0450\n",
      "      6        \u001b[36m0.4656\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.4532\u001b[0m  0.0430\n",
      "      8        \u001b[36m0.4463\u001b[0m  0.0440\n",
      "      9        \u001b[36m0.4413\u001b[0m  0.0450\n",
      "     10        \u001b[36m0.4365\u001b[0m  0.0430\n",
      "     11        \u001b[36m0.4320\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.4268\u001b[0m  0.0450\n",
      "     13        \u001b[36m0.4222\u001b[0m  0.0440\n",
      "     14        \u001b[36m0.4174\u001b[0m  0.0460\n",
      "     15        \u001b[36m0.4129\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.4086\u001b[0m  0.0440\n",
      "     17        \u001b[36m0.4048\u001b[0m  0.0430\n",
      "     18        \u001b[36m0.4016\u001b[0m  0.0440\n",
      "     19        \u001b[36m0.3978\u001b[0m  0.0430\n",
      "     20        \u001b[36m0.3947\u001b[0m  0.0450\n",
      "     21        \u001b[36m0.3916\u001b[0m  0.0440\n",
      "     22        \u001b[36m0.3887\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3864\u001b[0m  0.0440\n",
      "     24        \u001b[36m0.3844\u001b[0m  0.0430\n",
      "     25        \u001b[36m0.3830\u001b[0m  0.0430\n",
      "     26        \u001b[36m0.3823\u001b[0m  0.0440\n",
      "     27        \u001b[36m0.3810\u001b[0m  0.0440\n",
      "     28        \u001b[36m0.3807\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3797\u001b[0m  0.0430\n",
      "     30        \u001b[36m0.3788\u001b[0m  0.0430\n",
      "     31        \u001b[36m0.3781\u001b[0m  0.0450\n",
      "     32        \u001b[36m0.3774\u001b[0m  0.0430\n",
      "     33        \u001b[36m0.3770\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3768\u001b[0m  0.0430\n",
      "     35        \u001b[36m0.3764\u001b[0m  0.0450\n",
      "     36        \u001b[36m0.3762\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3760\u001b[0m  0.0430\n",
      "     38        \u001b[36m0.3758\u001b[0m  0.0430\n",
      "     39        \u001b[36m0.3756\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3755\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3753\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.3752\u001b[0m  0.0430\n",
      "     43        \u001b[36m0.3752\u001b[0m  0.0440\n",
      "     44        \u001b[36m0.3751\u001b[0m  0.0430\n",
      "     45        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     46        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     47        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     48        \u001b[36m0.3748\u001b[0m  0.0430\n",
      "     49        \u001b[36m0.3747\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3747\u001b[0m  0.0440\n",
      "     51        \u001b[36m0.3746\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     53        0.3746  0.0440\n",
      "     54        0.3746  0.0440\n",
      "     55        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     56        \u001b[36m0.3745\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3745\u001b[0m  0.0480\n",
      "     58        \u001b[36m0.3744\u001b[0m  0.0440\n",
      "     59        \u001b[36m0.3744\u001b[0m  0.0450\n",
      "     60        \u001b[36m0.3743\u001b[0m  0.0430\n",
      "     61        \u001b[36m0.3743\u001b[0m  0.0440\n",
      "     62        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     63        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     64        \u001b[36m0.3742\u001b[0m  0.0430\n",
      "     65        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     66        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     67        \u001b[36m0.3741\u001b[0m  0.0450\n",
      "     68        \u001b[36m0.3741\u001b[0m  0.0450\n",
      "     69        \u001b[36m0.3741\u001b[0m  0.0450\n",
      "     70        \u001b[36m0.3741\u001b[0m  0.0470\n",
      "     71        \u001b[36m0.3741\u001b[0m  0.0450\n",
      "     72        0.3741  0.0440\n",
      "     73        0.3741  0.0430\n",
      "     74        \u001b[36m0.3741\u001b[0m  0.0440\n",
      "     75        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     76        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     77        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     78        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     79        \u001b[36m0.3740\u001b[0m  0.0440\n",
      "     80        \u001b[36m0.3740\u001b[0m  0.0450\n",
      "     81        \u001b[36m0.3740\u001b[0m  0.0450\n",
      "     82        \u001b[36m0.3740\u001b[0m  0.0450\n",
      "     83        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     84        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     85        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     86        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     87        \u001b[36m0.3739\u001b[0m  0.0450\n",
      "     88        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     89        \u001b[36m0.3739\u001b[0m  0.0450\n",
      "     90        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     91        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     92        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     93        \u001b[36m0.3739\u001b[0m  0.0470\n",
      "     94        \u001b[36m0.3739\u001b[0m  0.0450\n",
      "     95        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     96        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     97        \u001b[36m0.3739\u001b[0m  0.0450\n",
      "     98        0.3739  0.0440\n",
      "     99        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "    100        \u001b[36m0.3738\u001b[0m  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5661\u001b[0m  0.0430\n",
      "      2        \u001b[36m0.5078\u001b[0m  0.0440\n",
      "      3        \u001b[36m0.4468\u001b[0m  0.0440\n",
      "      4        \u001b[36m0.4145\u001b[0m  0.0440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5        \u001b[36m0.4061\u001b[0m  0.0450\n",
      "      6        \u001b[36m0.4017\u001b[0m  0.0440\n",
      "      7        \u001b[36m0.3973\u001b[0m  0.0440\n",
      "      8        \u001b[36m0.3949\u001b[0m  0.0450\n",
      "      9        \u001b[36m0.3939\u001b[0m  0.0440\n",
      "     10        \u001b[36m0.3913\u001b[0m  0.0440\n",
      "     11        \u001b[36m0.3890\u001b[0m  0.0430\n",
      "     12        \u001b[36m0.3847\u001b[0m  0.0430\n",
      "     13        \u001b[36m0.3817\u001b[0m  0.0430\n",
      "     14        \u001b[36m0.3799\u001b[0m  0.0440\n",
      "     15        \u001b[36m0.3788\u001b[0m  0.0440\n",
      "     16        \u001b[36m0.3783\u001b[0m  0.0430\n",
      "     17        \u001b[36m0.3776\u001b[0m  0.0440\n",
      "     18        \u001b[36m0.3773\u001b[0m  0.0430\n",
      "     19        \u001b[36m0.3771\u001b[0m  0.0440\n",
      "     20        \u001b[36m0.3758\u001b[0m  0.0440\n",
      "     21        \u001b[36m0.3753\u001b[0m  0.0450\n",
      "     22        \u001b[36m0.3750\u001b[0m  0.0430\n",
      "     23        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     24        \u001b[36m0.3749\u001b[0m  0.0440\n",
      "     25        0.3749  0.0440\n",
      "     26        \u001b[36m0.3746\u001b[0m  0.0430\n",
      "     27        \u001b[36m0.3744\u001b[0m  0.0430\n",
      "     28        \u001b[36m0.3742\u001b[0m  0.0440\n",
      "     29        \u001b[36m0.3741\u001b[0m  0.0430\n",
      "     30        0.3747  0.0430\n",
      "     31        0.3743  0.0430\n",
      "     32        0.3743  0.0440\n",
      "     33        \u001b[36m0.3740\u001b[0m  0.0430\n",
      "     34        \u001b[36m0.3739\u001b[0m  0.0440\n",
      "     35        \u001b[36m0.3739\u001b[0m  0.0430\n",
      "     36        \u001b[36m0.3738\u001b[0m  0.0430\n",
      "     37        \u001b[36m0.3737\u001b[0m  0.0440\n",
      "     38        \u001b[36m0.3736\u001b[0m  0.0440\n",
      "     39        \u001b[36m0.3736\u001b[0m  0.0430\n",
      "     40        \u001b[36m0.3735\u001b[0m  0.0440\n",
      "     41        \u001b[36m0.3734\u001b[0m  0.0430\n",
      "     42        \u001b[36m0.3734\u001b[0m  0.0440\n",
      "     43        \u001b[36m0.3732\u001b[0m  0.0430\n",
      "     44        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "     45        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "     46        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "     47        0.3730  0.0450\n",
      "     48        0.3730  0.0440\n",
      "     49        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "     50        \u001b[36m0.3730\u001b[0m  0.0430\n",
      "     51        \u001b[36m0.3730\u001b[0m  0.0440\n",
      "     52        \u001b[36m0.3729\u001b[0m  0.0440\n",
      "     53        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     54        \u001b[36m0.3729\u001b[0m  0.0440\n",
      "     55        \u001b[36m0.3729\u001b[0m  0.0440\n",
      "     56        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     57        \u001b[36m0.3729\u001b[0m  0.0450\n",
      "     58        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     59        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     60        \u001b[36m0.3729\u001b[0m  0.0440\n",
      "     61        \u001b[36m0.3729\u001b[0m  0.0470\n",
      "     62        \u001b[36m0.3729\u001b[0m  0.0450\n",
      "     63        0.3729  0.0430\n",
      "     64        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     65        0.3729  0.0440\n",
      "     66        0.3729  0.0430\n",
      "     67        0.3729  0.0430\n",
      "     68        0.3729  0.0430\n",
      "     69        0.3729  0.0440\n",
      "     70        0.3729  0.0440\n",
      "     71        0.3729  0.0440\n",
      "     72        0.3729  0.0430\n",
      "     73        0.3729  0.0440\n",
      "     74        0.3729  0.0440\n",
      "     75        \u001b[36m0.3729\u001b[0m  0.0440\n",
      "     76        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     77        0.3729  0.0450\n",
      "     78        0.3729  0.0430\n",
      "     79        0.3729  0.0440\n",
      "     80        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     81        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     82        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     83        \u001b[36m0.3729\u001b[0m  0.0430\n",
      "     84        0.3729  0.0460\n",
      "     85        0.3729  0.0440\n",
      "     86        0.3729  0.0440\n",
      "     87        0.3729  0.0440\n",
      "     88        0.3729  0.0430\n",
      "     89        0.3729  0.0450\n",
      "     90        0.3729  0.0440\n",
      "     91        0.3729  0.0440\n",
      "     92        0.3729  0.0440\n",
      "     93        0.3729  0.0440\n",
      "     94        0.3729  0.0440\n",
      "     95        0.3729  0.0430\n",
      "     96        0.3729  0.0440\n",
      "     97        0.3729  0.0440\n",
      "     98        0.3729  0.0440\n",
      "     99        0.3729  0.0440\n",
      "    100        0.3729  0.0440\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.5936\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.5721\u001b[0m  0.0350\n",
      "      3        \u001b[36m0.5676\u001b[0m  0.0330\n",
      "      4        \u001b[36m0.5603\u001b[0m  0.0370\n",
      "      5        \u001b[36m0.5414\u001b[0m  0.0340\n",
      "      6        \u001b[36m0.5377\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.5343\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.5310\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.5280\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.5251\u001b[0m  0.0340\n",
      "     11        \u001b[36m0.5224\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.5198\u001b[0m  0.0350\n",
      "     13        \u001b[36m0.5174\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.5151\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.5129\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.5108\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.5088\u001b[0m  0.0330\n",
      "     18        \u001b[36m0.5069\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.5051\u001b[0m  0.0350\n",
      "     20        \u001b[36m0.5034\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.5017\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.5002\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.4987\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.4973\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.4959\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.4946\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.4933\u001b[0m  0.0330\n",
      "     28        \u001b[36m0.4921\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.4909\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.4898\u001b[0m  0.0330\n",
      "     31        \u001b[36m0.4888\u001b[0m  0.0350\n",
      "     32        \u001b[36m0.4877\u001b[0m  0.0330\n",
      "     33        \u001b[36m0.4867\u001b[0m  0.0330\n",
      "     34        \u001b[36m0.4858\u001b[0m  0.0340\n",
      "     35        \u001b[36m0.4849\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.4840\u001b[0m  0.0330\n",
      "     37        \u001b[36m0.4831\u001b[0m  0.0350\n",
      "     38        \u001b[36m0.4823\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.4815\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.4807\u001b[0m  0.0330\n",
      "     41        \u001b[36m0.4800\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.4792\u001b[0m  0.0330\n",
      "     43        \u001b[36m0.4785\u001b[0m  0.0350\n",
      "     44        \u001b[36m0.4778\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.4772\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.4765\u001b[0m  0.0330\n",
      "     47        \u001b[36m0.4759\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.4753\u001b[0m  0.0360\n",
      "     49        \u001b[36m0.4745\u001b[0m  0.0340\n",
      "     50        \u001b[36m0.4739\u001b[0m  0.0330\n",
      "     51        \u001b[36m0.4734\u001b[0m  0.0330\n",
      "     52        \u001b[36m0.4729\u001b[0m  0.0340\n",
      "     53        \u001b[36m0.4723\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.4718\u001b[0m  0.0350\n",
      "     55        \u001b[36m0.4713\u001b[0m  0.0350\n",
      "     56        \u001b[36m0.4708\u001b[0m  0.0330\n",
      "     57        \u001b[36m0.4704\u001b[0m  0.0350\n",
      "     58        \u001b[36m0.4699\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.4695\u001b[0m  0.0330\n",
      "     60        \u001b[36m0.4690\u001b[0m  0.0350\n",
      "     61        \u001b[36m0.4686\u001b[0m  0.0330\n",
      "     62        \u001b[36m0.4682\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.4677\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.4673\u001b[0m  0.0330\n",
      "     65        \u001b[36m0.4669\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.4666\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.4662\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.4658\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.4654\u001b[0m  0.0330\n",
      "     70        \u001b[36m0.4651\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.4647\u001b[0m  0.0390\n",
      "     72        \u001b[36m0.4644\u001b[0m  0.0350\n",
      "     73        \u001b[36m0.4640\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.4637\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.4634\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.4631\u001b[0m  0.0350\n",
      "     77        \u001b[36m0.4627\u001b[0m  0.0340\n",
      "     78        \u001b[36m0.4624\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.4621\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.4618\u001b[0m  0.0330\n",
      "     81        \u001b[36m0.4615\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.4612\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.4609\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.4607\u001b[0m  0.0330\n",
      "     85        \u001b[36m0.4604\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.4601\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.4598\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.4596\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.4593\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.4590\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.4588\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.4585\u001b[0m  0.0350\n",
      "     93        \u001b[36m0.4583\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.4580\u001b[0m  0.0330\n",
      "     95        \u001b[36m0.4578\u001b[0m  0.0350\n",
      "     96        \u001b[36m0.4575\u001b[0m  0.0330\n",
      "     97        \u001b[36m0.4573\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.4571\u001b[0m  0.0350\n",
      "     99        \u001b[36m0.4568\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.4566\u001b[0m  0.0330\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7487\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.7354\u001b[0m  0.0330\n",
      "      3        \u001b[36m0.7303\u001b[0m  0.0370\n",
      "      4        \u001b[36m0.7259\u001b[0m  0.0350\n",
      "      5        \u001b[36m0.7201\u001b[0m  0.0340"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      6        \u001b[36m0.7164\u001b[0m  0.0340\n",
      "      7        \u001b[36m0.7128\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.7093\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.7058\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.7023\u001b[0m  0.0330\n",
      "     11        \u001b[36m0.6988\u001b[0m  0.0330\n",
      "     12        \u001b[36m0.6954\u001b[0m  0.0350\n",
      "     13        \u001b[36m0.6920\u001b[0m  0.0330\n",
      "     14        \u001b[36m0.6885\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.6851\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.6816\u001b[0m  0.0330\n",
      "     17        \u001b[36m0.6781\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.6746\u001b[0m  0.0330\n",
      "     19        \u001b[36m0.6710\u001b[0m  0.0330\n",
      "     20        \u001b[36m0.6673\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.6636\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.6598\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.6559\u001b[0m  0.0330\n",
      "     24        \u001b[36m0.6519\u001b[0m  0.0350\n",
      "     25        \u001b[36m0.6477\u001b[0m  0.0350\n",
      "     26        \u001b[36m0.6435\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.6390\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.6344\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.6296\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.6234\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.6170\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.6116\u001b[0m  0.0330\n",
      "     33        \u001b[36m0.6060\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.6002\u001b[0m  0.0350\n",
      "     35        \u001b[36m0.5941\u001b[0m  0.0340\n",
      "     36        \u001b[36m0.5879\u001b[0m  0.0340\n",
      "     37        \u001b[36m0.5815\u001b[0m  0.0330\n",
      "     38        \u001b[36m0.5749\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.5682\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.5615\u001b[0m  0.0330\n",
      "     41        \u001b[36m0.5547\u001b[0m  0.0350\n",
      "     42        \u001b[36m0.5480\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.5414\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.5349\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.5285\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.5224\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.5164\u001b[0m  0.0330\n",
      "     48        \u001b[36m0.5108\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.5053\u001b[0m  0.0340\n",
      "     50        \u001b[36m0.5001\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.4952\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.4905\u001b[0m  0.0330\n",
      "     53        \u001b[36m0.4861\u001b[0m  0.0350\n",
      "     54        \u001b[36m0.4819\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.4779\u001b[0m  0.0330\n",
      "     56        \u001b[36m0.4742\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.4706\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.4673\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.4641\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.4611\u001b[0m  0.0330\n",
      "     61        \u001b[36m0.4582\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.4556\u001b[0m  0.0330\n",
      "     63        \u001b[36m0.4530\u001b[0m  0.0350\n",
      "     64        \u001b[36m0.4506\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.4484\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.4462\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.4442\u001b[0m  0.0330\n",
      "     68        \u001b[36m0.4422\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.4404\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.4387\u001b[0m  0.0350\n",
      "     71        \u001b[36m0.4370\u001b[0m  0.0330\n",
      "     72        \u001b[36m0.4354\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.4339\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.4325\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.4311\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.4298\u001b[0m  0.0350\n",
      "     77        \u001b[36m0.4285\u001b[0m  0.0330\n",
      "     78        \u001b[36m0.4274\u001b[0m  0.0360\n",
      "     79        \u001b[36m0.4262\u001b[0m  0.0350\n",
      "     80        \u001b[36m0.4251\u001b[0m  0.0370\n",
      "     81        \u001b[36m0.4241\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.4230\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.4221\u001b[0m  0.0350\n",
      "     84        \u001b[36m0.4211\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.4202\u001b[0m  0.0350\n",
      "     86        \u001b[36m0.4194\u001b[0m  0.0330\n",
      "     87        \u001b[36m0.4185\u001b[0m  0.0330\n",
      "     88        \u001b[36m0.4177\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.4170\u001b[0m  0.0330\n",
      "     90        \u001b[36m0.4162\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.4155\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.4148\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.4141\u001b[0m  0.0340\n",
      "     94        \u001b[36m0.4135\u001b[0m  0.0330\n",
      "     95        \u001b[36m0.4128\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.4122\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.4116\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.4110\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.4105\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.4099\u001b[0m  0.0330\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7170\u001b[0m  0.0350\n",
      "      2        \u001b[36m0.7099\u001b[0m  0.0330\n",
      "      3        \u001b[36m0.7025\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.6949\u001b[0m  0.0330\n",
      "      5        \u001b[36m0.6871\u001b[0m  0.0340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      6        \u001b[36m0.6790\u001b[0m  0.0350\n",
      "      7        \u001b[36m0.6707\u001b[0m  0.0330\n",
      "      8        \u001b[36m0.6622\u001b[0m  0.0350\n",
      "      9        \u001b[36m0.6534\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.6445\u001b[0m  0.0340\n",
      "     11        \u001b[36m0.6354\u001b[0m  0.0350\n",
      "     12        \u001b[36m0.6263\u001b[0m  0.0340\n",
      "     13        \u001b[36m0.6170\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.6077\u001b[0m  0.0350\n",
      "     15        \u001b[36m0.5985\u001b[0m  0.0330\n",
      "     16        \u001b[36m0.5893\u001b[0m  0.0350\n",
      "     17        \u001b[36m0.5802\u001b[0m  0.0360\n",
      "     18        \u001b[36m0.5713\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.5626\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.5542\u001b[0m  0.0350\n",
      "     21        \u001b[36m0.5460\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.5382\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.5307\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.5235\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.5168\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.5103\u001b[0m  0.0350\n",
      "     27        \u001b[36m0.5043\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.4986\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.4932\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.4882\u001b[0m  0.0350\n",
      "     31        \u001b[36m0.4835\u001b[0m  0.0330\n",
      "     32        \u001b[36m0.4791\u001b[0m  0.0340\n",
      "     33        \u001b[36m0.4749\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.4710\u001b[0m  0.0340\n",
      "     35        \u001b[36m0.4674\u001b[0m  0.0330\n",
      "     36        \u001b[36m0.4640\u001b[0m  0.0350\n",
      "     37        \u001b[36m0.4608\u001b[0m  0.0340\n",
      "     38        \u001b[36m0.4577\u001b[0m  0.0340\n",
      "     39        \u001b[36m0.4549\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.4523\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.4498\u001b[0m  0.0440\n",
      "     42        \u001b[36m0.4474\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.4452\u001b[0m  0.0340\n",
      "     44        \u001b[36m0.4431\u001b[0m  0.0340\n",
      "     45        \u001b[36m0.4411\u001b[0m  0.0350\n",
      "     46        \u001b[36m0.4392\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.4374\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.4357\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.4341\u001b[0m  0.0350\n",
      "     50        \u001b[36m0.4326\u001b[0m  0.0352\n",
      "     51        \u001b[36m0.4312\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.4298\u001b[0m  0.0350\n",
      "     53        \u001b[36m0.4285\u001b[0m  0.0360\n",
      "     54        \u001b[36m0.4272\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.4260\u001b[0m  0.0360\n",
      "     56        \u001b[36m0.4249\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.4238\u001b[0m  0.0340\n",
      "     58        \u001b[36m0.4227\u001b[0m  0.0330\n",
      "     59        \u001b[36m0.4217\u001b[0m  0.0350\n",
      "     60        \u001b[36m0.4208\u001b[0m  0.0340\n",
      "     61        \u001b[36m0.4198\u001b[0m  0.0340\n",
      "     62        \u001b[36m0.4189\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.4181\u001b[0m  0.0340\n",
      "     64        \u001b[36m0.4173\u001b[0m  0.0340\n",
      "     65        \u001b[36m0.4165\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.4157\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.4150\u001b[0m  0.0340\n",
      "     68        \u001b[36m0.4143\u001b[0m  0.0350\n",
      "     69        \u001b[36m0.4136\u001b[0m  0.0330\n",
      "     70        \u001b[36m0.4129\u001b[0m  0.0350\n",
      "     71        \u001b[36m0.4123\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.4117\u001b[0m  0.0340\n",
      "     73        \u001b[36m0.4111\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.4105\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.4100\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.4094\u001b[0m  0.0330\n",
      "     77        \u001b[36m0.4089\u001b[0m  0.0350\n",
      "     78        \u001b[36m0.4084\u001b[0m  0.0350\n",
      "     79        \u001b[36m0.4079\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.4074\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.4070\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.4065\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.4061\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.4056\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.4052\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.4048\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.4044\u001b[0m  0.0350\n",
      "     88        \u001b[36m0.4041\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.4037\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.4033\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.4030\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.4026\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.4023\u001b[0m  0.0380\n",
      "     94        \u001b[36m0.4020\u001b[0m  0.0340\n",
      "     95        \u001b[36m0.4017\u001b[0m  0.0330\n",
      "     96        \u001b[36m0.4013\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.4010\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.4007\u001b[0m  0.0350\n",
      "     99        \u001b[36m0.4005\u001b[0m  0.0330\n",
      "    100        \u001b[36m0.4002\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.6935\u001b[0m  0.0330\n",
      "      2        \u001b[36m0.6906\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.6878\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.6850\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.6821\u001b[0m  0.0330"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      6        \u001b[36m0.6793\u001b[0m  0.0350\n",
      "      7        \u001b[36m0.6763\u001b[0m  0.0340\n",
      "      8        \u001b[36m0.6734\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.6691\u001b[0m  0.0330\n",
      "     10        \u001b[36m0.6594\u001b[0m  0.0340\n",
      "     11        \u001b[36m0.6562\u001b[0m  0.0350\n",
      "     12        \u001b[36m0.6529\u001b[0m  0.0340\n",
      "     13        \u001b[36m0.6493\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.6454\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.6413\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.6365\u001b[0m  0.0340\n",
      "     17        \u001b[36m0.6314\u001b[0m  0.0330\n",
      "     18        \u001b[36m0.6256\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.6191\u001b[0m  0.0340\n",
      "     20        \u001b[36m0.6113\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.6033\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.5951\u001b[0m  0.0340\n",
      "     23        \u001b[36m0.5867\u001b[0m  0.0330\n",
      "     24        \u001b[36m0.5785\u001b[0m  0.0330\n",
      "     25        \u001b[36m0.5709\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.5640\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.5580\u001b[0m  0.0350\n",
      "     28        \u001b[36m0.5529\u001b[0m  0.0340\n",
      "     29        \u001b[36m0.5485\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.5447\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.5415\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.5387\u001b[0m  0.0340\n",
      "     33        \u001b[36m0.5363\u001b[0m  0.0340\n",
      "     34        \u001b[36m0.5342\u001b[0m  0.0330\n",
      "     35        \u001b[36m0.5323\u001b[0m  0.0360\n",
      "     36        \u001b[36m0.5306\u001b[0m  0.0590\n",
      "     37        \u001b[36m0.5290\u001b[0m  0.0390\n",
      "     38        \u001b[36m0.5276\u001b[0m  0.0360\n",
      "     39        \u001b[36m0.5221\u001b[0m  0.0370\n",
      "     40        \u001b[36m0.5200\u001b[0m  0.0340\n",
      "     41        \u001b[36m0.5191\u001b[0m  0.0350\n",
      "     42        \u001b[36m0.5183\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.5176\u001b[0m  0.0350\n",
      "     44        \u001b[36m0.5168\u001b[0m  0.0350\n",
      "     45        \u001b[36m0.5161\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.5155\u001b[0m  0.0350\n",
      "     47        \u001b[36m0.5148\u001b[0m  0.0340\n",
      "     48        \u001b[36m0.5142\u001b[0m  0.0340\n",
      "     49        \u001b[36m0.5135\u001b[0m  0.0330\n",
      "     50        \u001b[36m0.5129\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.5123\u001b[0m  0.0340\n",
      "     52        \u001b[36m0.5118\u001b[0m  0.0340\n",
      "     53        \u001b[36m0.5112\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.5106\u001b[0m  0.0350\n",
      "     55        \u001b[36m0.5101\u001b[0m  0.0350\n",
      "     56        \u001b[36m0.5096\u001b[0m  0.0340\n",
      "     57        \u001b[36m0.5091\u001b[0m  0.0350\n",
      "     58        \u001b[36m0.5085\u001b[0m  0.0350\n",
      "     59        \u001b[36m0.5080\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.5076\u001b[0m  0.0340\n",
      "     61        \u001b[36m0.5071\u001b[0m  0.0330\n",
      "     62        \u001b[36m0.5066\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.5061\u001b[0m  0.0350\n",
      "     64        \u001b[36m0.5057\u001b[0m  0.0330\n",
      "     65        \u001b[36m0.5052\u001b[0m  0.0350\n",
      "     66        \u001b[36m0.5048\u001b[0m  0.0380\n",
      "     67        \u001b[36m0.5043\u001b[0m  0.0330\n",
      "     68        \u001b[36m0.5039\u001b[0m  0.0340\n",
      "     69        \u001b[36m0.5034\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.5030\u001b[0m  0.0340\n",
      "     71        \u001b[36m0.5026\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.5022\u001b[0m  0.0330\n",
      "     73        \u001b[36m0.5017\u001b[0m  0.0350\n",
      "     74        \u001b[36m0.5013\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.5009\u001b[0m  0.0350\n",
      "     76        \u001b[36m0.5005\u001b[0m  0.0360\n",
      "     77        \u001b[36m0.5001\u001b[0m  0.0350\n",
      "     78        \u001b[36m0.4997\u001b[0m  0.0350\n",
      "     79        \u001b[36m0.4993\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.4989\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.4985\u001b[0m  0.0340\n",
      "     82        \u001b[36m0.4981\u001b[0m  0.0340\n",
      "     83        \u001b[36m0.4978\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.4974\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.4970\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.4966\u001b[0m  0.0340\n",
      "     87        \u001b[36m0.4962\u001b[0m  0.0340\n",
      "     88        \u001b[36m0.4959\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.4955\u001b[0m  0.0340\n",
      "     90        \u001b[36m0.4951\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.4947\u001b[0m  0.0350\n",
      "     92        \u001b[36m0.4944\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.4940\u001b[0m  0.0350\n",
      "     94        \u001b[36m0.4936\u001b[0m  0.0330\n",
      "     95        \u001b[36m0.4933\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.4929\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.4925\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.4922\u001b[0m  0.0360\n",
      "     99        \u001b[36m0.4918\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.4914\u001b[0m  0.0350\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1        \u001b[36m0.7479\u001b[0m  0.0340\n",
      "      2        \u001b[36m0.7427\u001b[0m  0.0340\n",
      "      3        \u001b[36m0.7377\u001b[0m  0.0340\n",
      "      4        \u001b[36m0.7326\u001b[0m  0.0340\n",
      "      5        \u001b[36m0.7277\u001b[0m  0.0340\n",
      "      6        \u001b[36m0.7227\u001b[0m  0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      7        \u001b[36m0.7178\u001b[0m  0.0350\n",
      "      8        \u001b[36m0.7131\u001b[0m  0.0340\n",
      "      9        \u001b[36m0.7084\u001b[0m  0.0340\n",
      "     10        \u001b[36m0.7038\u001b[0m  0.0330\n",
      "     11        \u001b[36m0.6993\u001b[0m  0.0340\n",
      "     12        \u001b[36m0.6950\u001b[0m  0.0340\n",
      "     13        \u001b[36m0.6907\u001b[0m  0.0340\n",
      "     14        \u001b[36m0.6866\u001b[0m  0.0340\n",
      "     15        \u001b[36m0.6826\u001b[0m  0.0340\n",
      "     16        \u001b[36m0.6788\u001b[0m  0.0350\n",
      "     17        \u001b[36m0.6750\u001b[0m  0.0340\n",
      "     18        \u001b[36m0.6713\u001b[0m  0.0340\n",
      "     19        \u001b[36m0.6678\u001b[0m  0.0350\n",
      "     20        \u001b[36m0.6643\u001b[0m  0.0340\n",
      "     21        \u001b[36m0.6609\u001b[0m  0.0340\n",
      "     22        \u001b[36m0.6576\u001b[0m  0.0370\n",
      "     23        \u001b[36m0.6544\u001b[0m  0.0340\n",
      "     24        \u001b[36m0.6513\u001b[0m  0.0340\n",
      "     25        \u001b[36m0.6482\u001b[0m  0.0340\n",
      "     26        \u001b[36m0.6451\u001b[0m  0.0340\n",
      "     27        \u001b[36m0.6422\u001b[0m  0.0340\n",
      "     28        \u001b[36m0.6393\u001b[0m  0.0330\n",
      "     29        \u001b[36m0.6364\u001b[0m  0.0340\n",
      "     30        \u001b[36m0.6336\u001b[0m  0.0340\n",
      "     31        \u001b[36m0.6308\u001b[0m  0.0340\n",
      "     32        \u001b[36m0.6280\u001b[0m  0.0350\n",
      "     33        \u001b[36m0.6253\u001b[0m  0.0360\n",
      "     34        \u001b[36m0.6226\u001b[0m  0.0350\n",
      "     35        \u001b[36m0.6199\u001b[0m  0.0360\n",
      "     36        \u001b[36m0.6173\u001b[0m  0.0370\n",
      "     37        \u001b[36m0.6147\u001b[0m  0.0350\n",
      "     38        \u001b[36m0.6121\u001b[0m  0.0350\n",
      "     39        \u001b[36m0.6096\u001b[0m  0.0340\n",
      "     40        \u001b[36m0.6070\u001b[0m  0.0350\n",
      "     41        \u001b[36m0.6045\u001b[0m  0.0340\n",
      "     42        \u001b[36m0.6021\u001b[0m  0.0340\n",
      "     43        \u001b[36m0.5996\u001b[0m  0.0330\n",
      "     44        \u001b[36m0.5972\u001b[0m  0.0350\n",
      "     45        \u001b[36m0.5949\u001b[0m  0.0340\n",
      "     46        \u001b[36m0.5925\u001b[0m  0.0340\n",
      "     47        \u001b[36m0.5902\u001b[0m  0.0350\n",
      "     48        \u001b[36m0.5880\u001b[0m  0.0330\n",
      "     49        \u001b[36m0.5858\u001b[0m  0.0350\n",
      "     50        \u001b[36m0.5836\u001b[0m  0.0340\n",
      "     51        \u001b[36m0.5815\u001b[0m  0.0350\n",
      "     52        \u001b[36m0.5795\u001b[0m  0.0350\n",
      "     53        \u001b[36m0.5774\u001b[0m  0.0340\n",
      "     54        \u001b[36m0.5755\u001b[0m  0.0340\n",
      "     55        \u001b[36m0.5736\u001b[0m  0.0340\n",
      "     56        \u001b[36m0.5718\u001b[0m  0.0330\n",
      "     57        \u001b[36m0.5700\u001b[0m  0.0330\n",
      "     58        \u001b[36m0.5683\u001b[0m  0.0340\n",
      "     59        \u001b[36m0.5666\u001b[0m  0.0340\n",
      "     60        \u001b[36m0.5650\u001b[0m  0.0330\n",
      "     61        \u001b[36m0.5634\u001b[0m  0.0350\n",
      "     62        \u001b[36m0.5619\u001b[0m  0.0340\n",
      "     63        \u001b[36m0.5605\u001b[0m  0.0330\n",
      "     64        \u001b[36m0.5591\u001b[0m  0.0350\n",
      "     65        \u001b[36m0.5578\u001b[0m  0.0340\n",
      "     66        \u001b[36m0.5565\u001b[0m  0.0340\n",
      "     67        \u001b[36m0.5553\u001b[0m  0.0350\n",
      "     68        \u001b[36m0.5541\u001b[0m  0.0350\n",
      "     69        \u001b[36m0.5529\u001b[0m  0.0340\n",
      "     70        \u001b[36m0.5519\u001b[0m  0.0350\n",
      "     71        \u001b[36m0.5508\u001b[0m  0.0340\n",
      "     72        \u001b[36m0.5498\u001b[0m  0.0350\n",
      "     73        \u001b[36m0.5488\u001b[0m  0.0340\n",
      "     74        \u001b[36m0.5479\u001b[0m  0.0340\n",
      "     75        \u001b[36m0.5470\u001b[0m  0.0340\n",
      "     76        \u001b[36m0.5461\u001b[0m  0.0340\n",
      "     77        \u001b[36m0.5453\u001b[0m  0.0360\n",
      "     78        \u001b[36m0.5445\u001b[0m  0.0340\n",
      "     79        \u001b[36m0.5437\u001b[0m  0.0340\n",
      "     80        \u001b[36m0.5429\u001b[0m  0.0340\n",
      "     81        \u001b[36m0.5422\u001b[0m  0.0350\n",
      "     82        \u001b[36m0.5415\u001b[0m  0.0350\n",
      "     83        \u001b[36m0.5408\u001b[0m  0.0340\n",
      "     84        \u001b[36m0.5402\u001b[0m  0.0340\n",
      "     85        \u001b[36m0.5395\u001b[0m  0.0340\n",
      "     86        \u001b[36m0.5389\u001b[0m  0.0330\n",
      "     87        \u001b[36m0.5383\u001b[0m  0.0350\n",
      "     88        \u001b[36m0.5378\u001b[0m  0.0340\n",
      "     89        \u001b[36m0.5372\u001b[0m  0.0330\n",
      "     90        \u001b[36m0.5367\u001b[0m  0.0340\n",
      "     91        \u001b[36m0.5362\u001b[0m  0.0340\n",
      "     92        \u001b[36m0.5356\u001b[0m  0.0340\n",
      "     93        \u001b[36m0.5351\u001b[0m  0.0400\n",
      "     94        \u001b[36m0.5347\u001b[0m  0.0350\n",
      "     95        \u001b[36m0.5342\u001b[0m  0.0340\n",
      "     96        \u001b[36m0.5337\u001b[0m  0.0340\n",
      "     97        \u001b[36m0.5333\u001b[0m  0.0340\n",
      "     98        \u001b[36m0.5329\u001b[0m  0.0340\n",
      "     99        \u001b[36m0.5325\u001b[0m  0.0340\n",
      "    100        \u001b[36m0.5320\u001b[0m  0.0340\n",
      "  epoch    train_loss     dur\n",
      "-------  ------------  ------\n",
      "      1       \u001b[36m37.2583\u001b[0m  0.0500\n",
      "      2       37.2583  0.0500\n",
      "      3       37.2583  0.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:696: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 687, in _score\n",
      "    scores = scorer(estimator, X_test, y_test)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 199, in __call__\n",
      "    return self._score(partial(_cached_call, None), estimator, X, y_true,\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 236, in _score\n",
      "    y_pred = method_caller(estimator, \"predict\", X)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_scorer.py\", line 53, in _cached_call\n",
      "    return getattr(estimator, method)(*args, **kwargs)\n",
      "  File \"C:\\Users\\jason\\anaconda3\\lib\\site-packages\\skorch\\classifier.py\", line 357, in predict\n",
      "    return (y_proba[:, 1] > self.threshold).astype('uint8')\n",
      "IndexError: too many indices for array: array is 1-dimensional, but 2 were indexed\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\jason\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:922: UserWarning: One or more of the test scores are non-finite: [0.81689179 0.6274181  0.8120944  0.6274181  0.65201056 0.56773793\n",
      " 0.58684987 0.5217047  0.6274181  0.6274181  0.62917249 0.6274181\n",
      " 0.64143766 0.579941   0.70465766 0.62394038        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      4       37.2583  0.0540\n",
      "      5       37.2583  0.0520\n",
      "      6       37.2583  0.0510\n",
      "      7       37.2583  0.0520\n",
      "      8       37.2583  0.0500\n",
      "      9       37.2583  0.0490\n",
      "     10       37.2583  0.0500\n",
      "     11       37.2583  0.0490\n",
      "     12       37.2583  0.0490\n",
      "     13       37.2583  0.0500\n",
      "     14       37.2583  0.0490\n",
      "     15       37.2583  0.0500\n",
      "     16       37.2583  0.0490\n",
      "     17       37.2583  0.0500\n",
      "     18       37.2583  0.0500\n",
      "     19       \u001b[36m19.9941\u001b[0m  0.0500\n",
      "     20        \u001b[36m0.5471\u001b[0m  0.0490\n",
      "     21        \u001b[36m0.5400\u001b[0m  0.0500\n",
      "     22        \u001b[36m0.5269\u001b[0m  0.0490\n",
      "     23        \u001b[36m0.5163\u001b[0m  0.0490\n",
      "     24        \u001b[36m0.5037\u001b[0m  0.0510\n",
      "     25        \u001b[36m0.4898\u001b[0m  0.0490\n",
      "     26        \u001b[36m0.4776\u001b[0m  0.0500\n",
      "     27        \u001b[36m0.4656\u001b[0m  0.0490\n",
      "     28        \u001b[36m0.4550\u001b[0m  0.0490\n",
      "     29        \u001b[36m0.4427\u001b[0m  0.0490\n",
      "     30        \u001b[36m0.4291\u001b[0m  0.0490\n",
      "     31        \u001b[36m0.4158\u001b[0m  0.0500\n",
      "     32        \u001b[36m0.4010\u001b[0m  0.0500\n",
      "     33        \u001b[36m0.3906\u001b[0m  0.0490\n",
      "     34        \u001b[36m0.3721\u001b[0m  0.0480\n",
      "     35        0.3726  0.0490\n",
      "     36        \u001b[36m0.3529\u001b[0m  0.0490\n",
      "     37        \u001b[36m0.3386\u001b[0m  0.0500\n",
      "     38        0.3511  0.0500\n",
      "     39        \u001b[36m0.3290\u001b[0m  0.0500\n",
      "     40        \u001b[36m0.3192\u001b[0m  0.0490\n",
      "     41        0.3275  0.0500\n",
      "     42        \u001b[36m0.3023\u001b[0m  0.0500\n",
      "     43        0.3260  0.0490\n",
      "     44        \u001b[36m0.2887\u001b[0m  0.0510\n",
      "     45        0.3127  0.0500\n",
      "     46        \u001b[36m0.2734\u001b[0m  0.0490\n",
      "     47        0.3039  0.0510\n",
      "     48        0.2980  0.0500\n",
      "     49        \u001b[36m0.2720\u001b[0m  0.0510\n",
      "     50        \u001b[36m0.2593\u001b[0m  0.0490\n",
      "     51        0.2598  0.0500\n",
      "     52        0.2758  0.0490\n",
      "     53        \u001b[36m0.2515\u001b[0m  0.0500\n",
      "     54        0.2564  0.0490\n",
      "     55        0.2566  0.0500\n",
      "     56        \u001b[36m0.2496\u001b[0m  0.0500\n",
      "     57        0.2536  0.0510\n",
      "     58        \u001b[36m0.2435\u001b[0m  0.0490\n",
      "     59        \u001b[36m0.2206\u001b[0m  0.0510\n",
      "     60        0.2385  0.0500\n",
      "     61        0.2527  0.0490\n",
      "     62        0.2420  0.0500\n",
      "     63        0.2280  0.0490\n",
      "     64        \u001b[36m0.2132\u001b[0m  0.0500\n",
      "     65        0.2237  0.0490\n",
      "     66        0.2324  0.0490\n",
      "     67        0.2200  0.0490\n",
      "     68        0.2281  0.0500\n",
      "     69        0.2155  0.0490\n",
      "     70        \u001b[36m0.2037\u001b[0m  0.0490\n",
      "     71        0.2087  0.0500\n",
      "     72        0.2105  0.0500\n",
      "     73        \u001b[36m0.1969\u001b[0m  0.0500\n",
      "     74        0.1997  0.0500\n",
      "     75        0.1989  0.0500\n",
      "     76        0.2041  0.0500\n",
      "     77        0.1984  0.0500\n",
      "     78        0.1971  0.0500\n",
      "     79        \u001b[36m0.1856\u001b[0m  0.0500\n",
      "     80        0.1911  0.0500\n",
      "     81        0.1945  0.0490\n",
      "     82        0.1895  0.0500\n",
      "     83        0.1914  0.0490\n",
      "     84        0.1906  0.0500\n",
      "     85        0.1978  0.0510\n",
      "     86        \u001b[36m0.1740\u001b[0m  0.0540\n",
      "     87        0.1861  0.0550\n",
      "     88        0.1838  0.0770\n",
      "     89        0.1803  0.0830\n",
      "     90        0.1833  0.0890\n",
      "     91        0.1770  0.0700\n",
      "     92        0.1795  0.0840\n",
      "     93        0.1832  0.0820\n",
      "     94        0.1788  0.0880\n",
      "     95        0.1755  0.0620\n",
      "     96        0.1795  0.0800\n",
      "     97        \u001b[36m0.1737\u001b[0m  0.0750\n",
      "     98        0.1768  0.0750\n",
      "     99        \u001b[36m0.1714\u001b[0m  0.0570\n",
      "    100        \u001b[36m0.1635\u001b[0m  0.0540\n"
     ]
    }
   ],
   "source": [
    "grid_search = grid_search.fit(previsores, classe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cdcbe9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "melhores_parametros = grid_search.best_params_\n",
    "melhor_precisao = grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b0a7159f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 10,\n",
       " 'criterion': torch.nn.modules.loss.BCELoss,\n",
       " 'max_epochs': 100,\n",
       " 'module__activation': <function torch.nn.functional.relu(input: torch.Tensor, inplace: bool = False) -> torch.Tensor>,\n",
       " 'module__initializer': <function torch.nn.init.uniform_(tensor: torch.Tensor, a: float = 0.0, b: float = 1.0) -> torch.Tensor>,\n",
       " 'module__neurons': 8,\n",
       " 'optimizer': torch.optim.adam.Adam}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhores_parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "45ba72aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816891786989598"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melhor_precisao"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
